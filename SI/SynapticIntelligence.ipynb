{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"SynapticIntelligence.ipynb","provenance":[{"file_id":"https://github.com/dchu1/AI_P2_cl/blob/master/SynapticIntelligence.ipynb","timestamp":1604799720719}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"326b32ff74e849cfb19790eaa763124d":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_a69a09289cc94eabb9d1ef9f1fca07f9","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_92c1164a2a0f49eeba44fd7a4ccc2238","IPY_MODEL_419e0e2672d043108677dbac53fbe5e8"]}},"a69a09289cc94eabb9d1ef9f1fca07f9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"92c1164a2a0f49eeba44fd7a4ccc2238":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_b560d6d73ad345b2a49137ac405057dc","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"info","max":1,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":1,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_5ab7a9f7f1094b9e94af435983f338b5"}},"419e0e2672d043108677dbac53fbe5e8":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_93fe05ba6a1c4236b61cf9a26d43cfc2","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 9920512/? [00:20&lt;00:00, 1845697.50it/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_4d0cd8254c8b4a9cb856655d07052201"}},"b560d6d73ad345b2a49137ac405057dc":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"5ab7a9f7f1094b9e94af435983f338b5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"93fe05ba6a1c4236b61cf9a26d43cfc2":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"4d0cd8254c8b4a9cb856655d07052201":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"2a63e79ed5ea4c8c86f6bbd0d8e2ed58":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_2458b7fe34224f17bbdc49e27c642933","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_9f07470518d548efa2c0a5c99037183e","IPY_MODEL_a35e750a3e4247b4b87c7469f595ece0"]}},"2458b7fe34224f17bbdc49e27c642933":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"9f07470518d548efa2c0a5c99037183e":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_bd7dada4c26f4e51ad21614ec7250203","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":1,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":1,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_14507526a1c34ad5afd461f0ea112188"}},"a35e750a3e4247b4b87c7469f595ece0":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_0a0af09768004bf98bf59896e44a2e80","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 32768/? [00:00&lt;00:00, 66011.36it/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_e3d994624b8e4eb19ff6147433655909"}},"bd7dada4c26f4e51ad21614ec7250203":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"14507526a1c34ad5afd461f0ea112188":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"0a0af09768004bf98bf59896e44a2e80":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"e3d994624b8e4eb19ff6147433655909":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"775c893d21c34c049eb35d576af1d750":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_d798e565ed564bec89b773221767dfed","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_7639da481ed94dbf80b3c6c1219eeb70","IPY_MODEL_95d31b10a7ca428186b72fd0016b778d"]}},"d798e565ed564bec89b773221767dfed":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"7639da481ed94dbf80b3c6c1219eeb70":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_d041134de1ee463f80436652fe035039","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":1,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":1,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_a372ba47ad1a4e6b93e5bb5966d756bc"}},"95d31b10a7ca428186b72fd0016b778d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_5f3390da066943bea5cfadcd87b09974","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 1654784/? [00:00&lt;00:00, 4770906.56it/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_03410e1acaf947c2b50b7b58d5470edf"}},"d041134de1ee463f80436652fe035039":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"a372ba47ad1a4e6b93e5bb5966d756bc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"5f3390da066943bea5cfadcd87b09974":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"03410e1acaf947c2b50b7b58d5470edf":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"e584740c5d8c4ac48708de15f74092ca":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_e952a0cae46a42e5bbfee4f53db9931b","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_c6b8754690f24e6791a3b9f2a3244954","IPY_MODEL_955041a531534e96bcf9a36a4957b551"]}},"e952a0cae46a42e5bbfee4f53db9931b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"c6b8754690f24e6791a3b9f2a3244954":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_ef49364b9b6a478191857305b23b0955","_dom_classes":[],"description":"  0%","_model_name":"FloatProgressModel","bar_style":"info","max":1,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":0,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_6c373f1d1e7649ca9038805e92e4000b"}},"955041a531534e96bcf9a36a4957b551":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_8649a672b86e4f59909e4a40fd5679a0","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 0/4542 [00:00&lt;?, ?it/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_735e884cdc80424580a4e4993c381a1b"}},"ef49364b9b6a478191857305b23b0955":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"6c373f1d1e7649ca9038805e92e4000b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"8649a672b86e4f59909e4a40fd5679a0":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"735e884cdc80424580a4e4993c381a1b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"markdown","metadata":{"id":"aEXzD84NQHfr"},"source":["# Import Library"]},{"cell_type":"code","metadata":{"id":"2tXqCHkzCt4L","executionInfo":{"status":"ok","timestamp":1614259737977,"user_tz":-420,"elapsed":4406,"user":{"displayName":"Lâm Trần","photoUrl":"","userId":"03210360344194418537"}}},"source":["import math\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","import torchvision\n","import torch.utils.data as data_utils\n","import numpy as np\n","import subprocess\n","import os\n","import random\n","import matplotlib.pyplot as plt\n","\n","from torch.nn.parameter import Parameter\n","from torch.nn import init\n","from torch.nn import Module\n","from torch.nn import init\n","from torchvision import datasets, transforms\n","from PIL import Image\n","from IPython.core.debugger import set_trace\n","\n","\n","n_tasks = 10\n","# n_epochs = 4\n","print_messages = True"],"execution_count":1,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":746,"referenced_widgets":["326b32ff74e849cfb19790eaa763124d","a69a09289cc94eabb9d1ef9f1fca07f9","92c1164a2a0f49eeba44fd7a4ccc2238","419e0e2672d043108677dbac53fbe5e8","b560d6d73ad345b2a49137ac405057dc","5ab7a9f7f1094b9e94af435983f338b5","93fe05ba6a1c4236b61cf9a26d43cfc2","4d0cd8254c8b4a9cb856655d07052201","2a63e79ed5ea4c8c86f6bbd0d8e2ed58","2458b7fe34224f17bbdc49e27c642933","9f07470518d548efa2c0a5c99037183e","a35e750a3e4247b4b87c7469f595ece0","bd7dada4c26f4e51ad21614ec7250203","14507526a1c34ad5afd461f0ea112188","0a0af09768004bf98bf59896e44a2e80","e3d994624b8e4eb19ff6147433655909","775c893d21c34c049eb35d576af1d750","d798e565ed564bec89b773221767dfed","7639da481ed94dbf80b3c6c1219eeb70","95d31b10a7ca428186b72fd0016b778d","d041134de1ee463f80436652fe035039","a372ba47ad1a4e6b93e5bb5966d756bc","5f3390da066943bea5cfadcd87b09974","03410e1acaf947c2b50b7b58d5470edf","e584740c5d8c4ac48708de15f74092ca","e952a0cae46a42e5bbfee4f53db9931b","c6b8754690f24e6791a3b9f2a3244954","955041a531534e96bcf9a36a4957b551","ef49364b9b6a478191857305b23b0955","6c373f1d1e7649ca9038805e92e4000b","8649a672b86e4f59909e4a40fd5679a0","735e884cdc80424580a4e4993c381a1b"]},"id":"r0Siy4iBYVdb","outputId":"57bf9817-e350-40e6-bec3-963b7cfe9f9a"},"source":["def create_data():\n","  train_set = datasets.MNIST('./data', train=True, download=True)\n","  test_set = datasets.MNIST('./data', train=False, download=True)\n","  X_train = train_set.data.view(60000, 28*28).float()\n","  y_train = train_set.targets\n","  X_test = test_set.data.view(10000, 28*28).float()\n","  y_test = test_set.targets \n","  return X_train, y_train, X_test, y_test\n","\n","import torch\n","import torch.nn as nn\n","import numpy as np\n","import torchvision.datasets as datasets\n","import torchvision.transforms as T\n","from torch import linalg as LA\n","\n","\n","input_size = 28*28\n","hidden_size = 64\n","num_epochs = 4\n","bs = 256\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","\n","X_train, y_train, X_test, y_test = create_data()\n","# y_train = y_train.view(1, y_train.shape[0])\n","# y_test = y_test.view(1, y_test.shape[0])\n","n_train = X_train.shape[0]\n","# X_train = torch.tensor(X_train, dtype=torch.float)\n","# y_train = torch.tensor(y_train, dtype=torch.long)\n","# X_test = torch.tensor(X_test, dtype=torch.float)\n","# y_test = torch.tensor(y_test, dtype=torch.long)\n","# print(X_train.shape)\n","# print(y_train.shape)\n","train_dataset = torch.utils.data.TensorDataset(X_train, y_train)\n","train_loader = torch.utils.data.DataLoader(dataset = train_dataset, batch_size=bs, shuffle=True)\n","test_dataset = torch.utils.data.TensorDataset(X_test, y_test)\n","test_loader = torch.utils.data.DataLoader(dataset = test_dataset, batch_size=bs, shuffle=False)\n","\n","class Net(nn.Module):\n","  def __init__(self, input_size, hidden_size):\n","    super(Net, self).__init__()\n","    self.fc1 = nn.Linear(input_size, hidden_size)\n","    self.relu = nn.ReLU()\n","    self.fc2 = nn.Linear(hidden_size, hidden_size)\n","    self.last = nn.Linear(hidden_size, 10)\n","  def forward(self, x):\n","    out = self.fc1(x)\n","    out = self.relu(out)\n","    out = self.fc2(out)\n","    out = self.relu(out)\n","    out = self.last(out)\n","    # out = self.relu(out)\n","    return out\n","  def u(self,x):\n","    u = self.forward(x)\n","    u.requires_grad_(True)\n","    return u\n","  \n","model = Net(input_size, hidden_size).to(device)\n","criterion = nn.CrossEntropyLoss()    #MSELoss\n","lr = 0.01\n","optim = torch.optim.Adam(model.parameters(), lr=lr)\n","\n","training_loss = []\n","total_step = len(train_loader)\n","\n","def train():\n","  for epoch in range(num_epochs):\n","    i=0\n","    for (x,y) in (train_loader):\n","      i+=1\n","      x = x.to(device)\n","      x.requires_grad_(True)\n","      y = y.to(device)\n","\n","      output = model(x)\n","      loss = criterion(output, y)\n","\n","      optim.zero_grad()\n","      loss.backward()\n","      optim.step()\n","\n","      training_loss.append(loss.item())\n","\n","      if (i+1) % 10 == 0:\n","        model.eval()\n","        test_loss = 0\n","        correct = 0\n","        with torch.no_grad():\n","          for x_test,y_test in test_loader:\n","            x_test = x_test.to(device)\n","            y_test = y_test.to(device)\n","            yhat = model(x_test)\n","            test_loss += criterion(yhat, y_test).item()\n","            pred = torch.max(yhat.data,1)[1]\n","            correct += (pred == y_test).sum()\n","        print ('Epoch [{}/{}], Step [{}/{}], Train Loss: {:.8f}, Test Loss: {:.8f}, Acc: {:.3f}' \n","                    .format(epoch+1, num_epochs, i, total_step, loss.item(), test_loss, 100. * correct/len(test_loader.dataset)))\n","        \n","\n","X_train.requires_grad_(True)\n","\n","#1. Before training\n","L_before = 0\n","yhat_before = model.u(X_train.to(device))\n","# print(criterion(y_train.view(1, y_train.shape[0]).to(device), yhat_before))\n","for i in range(100):\n","  jac = torch.autograd.functional.jacobian(model.u, (X_train[i].view(-1,28*28)).to(device), create_graph=True)\n","  l = LA.norm(jac.view(10,28*28), 2)\n","  L_before = max(L_before, l)\n","print(\"Before training, L = \", L_before)\n","\n","#2. After training\n","L_after = 0\n","train()\n","yhat_after = model.u(X_train.to(device))\n","# print(criterion(y_train.to(device), yhat_after))\n","log = 1\n","\n","for i in range(100):\n","  jac = torch.autograd.functional.jacobian(model.u, (X_train[i].view(-1,28*28)).to(device), create_graph=True)\n","  l = LA.norm(jac.view(10,28*28), 2)\n","  L_after = max(L_after, l)\n","  if i == log:\n","    print(jac.view(10,28*28))\n","print(\"After training, L = \", L_after)\n","\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"326b32ff74e849cfb19790eaa763124d","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n","Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"2a63e79ed5ea4c8c86f6bbd0d8e2ed58","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n","Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"775c893d21c34c049eb35d576af1d750","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n","Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n","\n","\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e584740c5d8c4ac48708de15f74092ca","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n","Processing...\n","Done!\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torchvision/datasets/mnist.py:480: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /pytorch/torch/csrc/utils/tensor_numpy.cpp:141.)\n","  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\n"],"name":"stderr"},{"output_type":"stream","text":["Before training, L =  tensor(0.2526, device='cuda:0', grad_fn=<CopyBackwards>)\n","Epoch [1/4], Step [9/235], Train Loss: 5.50314951, Test Loss: 164.66328597, Acc: 19.390\n","Epoch [1/4], Step [19/235], Train Loss: 1.79202485, Test Loss: 73.92517602, Acc: 43.740\n","Epoch [1/4], Step [29/235], Train Loss: 1.37780190, Test Loss: 54.59385216, Acc: 55.430\n","Epoch [1/4], Step [39/235], Train Loss: 1.15504384, Test Loss: 45.55958271, Acc: 66.070\n","Epoch [1/4], Step [49/235], Train Loss: 0.91023302, Test Loss: 37.09065193, Acc: 72.780\n","Epoch [1/4], Step [59/235], Train Loss: 0.90739602, Test Loss: 33.02949399, Acc: 74.950\n","Epoch [1/4], Step [69/235], Train Loss: 0.79380929, Test Loss: 30.73508584, Acc: 78.750\n","Epoch [1/4], Step [79/235], Train Loss: 0.64854139, Test Loss: 27.00860775, Acc: 79.430\n","Epoch [1/4], Step [89/235], Train Loss: 0.53463775, Test Loss: 25.16336834, Acc: 82.870\n","Epoch [1/4], Step [99/235], Train Loss: 0.53553331, Test Loss: 23.30355057, Acc: 83.210\n","Epoch [1/4], Step [109/235], Train Loss: 0.47874069, Test Loss: 20.26094125, Acc: 85.810\n","Epoch [1/4], Step [119/235], Train Loss: 0.59070003, Test Loss: 20.58810038, Acc: 86.410\n","Epoch [1/4], Step [129/235], Train Loss: 0.45075381, Test Loss: 18.35139454, Acc: 87.650\n","Epoch [1/4], Step [139/235], Train Loss: 0.61343825, Test Loss: 18.42723566, Acc: 86.990\n","Epoch [1/4], Step [149/235], Train Loss: 0.49220341, Test Loss: 18.61511995, Acc: 86.780\n","Epoch [1/4], Step [159/235], Train Loss: 0.55204749, Test Loss: 17.15405294, Acc: 88.310\n","Epoch [1/4], Step [169/235], Train Loss: 0.42928249, Test Loss: 17.35757795, Acc: 88.090\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"w6X8N2ucCkBX"},"source":["# Download package"]},{"cell_type":"code","metadata":{"id":"tyJtcwJQKwOg"},"source":["pip install ax-platform"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1VKScODhve7t","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1609321978823,"user_tz":-420,"elapsed":825,"user":{"displayName":"Lâm Trần","photoUrl":"","userId":"03210360344194418537"}},"outputId":"373fe219-4d54-4320-9eb9-0d60a226429a"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"x4JqoVPDvOVs"},"source":["# Constructing Data Set"]},{"cell_type":"code","metadata":{"id":"BmcW2shcjnsr"},"source":["class ConstructDataset():\n","  def __init__(self, type_data, data_path, mnist_path):\n","    self.type_data = type_data\n","    self.data_path = data_path\n","    self.mnist_path = mnist_path\n","    f = np.load(os.path.join(data_path, mnist_path))\n","    self.x_tr = torch.from_numpy(f[\"x_train\"])\n","    self.y_tr = torch.from_numpy(f[\"y_train\"]).long()\n","    self.x_te = torch.from_numpy(f[\"x_test\"])\n","    self.y_te = torch.from_numpy(f[\"y_test\"]).long()\n","    f.close()\n","\n","  def create_one_task(self, type_data, seed, t, n_tasks):   # return one task as np array\n","    np.random.seed(seed)\n","    random.seed(seed)\n","\n","    if type_data == 'rotated_mnist':\n","      def rotate_dataset(d, rotation):\n","        result = torch.FloatTensor(d.size(0), 784)  # 60000*784\n","        tensor = transforms.ToTensor()\n","\n","        for i in range(d.size(0)):\n","          img = Image.fromarray(d[i].numpy(), mode=\"L\")\n","          result[i] = tensor(img.rotate(rotation)).view(784)\n","        return result     # 60000 vectors-784\n","      \n","      min_rot = 1.0 * t / n_tasks * (180.0 - 0.0) + 0.0\n","      max_rot = 1.0 * (t+1) / n_tasks * (180.0 - 0.0) + 0.0\n","      rot = random.random() * (max_rot - min_rot) + min_rot\n","      task_tr = [rotate_dataset(self.x_tr, rot), self.y_tr, rot]\n","      task_te = [rotate_dataset(self.x_te, rot), self.y_te, rot]\n","\n","      return task_tr, task_te\n","    \n","    if type_data == 'permuted_mnist':\n","      def permute(d, permutation):   # d[i] is flattened\n","        res = torch.FloatTensor(d.shape[0], 784)\n","        for i in range(d.shape[0]):\n","          res[i] = d[i][permutation]\n","        return res\n","\n","      permutation = np.random.permutation(784)\n","      task_tr = [permute(self.x_tr.view(self.x_tr.shape[0], 28*28), permutation), self.y_tr]\n","      task_te = [permute(self.x_te.view(self.x_te.shape[0], 28*28), permutation), self.y_te]\n","\n","      return task_tr, task_te\n","\n","  def create_several_tasks(self, num_tasks, file_name, type_data):\n","    tasks_tr = []\n","    tasks_te = []\n","    if file_name != \"\":   # if save constructed data to file\n","      path = file_name + \"_\" + str(num_tasks) + \".pt\"\n","      if not os.path.exists(os.path.join(self.data_path, path)):\n","        for t in range(num_tasks):\n","          task_tr, task_te = self.create_one_task(type_data, t, t, num_tasks) \n","          tasks_tr.append(task_tr)\n","          tasks_te.append(task_te)\n","        torch.save([tasks_tr, tasks_te], self.data_path + \"/\" + path)\n","      return self.data_path + \"/\" + path\n","    else:   # load all task to RAM\n","      if type_data == 'rotated_mnist':\n","        path = \"mnist_rotations\" + \"_\" + str(num_tasks) + \".pt\"\n","        if not os.path.exists(os.path.join(self.data_path, path)):\n","          for t in range(num_tasks):\n","            task_tr, task_te = self.create_one_task(type_data, t, t, num_tasks) \n","            tasks_tr.append(task_tr)\n","            tasks_te.append(task_te)\n","          # torch.save([tasks_tr, tasks_te], self.data_path + \"/\" + path)\n","        else:\n","          tasks_tr, tasks_te = torch.load(self.data_path + \"/\" + path)\n","      if type_data == 'permuted_mnist':\n","        path = \"mnist_permutations\" + \"_\" + str(num_tasks) + \".pt\"\n","        if not os.path.exists(os.path.join(self.data_path, path)):\n","          for t in range(num_tasks):\n","            task_tr, task_te = self.create_one_task(type_data, t, t, num_tasks) \n","            tasks_tr.append(task_tr)\n","            tasks_te.append(task_te)\n","          # torch.save([tasks_tr, tasks_te], self.data_path + \"/\" + path)\n","        else:\n","          tasks_tr, tasks_te = torch.load(self.data_path + \"/\" + path)\n","      return tasks_tr, tasks_te"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9R5sbiRDplDZ"},"source":["data_path = '/content/drive/MyDrive/Colab_Notebooks/LAB/Some Models/SI/data'\n","data = ConstructDataset('permuted_mnist', data_path, 'mnist.npz')\n","tasks_tr, tasks_te = data.create_several_tasks(n_tasks, \"\", 'permuted_mnist')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"V3w5psinhrGb"},"source":["# Defining Synaptic Intelligence Model (Simple)"]},{"cell_type":"code","metadata":{"id":"9DhQQzqOhuJw"},"source":["class SIModel(nn.Module):\n","    def __init__(self):\n","        super(SIModel, self).__init__()\n","\n","        # SI Hyperparameters\n","        self.si_c = 0.          # regularization strength\n","        self.epsilon = 0.1      # dampening parameter\n","    \n","    def init_weights(self, m):\n","        if type(m) == nn.Linear:\n","            torch.nn.init.kaiming_uniform_(m.weight)\n","            m.bias.data.fill_(0.01)\n","\n","    def init(self, n_neurons):\n","        # Simple MLP\n","        self.net = nn.Sequential(\n","            nn.Linear(28*28, n_neurons),\n","            nn.ReLU(),\n","            nn.Linear(n_neurons, n_neurons),\n","            nn.ReLU(),\n","            nn.Linear(n_neurons, n_neurons),\n","            nn.ReLU(),\n","            nn.Linear(n_neurons, 10)\n","        )\n","        self.net.apply(self.init_weights)\n","\n","    def forward(self, x):\n","        return self.net(x)\n","\n","    def update_omega(self, W):\n","        '''\n","        After completing training on a task, update the per-parameter regularization strength.\n","        [W] <dict> estimated parameter-specific contribution to changes in total loss of completed task\n","        '''\n","\n","        # Loop over all parameters\n","        for n, p in self.named_parameters():\n","            if p.requires_grad:\n","                n = n.replace('.', '__')\n","\n","                # Find/calculate new values for quadratic penalty on parameters\n","                p_prev = getattr(self, '{}_SI_prev_task'.format(n))\n","                p_current = p.detach().clone()\n","                p_change = p_current - p_prev\n","                \n","                omega_add = W[n]/(p_change**2 + self.epsilon)\n","                try:\n","                    omega = getattr(self, '{}_SI_omega'.format(n))\n","                except AttributeError:\n","                    omega = p.detach().clone().zero_()\n","                omega_new = omega + omega_add\n","\n","                # Store these new values in the model\n","                self.register_buffer('{}_SI_prev_task'.format(n), p_current)\n","                self.register_buffer('{}_SI_omega'.format(n), omega_new)\n","\n","    def surrogate_loss(self):\n","        '''\n","        Calculate SI's surrogate loss.\n","        '''\n","        try:\n","            losses = []\n","            for n, p in self.named_parameters():\n","                if p.requires_grad:\n","                    # Retrieve previous parameter values and their normalized path integral (i.e., omega)\n","                    n = n.replace('.', '__')\n","                    prev_values = getattr(self, '{}_SI_prev_task'.format(n))\n","                    omega = getattr(self, '{}_SI_omega'.format(n))\n","                    # Calculate SI's surrogate loss, sum over all parameters\n","                    losses.append((omega * (p-prev_values)**2).sum())\n","            return sum(losses)\n","        except AttributeError:\n","            # SI-loss is 0 if there is no stored omega yet\n","            return torch.tensor(0., device=self._device())\n","\n","    def _device(self):\n","        return next(self.parameters()).device\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZzvUNaI20C_G"},"source":["# Running experiment"]},{"cell_type":"code","metadata":{"id":"846N7L_Q2asU"},"source":["def train_task(model, device, train_loader, optimizer, batch_log, n_epochs):\n","\n","    model.train()\n","\n","    # Prepare <dicts> to store running importance estimates and param-values before update\n","    W = {}\n","    param_old = {}\n","    for name, param in model.named_parameters():\n","        if param.requires_grad:\n","            name = name.replace('.', '__')\n","            W[name] = param.data.clone().zero_()\n","            param_old[name] = param.data.clone()\n","\n","    losses = []\n","    total_losses = []\n","    for k in range(n_epochs):\n","        if print_messages:\n","            print(\"----> Epoch {}:\".format(k))\n","        for batch_idx, (x, y) in enumerate(train_loader):\n","            x, y = x.to(device), y.to(device)\n","            optimizer.zero_grad()\n","            # Get the prediction\n","            y_hat = model(x)\n","            # Calculate training-precision\n","            precision = (y == y_hat.max(1)[1]).sum().item() / x.size(0)\n","            # Calculate the loss using CROSS ENTROPY and the SURROGATE LOSS\n","            loss = F.cross_entropy(input=y_hat, target=y, reduction='mean')\n","            surrogate_loss = model.surrogate_loss()\n","            total_loss = loss + model.si_c * surrogate_loss\n","            # Backpropagate errors\n","            total_loss.backward()\n","            # Take optimization-step\n","            optimizer.step()\n","\n","            # Update running parameter importance estimates in W\n","            # \"In practice, we can approximate w as the running sum of the \n","            # product of the gradient g(w) and the parameter update\" \n","            for name, param in model.named_parameters():\n","                if param.requires_grad:\n","                    name = name.replace('.', '__')\n","                    if param.grad is not None:\n","                        W[name].add_(-param.grad*(param.detach()-param_old[name]))\n","                    param_old[name] = param.detach().clone()\n","\n","            # Print out a log\n","            if batch_idx % batch_log == 0:\n","                losses.append(loss.item())\n","                total_losses.append(total_loss.item())\n","                if print_messages:\n","                    print('---->[{}/{} ({:.0f}%)]\\tPrecision: {:.6f}\\tLoss: {:.6f}\\tSurrogate Loss: {:.6f}\\tTotal Loss: {:.6f}'.format(\n","                        batch_idx * len(x), len(train_loader.dataset),\n","                        100. * batch_idx / len(train_loader), \n","                        precision, loss.item(), surrogate_loss.item(), total_loss.item()))\n","            \n","    # After finishing training on a task, update the omega value in the model\n","    model.update_omega(W)\n","\n","    return losses, total_losses\n","    \n","def test(model, device, test_loader):\n","    model.eval()\n","    test_loss = 0\n","    correct = 0\n","    with torch.no_grad():\n","        for x, y in test_loader:\n","            x, y = x.to(device), y.to(device)\n","            y_hat = model(x)\n","            test_loss += F.cross_entropy(input=y_hat, target=y, reduction='mean')\n","            pred = y_hat.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n","            correct += pred.eq(y.view_as(pred)).sum().item()\n","    \n","    test_loss /= len(test_loader.dataset)\n","    return correct, test_loss\n","\n","def eval_on_tasks(model, device, test_loaders):\n","    acc = []  # accuracy of each task\n","    test_losses = []     # loss of each task\n","    for j in range(n_tasks):\n","        correct, test_loss = test(model, device, test_loaders[j])\n","        acc.append(correct / len(test_loaders[j].dataset))\n","        test_losses.append(test_loss)\n","        if print_messages:\n","            print('---->Test set {}: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)'.format(\n","                j, test_loss, correct, len(test_loaders[j].dataset),\n","                100. * correct / len(test_loaders[j].dataset)))\n","    return acc, test_losses"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VznhdxdQapPC"},"source":["config={\n","    \"lr\": 0.003, \n","    \"si_c\": 0.1, \n","    \"si_epsilon\": 0.01,\n","    \"optimizer\": \"adam\",\n","    \"batch_size\": 64,\n","    \"n_neurons\": 2000,\n","    \"sample_size\": 60000\n","    }"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hpPc6HSRvHYK"},"source":["def main(config, n_epochs): \n","    # Use cuda?\n","    cuda = torch.cuda.is_available()\n","    device = torch.device(\"cuda\" if cuda else \"cpu\")\n","\n","    # Create Model\n","    model = SIModel()\n","    model.init(config[\"n_neurons\"])\n","    model.to(device)\n","    optim_list = [{'params': filter(lambda p: p.requires_grad, model.parameters()), 'lr': config['lr']}]\n","    if config['optimizer'] == \"adam\":\n","        optimizer = optim.Adam(optim_list, betas=(0.9, 0.999))\n","    else:\n","        optimizer = optim.SGD(optim_list)\n","\n","    # SI Parameters\n","    model.si_c = config[\"si_c\"]\n","    model.epsilon = config[\"si_epsilon\"]\n","\n","    for name, param in model.named_parameters():\n","        if param.requires_grad:\n","            name = name.replace('.', '__')\n","            model.register_buffer('{}_SI_prev_task'.format(name), param.data.clone())\n","\n","    # Load test data\n","    test_loaders = []\n","    for i in range(n_tasks):\n","        test_loaders.append(data_utils.DataLoader(data_utils.TensorDataset(tasks_te[i][0], tasks_te[i][1]), batch_size=1000, shuffle = False))\n","\n","    # Training\n","    if print_messages:\n","        print(\"--> Training:\")\n","\n","    total_acc = []\n","    total_test_losses = []\n","    \n","    # Before we start training we will get a baseline by evaluating our tasks\n","    acc, test_losses = eval_on_tasks(model, device, test_loaders)\n","    total_acc.append(acc)\n","    total_test_losses.append(test_losses)\n","\n","    for i in range(n_tasks):\n","        if print_messages:\n","            print(\"--> Training Task {}:\".format(i))\n","\n","        perm = np.random.permutation(tasks_tr[i][1].size(0))    # data shuffle\n","        perm = perm[:config['sample_size']]\n","        train_data = data_utils.TensorDataset(tasks_tr[i][0], tasks_tr[i][1])\n","        train_loader = data_utils.DataLoader(train_data, batch_size=config[\"batch_size\"], \n","                                      sampler = data_utils.SubsetRandomSampler(perm), drop_last = True)\n","        \n","        train_losses, total_train_losses = train_task(model, device, train_loader, optimizer, 200, n_epochs)\n","        \n","        # Reset the optimizer (if using adam)\n","        if config['optimizer'] == \"adam\":\n","            model.optimizer = optim.Adam(optim_list, betas=(0.9, 0.999))\n","\n","        if print_messages:\n","            print(train_losses)\n","            print(total_train_losses)\n","            print(\"--> Finished Training Task {}. Starting Test phase:\".format(i))\n","\n","        acc, test_losses = eval_on_tasks(model, device, test_loaders)\n","        total_acc.append(acc)\n","        total_test_losses.append(test_losses)\n","    \n","    haha=model\n","\n","    return total_acc, total_test_losses"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HSffNskZDu8q","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"error","timestamp":1609321598218,"user_tz":-420,"elapsed":3135851,"user":{"displayName":"Lâm Trần","photoUrl":"","userId":"03210360344194418537"}},"outputId":"e860137a-0517-4b1e-980c-6066f3323f76"},"source":["# result_path = '/content/drive/MyDrive/Colab_Notebooks/LAB/Some Models/SI/result/over_parameter'\n","\n","# n_epochs_range = [1,2,3,4,5,6,7,8,9,10]\n","\n","# for n_epochs in n_epochs_range:\n","#   total_acc, total_test_losses = main(config, n_epochs)    # acc of 10 tasks after training each task \n","\n","#   # Get the accuracy metric as defined by Facebook paper: sum(R_Ti) \n","#   # where T is the test set of the last Task and i is the current trained task\n","#   average_acc = np.mean(total_acc[n_tasks])\n","#   print(\"Accuracy:\", average_acc)\n","#   print(\"Confusion matrix:\")\n","#   print('\\n'.join([','.join([str(item) for item in row]) for row in total_acc]))\n","\n","#   result_name = '0.1_e' + str(n_epochs) + '_big_si_permuted_10'\n","#   with open(result_path + \"/\" + result_name + \".txt\", \"w\") as f:\n","#     for t in range(1, n_tasks+1):\n","#       f.write(\"{} : \".format(str(t)))\n","#       for acc in total_acc[t]:\n","#         f.write(\"{:.4f} \".format(acc))\n","#       # f.write(\" \".join(str(total_acc[t])))\n","#       f.write(\"\\n\")\n","\n","# only performance on task 1 is good????"],"execution_count":null,"outputs":[{"output_type":"stream","text":["--> Training:\n","---->Test set 0: Average loss: 0.1601, Accuracy: 774/10000 (8%)\n","---->Test set 1: Average loss: 0.1545, Accuracy: 882/10000 (9%)\n","---->Test set 2: Average loss: 0.1657, Accuracy: 878/10000 (9%)\n","---->Test set 3: Average loss: 0.1313, Accuracy: 1173/10000 (12%)\n","---->Test set 4: Average loss: 0.1961, Accuracy: 830/10000 (8%)\n","---->Test set 5: Average loss: 0.1488, Accuracy: 1084/10000 (11%)\n","---->Test set 6: Average loss: 0.1650, Accuracy: 1070/10000 (11%)\n","---->Test set 7: Average loss: 0.1542, Accuracy: 1341/10000 (13%)\n","---->Test set 8: Average loss: 0.1617, Accuracy: 669/10000 (7%)\n","---->Test set 9: Average loss: 0.1588, Accuracy: 656/10000 (7%)\n","--> Training Task 0:\n","----> Epoch 0:\n","---->[0/60000 (0%)]\tPrecision: 0.046875\tLoss: 169.323257\tSurrogate Loss: 0.000000\tTotal Loss: 169.323257\n","---->[12800/60000 (21%)]\tPrecision: 0.734375\tLoss: 0.937317\tSurrogate Loss: 0.000000\tTotal Loss: 0.937317\n","---->[25600/60000 (43%)]\tPrecision: 0.906250\tLoss: 0.624988\tSurrogate Loss: 0.000000\tTotal Loss: 0.624988\n","---->[38400/60000 (64%)]\tPrecision: 0.921875\tLoss: 0.178729\tSurrogate Loss: 0.000000\tTotal Loss: 0.178729\n","---->[51200/60000 (85%)]\tPrecision: 0.906250\tLoss: 0.443689\tSurrogate Loss: 0.000000\tTotal Loss: 0.443689\n","[169.32325744628906, 0.9373174905776978, 0.6249877214431763, 0.1787286251783371, 0.44368916749954224]\n","[169.32325744628906, 0.9373174905776978, 0.6249877214431763, 0.1787286251783371, 0.44368916749954224]\n","--> Finished Training Task 0. Starting Test phase:\n","---->Test set 0: Average loss: 0.0002, Accuracy: 9393/10000 (94%)\n","---->Test set 1: Average loss: 0.0044, Accuracy: 977/10000 (10%)\n","---->Test set 2: Average loss: 0.0048, Accuracy: 1234/10000 (12%)\n","---->Test set 3: Average loss: 0.0048, Accuracy: 1127/10000 (11%)\n","---->Test set 4: Average loss: 0.0042, Accuracy: 1137/10000 (11%)\n","---->Test set 5: Average loss: 0.0049, Accuracy: 1330/10000 (13%)\n","---->Test set 6: Average loss: 0.0057, Accuracy: 595/10000 (6%)\n","---->Test set 7: Average loss: 0.0071, Accuracy: 1046/10000 (10%)\n","---->Test set 8: Average loss: 0.0049, Accuracy: 839/10000 (8%)\n","---->Test set 9: Average loss: 0.0040, Accuracy: 963/10000 (10%)\n","--> Training Task 1:\n","----> Epoch 0:\n","---->[0/60000 (0%)]\tPrecision: 0.093750\tLoss: 5.371215\tSurrogate Loss: 0.000000\tTotal Loss: 5.371215\n","---->[12800/60000 (21%)]\tPrecision: 0.875000\tLoss: 0.576670\tSurrogate Loss: 7.119407\tTotal Loss: 1.288611\n","---->[25600/60000 (43%)]\tPrecision: 0.937500\tLoss: 0.140112\tSurrogate Loss: 3.743314\tTotal Loss: 0.514443\n","---->[38400/60000 (64%)]\tPrecision: 0.859375\tLoss: 0.556322\tSurrogate Loss: 2.736056\tTotal Loss: 0.829927\n","---->[51200/60000 (85%)]\tPrecision: 0.984375\tLoss: 0.064555\tSurrogate Loss: 2.730923\tTotal Loss: 0.337648\n","[5.371215343475342, 0.5766701698303223, 0.14011190831661224, 0.5563216805458069, 0.06455538421869278]\n","[5.371215343475342, 1.2886109352111816, 0.5144433379173279, 0.8299272060394287, 0.3376476764678955]\n","--> Finished Training Task 1. Starting Test phase:\n","---->Test set 0: Average loss: 0.0004, Accuracy: 8938/10000 (89%)\n","---->Test set 1: Average loss: 0.0004, Accuracy: 8896/10000 (89%)\n","---->Test set 2: Average loss: 0.0035, Accuracy: 862/10000 (9%)\n","---->Test set 3: Average loss: 0.0028, Accuracy: 946/10000 (9%)\n","---->Test set 4: Average loss: 0.0029, Accuracy: 1126/10000 (11%)\n","---->Test set 5: Average loss: 0.0033, Accuracy: 780/10000 (8%)\n","---->Test set 6: Average loss: 0.0028, Accuracy: 975/10000 (10%)\n","---->Test set 7: Average loss: 0.0029, Accuracy: 1515/10000 (15%)\n","---->Test set 8: Average loss: 0.0036, Accuracy: 959/10000 (10%)\n","---->Test set 9: Average loss: 0.0030, Accuracy: 861/10000 (9%)\n","--> Training Task 2:\n","----> Epoch 0:\n","---->[0/60000 (0%)]\tPrecision: 0.140625\tLoss: 3.261069\tSurrogate Loss: 0.000000\tTotal Loss: 3.261069\n","---->[12800/60000 (21%)]\tPrecision: 0.781250\tLoss: 0.681391\tSurrogate Loss: 4.179596\tTotal Loss: 1.099350\n","---->[25600/60000 (43%)]\tPrecision: 0.859375\tLoss: 0.359633\tSurrogate Loss: 3.534851\tTotal Loss: 0.713118\n","---->[38400/60000 (64%)]\tPrecision: 0.875000\tLoss: 0.292800\tSurrogate Loss: 2.957813\tTotal Loss: 0.588581\n","---->[51200/60000 (85%)]\tPrecision: 0.937500\tLoss: 0.171656\tSurrogate Loss: 2.892316\tTotal Loss: 0.460888\n","[3.2610692977905273, 0.6813905239105225, 0.35963305830955505, 0.29280006885528564, 0.1716560423374176]\n","[3.2610692977905273, 1.099350094795227, 0.7131181359291077, 0.5885814428329468, 0.4608876705169678]\n","--> Finished Training Task 2. Starting Test phase:\n","---->Test set 0: Average loss: 0.0006, Accuracy: 8536/10000 (85%)\n","---->Test set 1: Average loss: 0.0004, Accuracy: 8841/10000 (88%)\n","---->Test set 2: Average loss: 0.0003, Accuracy: 9184/10000 (92%)\n","---->Test set 3: Average loss: 0.0037, Accuracy: 792/10000 (8%)\n","---->Test set 4: Average loss: 0.0026, Accuracy: 1587/10000 (16%)\n","---->Test set 5: Average loss: 0.0028, Accuracy: 1018/10000 (10%)\n","---->Test set 6: Average loss: 0.0027, Accuracy: 969/10000 (10%)\n","---->Test set 7: Average loss: 0.0027, Accuracy: 1056/10000 (11%)\n","---->Test set 8: Average loss: 0.0036, Accuracy: 1041/10000 (10%)\n","---->Test set 9: Average loss: 0.0031, Accuracy: 805/10000 (8%)\n","--> Training Task 3:\n","----> Epoch 0:\n","---->[0/60000 (0%)]\tPrecision: 0.031250\tLoss: 3.841540\tSurrogate Loss: 0.000000\tTotal Loss: 3.841540\n","---->[12800/60000 (21%)]\tPrecision: 0.859375\tLoss: 0.521981\tSurrogate Loss: 5.661264\tTotal Loss: 1.088108\n","---->[25600/60000 (43%)]\tPrecision: 0.890625\tLoss: 0.530961\tSurrogate Loss: 5.439130\tTotal Loss: 1.074874\n","---->[38400/60000 (64%)]\tPrecision: 0.906250\tLoss: 0.347931\tSurrogate Loss: 3.877522\tTotal Loss: 0.735683\n","---->[51200/60000 (85%)]\tPrecision: 0.921875\tLoss: 0.504004\tSurrogate Loss: 4.366496\tTotal Loss: 0.940653\n","[3.8415396213531494, 0.5219813585281372, 0.5309610962867737, 0.3479308784008026, 0.5040035843849182]\n","[3.8415396213531494, 1.0881078243255615, 1.0748741626739502, 0.735683023929596, 0.9406532049179077]\n","--> Finished Training Task 3. Starting Test phase:\n","---->Test set 0: Average loss: 0.0007, Accuracy: 8059/10000 (81%)\n","---->Test set 1: Average loss: 0.0004, Accuracy: 8787/10000 (88%)\n","---->Test set 2: Average loss: 0.0003, Accuracy: 9090/10000 (91%)\n","---->Test set 3: Average loss: 0.0003, Accuracy: 9104/10000 (91%)\n","---->Test set 4: Average loss: 0.0029, Accuracy: 1399/10000 (14%)\n","---->Test set 5: Average loss: 0.0026, Accuracy: 1088/10000 (11%)\n","---->Test set 6: Average loss: 0.0029, Accuracy: 980/10000 (10%)\n","---->Test set 7: Average loss: 0.0028, Accuracy: 962/10000 (10%)\n","---->Test set 8: Average loss: 0.0031, Accuracy: 1142/10000 (11%)\n","---->Test set 9: Average loss: 0.0033, Accuracy: 1250/10000 (12%)\n","--> Training Task 4:\n","----> Epoch 0:\n","---->[0/60000 (0%)]\tPrecision: 0.203125\tLoss: 2.888780\tSurrogate Loss: 0.000000\tTotal Loss: 2.888780\n","---->[12800/60000 (21%)]\tPrecision: 0.921875\tLoss: 0.259882\tSurrogate Loss: 6.575459\tTotal Loss: 0.917428\n","---->[25600/60000 (43%)]\tPrecision: 0.828125\tLoss: 0.658500\tSurrogate Loss: 5.776845\tTotal Loss: 1.236185\n","---->[38400/60000 (64%)]\tPrecision: 0.765625\tLoss: 1.154305\tSurrogate Loss: 6.841544\tTotal Loss: 1.838459\n","---->[51200/60000 (85%)]\tPrecision: 0.906250\tLoss: 0.499703\tSurrogate Loss: 5.050108\tTotal Loss: 1.004713\n","[2.888779640197754, 0.25988203287124634, 0.6585001945495605, 1.1543048620224, 0.499702513217926]\n","[2.888779640197754, 0.9174278974533081, 1.2361847162246704, 1.8384592533111572, 1.0047132968902588]\n","--> Finished Training Task 4. Starting Test phase:\n","---->Test set 0: Average loss: 0.0008, Accuracy: 7864/10000 (79%)\n","---->Test set 1: Average loss: 0.0003, Accuracy: 8984/10000 (90%)\n","---->Test set 2: Average loss: 0.0003, Accuracy: 9211/10000 (92%)\n","---->Test set 3: Average loss: 0.0003, Accuracy: 9274/10000 (93%)\n","---->Test set 4: Average loss: 0.0003, Accuracy: 9263/10000 (93%)\n","---->Test set 5: Average loss: 0.0027, Accuracy: 1075/10000 (11%)\n","---->Test set 6: Average loss: 0.0030, Accuracy: 1049/10000 (10%)\n","---->Test set 7: Average loss: 0.0030, Accuracy: 996/10000 (10%)\n","---->Test set 8: Average loss: 0.0031, Accuracy: 1175/10000 (12%)\n","---->Test set 9: Average loss: 0.0031, Accuracy: 994/10000 (10%)\n","--> Training Task 5:\n","----> Epoch 0:\n","---->[0/60000 (0%)]\tPrecision: 0.031250\tLoss: 2.849074\tSurrogate Loss: 0.000000\tTotal Loss: 2.849074\n","---->[12800/60000 (21%)]\tPrecision: 0.921875\tLoss: 0.333496\tSurrogate Loss: 7.920310\tTotal Loss: 1.125527\n","---->[25600/60000 (43%)]\tPrecision: 0.953125\tLoss: 0.123024\tSurrogate Loss: 5.346536\tTotal Loss: 0.657678\n","---->[38400/60000 (64%)]\tPrecision: 0.921875\tLoss: 0.165699\tSurrogate Loss: 4.623848\tTotal Loss: 0.628084\n","---->[51200/60000 (85%)]\tPrecision: 0.906250\tLoss: 0.303278\tSurrogate Loss: 6.229165\tTotal Loss: 0.926194\n","[2.849073886871338, 0.33349576592445374, 0.12302423268556595, 0.16569872200489044, 0.3032777011394501]\n","[2.849073886871338, 1.125526785850525, 0.6576778292655945, 0.6280835866928101, 0.9261941909790039]\n","--> Finished Training Task 5. Starting Test phase:\n","---->Test set 0: Average loss: 0.0008, Accuracy: 7891/10000 (79%)\n","---->Test set 1: Average loss: 0.0004, Accuracy: 8951/10000 (90%)\n","---->Test set 2: Average loss: 0.0003, Accuracy: 9103/10000 (91%)\n","---->Test set 3: Average loss: 0.0004, Accuracy: 8924/10000 (89%)\n","---->Test set 4: Average loss: 0.0003, Accuracy: 9064/10000 (91%)\n","---->Test set 5: Average loss: 0.0003, Accuracy: 9018/10000 (90%)\n","---->Test set 6: Average loss: 0.0034, Accuracy: 1136/10000 (11%)\n","---->Test set 7: Average loss: 0.0032, Accuracy: 860/10000 (9%)\n","---->Test set 8: Average loss: 0.0035, Accuracy: 827/10000 (8%)\n","---->Test set 9: Average loss: 0.0036, Accuracy: 684/10000 (7%)\n","--> Training Task 6:\n","----> Epoch 0:\n","---->[0/60000 (0%)]\tPrecision: 0.140625\tLoss: 3.058806\tSurrogate Loss: 0.000000\tTotal Loss: 3.058806\n","---->[12800/60000 (21%)]\tPrecision: 0.812500\tLoss: 0.487455\tSurrogate Loss: 12.330570\tTotal Loss: 1.720512\n","---->[25600/60000 (43%)]\tPrecision: 0.968750\tLoss: 0.116807\tSurrogate Loss: 7.409074\tTotal Loss: 0.857715\n","---->[38400/60000 (64%)]\tPrecision: 0.875000\tLoss: 0.347706\tSurrogate Loss: 6.515062\tTotal Loss: 0.999212\n","---->[51200/60000 (85%)]\tPrecision: 0.890625\tLoss: 0.577529\tSurrogate Loss: 6.122700\tTotal Loss: 1.189799\n","[3.0588064193725586, 0.48745453357696533, 0.11680708825588226, 0.34770581126213074, 0.5775289535522461]\n","[3.0588064193725586, 1.720511555671692, 0.8577145338058472, 0.9992120265960693, 1.1897990703582764]\n","--> Finished Training Task 6. Starting Test phase:\n","---->Test set 0: Average loss: 0.0008, Accuracy: 7746/10000 (77%)\n","---->Test set 1: Average loss: 0.0004, Accuracy: 8904/10000 (89%)\n","---->Test set 2: Average loss: 0.0003, Accuracy: 9205/10000 (92%)\n","---->Test set 3: Average loss: 0.0004, Accuracy: 8839/10000 (88%)\n","---->Test set 4: Average loss: 0.0004, Accuracy: 8950/10000 (90%)\n","---->Test set 5: Average loss: 0.0003, Accuracy: 9024/10000 (90%)\n","---->Test set 6: Average loss: 0.0004, Accuracy: 8968/10000 (90%)\n","---->Test set 7: Average loss: 0.0030, Accuracy: 875/10000 (9%)\n","---->Test set 8: Average loss: 0.0034, Accuracy: 816/10000 (8%)\n","---->Test set 9: Average loss: 0.0033, Accuracy: 786/10000 (8%)\n","--> Training Task 7:\n","----> Epoch 0:\n","---->[0/60000 (0%)]\tPrecision: 0.031250\tLoss: 3.359275\tSurrogate Loss: 0.000000\tTotal Loss: 3.359275\n","---->[12800/60000 (21%)]\tPrecision: 0.875000\tLoss: 0.391758\tSurrogate Loss: 8.928425\tTotal Loss: 1.284600\n","---->[25600/60000 (43%)]\tPrecision: 0.765625\tLoss: 0.569685\tSurrogate Loss: 9.640586\tTotal Loss: 1.533744\n","---->[38400/60000 (64%)]\tPrecision: 0.921875\tLoss: 0.292332\tSurrogate Loss: 8.435377\tTotal Loss: 1.135870\n","---->[51200/60000 (85%)]\tPrecision: 0.906250\tLoss: 0.338098\tSurrogate Loss: 4.976379\tTotal Loss: 0.835736\n","[3.3592751026153564, 0.3917577564716339, 0.5696853399276733, 0.2923320233821869, 0.33809831738471985]\n","[3.3592751026153564, 1.2846002578735352, 1.5337438583374023, 1.1358697414398193, 0.8357362151145935]\n","--> Finished Training Task 7. Starting Test phase:\n","---->Test set 0: Average loss: 0.0010, Accuracy: 6970/10000 (70%)\n","---->Test set 1: Average loss: 0.0004, Accuracy: 8939/10000 (89%)\n","---->Test set 2: Average loss: 0.0003, Accuracy: 9075/10000 (91%)\n","---->Test set 3: Average loss: 0.0004, Accuracy: 8648/10000 (86%)\n","---->Test set 4: Average loss: 0.0004, Accuracy: 8726/10000 (87%)\n","---->Test set 5: Average loss: 0.0004, Accuracy: 8809/10000 (88%)\n","---->Test set 6: Average loss: 0.0004, Accuracy: 8674/10000 (87%)\n","---->Test set 7: Average loss: 0.0004, Accuracy: 8895/10000 (89%)\n","---->Test set 8: Average loss: 0.0030, Accuracy: 1177/10000 (12%)\n","---->Test set 9: Average loss: 0.0029, Accuracy: 855/10000 (9%)\n","--> Training Task 8:\n","----> Epoch 0:\n","---->[0/60000 (0%)]\tPrecision: 0.171875\tLoss: 3.187515\tSurrogate Loss: 0.000000\tTotal Loss: 3.187515\n","---->[12800/60000 (21%)]\tPrecision: 0.875000\tLoss: 0.731200\tSurrogate Loss: 11.368505\tTotal Loss: 1.868051\n","---->[25600/60000 (43%)]\tPrecision: 0.890625\tLoss: 0.361915\tSurrogate Loss: 7.783405\tTotal Loss: 1.140255\n","---->[38400/60000 (64%)]\tPrecision: 0.937500\tLoss: 0.189995\tSurrogate Loss: 4.896926\tTotal Loss: 0.679688\n","---->[51200/60000 (85%)]\tPrecision: 0.875000\tLoss: 0.461946\tSurrogate Loss: 6.724432\tTotal Loss: 1.134390\n","[3.1875150203704834, 0.7312002778053284, 0.36191487312316895, 0.18999522924423218, 0.4619463384151459]\n","[3.1875150203704834, 1.8680508136749268, 1.1402554512023926, 0.6796878576278687, 1.1343895196914673]\n","--> Finished Training Task 8. Starting Test phase:\n","---->Test set 0: Average loss: 0.0010, Accuracy: 6737/10000 (67%)\n","---->Test set 1: Average loss: 0.0004, Accuracy: 8708/10000 (87%)\n","---->Test set 2: Average loss: 0.0003, Accuracy: 9010/10000 (90%)\n","---->Test set 3: Average loss: 0.0004, Accuracy: 8552/10000 (86%)\n","---->Test set 4: Average loss: 0.0004, Accuracy: 8664/10000 (87%)\n","---->Test set 5: Average loss: 0.0005, Accuracy: 8489/10000 (85%)\n","---->Test set 6: Average loss: 0.0005, Accuracy: 8577/10000 (86%)\n","---->Test set 7: Average loss: 0.0005, Accuracy: 8673/10000 (87%)\n","---->Test set 8: Average loss: 0.0003, Accuracy: 9108/10000 (91%)\n","---->Test set 9: Average loss: 0.0033, Accuracy: 954/10000 (10%)\n","--> Training Task 9:\n","----> Epoch 0:\n","---->[0/60000 (0%)]\tPrecision: 0.078125\tLoss: 3.332283\tSurrogate Loss: 0.000000\tTotal Loss: 3.332283\n","---->[12800/60000 (21%)]\tPrecision: 0.828125\tLoss: 0.540048\tSurrogate Loss: 9.380959\tTotal Loss: 1.478144\n","---->[25600/60000 (43%)]\tPrecision: 0.812500\tLoss: 0.575529\tSurrogate Loss: 8.239007\tTotal Loss: 1.399430\n","---->[38400/60000 (64%)]\tPrecision: 0.953125\tLoss: 0.344588\tSurrogate Loss: 8.671398\tTotal Loss: 1.211727\n","---->[51200/60000 (85%)]\tPrecision: 0.859375\tLoss: 0.375534\tSurrogate Loss: 6.976986\tTotal Loss: 1.073233\n","[3.332282781600952, 0.5400484800338745, 0.5755289196968079, 0.3445875644683838, 0.37553417682647705]\n","[3.332282781600952, 1.4781444072723389, 1.3994295597076416, 1.2117273807525635, 1.073232889175415]\n","--> Finished Training Task 9. Starting Test phase:\n","---->Test set 0: Average loss: 0.0011, Accuracy: 6409/10000 (64%)\n","---->Test set 1: Average loss: 0.0005, Accuracy: 8747/10000 (87%)\n","---->Test set 2: Average loss: 0.0004, Accuracy: 8969/10000 (90%)\n","---->Test set 3: Average loss: 0.0005, Accuracy: 8603/10000 (86%)\n","---->Test set 4: Average loss: 0.0004, Accuracy: 8810/10000 (88%)\n","---->Test set 5: Average loss: 0.0006, Accuracy: 8336/10000 (83%)\n","---->Test set 6: Average loss: 0.0006, Accuracy: 8459/10000 (85%)\n","---->Test set 7: Average loss: 0.0005, Accuracy: 8753/10000 (88%)\n","---->Test set 8: Average loss: 0.0003, Accuracy: 9095/10000 (91%)\n","---->Test set 9: Average loss: 0.0004, Accuracy: 8929/10000 (89%)\n","Accuracy: 0.8511\n","Confusion matrix:\n","0.0774,0.0882,0.0878,0.1173,0.083,0.1084,0.107,0.1341,0.0669,0.0656\n","0.9393,0.0977,0.1234,0.1127,0.1137,0.133,0.0595,0.1046,0.0839,0.0963\n","0.8938,0.8896,0.0862,0.0946,0.1126,0.078,0.0975,0.1515,0.0959,0.0861\n","0.8536,0.8841,0.9184,0.0792,0.1587,0.1018,0.0969,0.1056,0.1041,0.0805\n","0.8059,0.8787,0.909,0.9104,0.1399,0.1088,0.098,0.0962,0.1142,0.125\n","0.7864,0.8984,0.9211,0.9274,0.9263,0.1075,0.1049,0.0996,0.1175,0.0994\n","0.7891,0.8951,0.9103,0.8924,0.9064,0.9018,0.1136,0.086,0.0827,0.0684\n","0.7746,0.8904,0.9205,0.8839,0.895,0.9024,0.8968,0.0875,0.0816,0.0786\n","0.697,0.8939,0.9075,0.8648,0.8726,0.8809,0.8674,0.8895,0.1177,0.0855\n","0.6737,0.8708,0.901,0.8552,0.8664,0.8489,0.8577,0.8673,0.9108,0.0954\n","0.6409,0.8747,0.8969,0.8603,0.881,0.8336,0.8459,0.8753,0.9095,0.8929\n","--> Training:\n","---->Test set 0: Average loss: 0.1989, Accuracy: 953/10000 (10%)\n","---->Test set 1: Average loss: 0.2087, Accuracy: 1111/10000 (11%)\n","---->Test set 2: Average loss: 0.2002, Accuracy: 658/10000 (7%)\n","---->Test set 3: Average loss: 0.2267, Accuracy: 714/10000 (7%)\n","---->Test set 4: Average loss: 0.1690, Accuracy: 1305/10000 (13%)\n","---->Test set 5: Average loss: 0.1409, Accuracy: 1419/10000 (14%)\n","---->Test set 6: Average loss: 0.2227, Accuracy: 1550/10000 (16%)\n","---->Test set 7: Average loss: 0.1814, Accuracy: 637/10000 (6%)\n","---->Test set 8: Average loss: 0.2038, Accuracy: 813/10000 (8%)\n","---->Test set 9: Average loss: 0.1700, Accuracy: 1407/10000 (14%)\n","--> Training Task 0:\n","----> Epoch 0:\n","---->[0/60000 (0%)]\tPrecision: 0.046875\tLoss: 223.413803\tSurrogate Loss: 0.000000\tTotal Loss: 223.413803\n","---->[12800/60000 (21%)]\tPrecision: 0.953125\tLoss: 0.279138\tSurrogate Loss: 0.000000\tTotal Loss: 0.279138\n","---->[25600/60000 (43%)]\tPrecision: 0.890625\tLoss: 0.410596\tSurrogate Loss: 0.000000\tTotal Loss: 0.410596\n","---->[38400/60000 (64%)]\tPrecision: 0.968750\tLoss: 0.060598\tSurrogate Loss: 0.000000\tTotal Loss: 0.060598\n","---->[51200/60000 (85%)]\tPrecision: 0.921875\tLoss: 0.218499\tSurrogate Loss: 0.000000\tTotal Loss: 0.218499\n","----> Epoch 1:\n","---->[0/60000 (0%)]\tPrecision: 0.921875\tLoss: 0.235684\tSurrogate Loss: 0.000000\tTotal Loss: 0.235684\n","---->[12800/60000 (21%)]\tPrecision: 0.953125\tLoss: 0.178521\tSurrogate Loss: 0.000000\tTotal Loss: 0.178521\n","---->[25600/60000 (43%)]\tPrecision: 0.921875\tLoss: 0.212530\tSurrogate Loss: 0.000000\tTotal Loss: 0.212530\n","---->[38400/60000 (64%)]\tPrecision: 0.968750\tLoss: 0.082947\tSurrogate Loss: 0.000000\tTotal Loss: 0.082947\n","---->[51200/60000 (85%)]\tPrecision: 0.968750\tLoss: 0.092649\tSurrogate Loss: 0.000000\tTotal Loss: 0.092649\n","[223.41380310058594, 0.2791377305984497, 0.41059568524360657, 0.060598224401474, 0.21849872171878815, 0.23568417131900787, 0.17852114140987396, 0.21253010630607605, 0.08294734358787537, 0.09264949709177017]\n","[223.41380310058594, 0.2791377305984497, 0.41059568524360657, 0.060598224401474, 0.21849872171878815, 0.23568417131900787, 0.17852114140987396, 0.21253010630607605, 0.08294734358787537, 0.09264949709177017]\n","--> Finished Training Task 0. Starting Test phase:\n","---->Test set 0: Average loss: 0.0002, Accuracy: 9563/10000 (96%)\n","---->Test set 1: Average loss: 0.0050, Accuracy: 633/10000 (6%)\n","---->Test set 2: Average loss: 0.0062, Accuracy: 1020/10000 (10%)\n","---->Test set 3: Average loss: 0.0059, Accuracy: 1157/10000 (12%)\n","---->Test set 4: Average loss: 0.0055, Accuracy: 842/10000 (8%)\n","---->Test set 5: Average loss: 0.0051, Accuracy: 1390/10000 (14%)\n","---->Test set 6: Average loss: 0.0075, Accuracy: 1039/10000 (10%)\n","---->Test set 7: Average loss: 0.0062, Accuracy: 1344/10000 (13%)\n","---->Test set 8: Average loss: 0.0068, Accuracy: 1009/10000 (10%)\n","---->Test set 9: Average loss: 0.0061, Accuracy: 761/10000 (8%)\n","--> Training Task 1:\n","----> Epoch 0:\n","---->[0/60000 (0%)]\tPrecision: 0.015625\tLoss: 5.499839\tSurrogate Loss: 0.000000\tTotal Loss: 5.499839\n","---->[12800/60000 (21%)]\tPrecision: 0.906250\tLoss: 0.196328\tSurrogate Loss: 4.683189\tTotal Loss: 0.664647\n","---->[25600/60000 (43%)]\tPrecision: 0.953125\tLoss: 0.170241\tSurrogate Loss: 3.024548\tTotal Loss: 0.472696\n","---->[38400/60000 (64%)]\tPrecision: 0.921875\tLoss: 0.273455\tSurrogate Loss: 2.356919\tTotal Loss: 0.509147\n","---->[51200/60000 (85%)]\tPrecision: 0.968750\tLoss: 0.160323\tSurrogate Loss: 2.805161\tTotal Loss: 0.440839\n","----> Epoch 1:\n","---->[0/60000 (0%)]\tPrecision: 0.968750\tLoss: 0.177318\tSurrogate Loss: 2.953312\tTotal Loss: 0.472649\n","---->[12800/60000 (21%)]\tPrecision: 0.921875\tLoss: 0.444858\tSurrogate Loss: 2.567004\tTotal Loss: 0.701558\n","---->[25600/60000 (43%)]\tPrecision: 0.968750\tLoss: 0.225785\tSurrogate Loss: 3.506880\tTotal Loss: 0.576473\n","---->[38400/60000 (64%)]\tPrecision: 0.921875\tLoss: 0.480471\tSurrogate Loss: 5.269401\tTotal Loss: 1.007412\n","---->[51200/60000 (85%)]\tPrecision: 0.953125\tLoss: 0.217279\tSurrogate Loss: 4.864540\tTotal Loss: 0.703733\n","[5.499838829040527, 0.19632841646671295, 0.1702410727739334, 0.27345508337020874, 0.16032321751117706, 0.17731773853302002, 0.4448575973510742, 0.2257852852344513, 0.48047149181365967, 0.21727867424488068]\n","[5.499838829040527, 0.6646473407745361, 0.47269588708877563, 0.5091469883918762, 0.44083935022354126, 0.472648948431015, 0.701557993888855, 0.5764732360839844, 1.0074115991592407, 0.7037326693534851]\n","--> Finished Training Task 1. Starting Test phase:\n","---->Test set 0: Average loss: 0.0002, Accuracy: 9404/10000 (94%)\n","---->Test set 1: Average loss: 0.0002, Accuracy: 9480/10000 (95%)\n","---->Test set 2: Average loss: 0.0031, Accuracy: 1179/10000 (12%)\n","---->Test set 3: Average loss: 0.0037, Accuracy: 829/10000 (8%)\n","---->Test set 4: Average loss: 0.0026, Accuracy: 1406/10000 (14%)\n","---->Test set 5: Average loss: 0.0029, Accuracy: 1378/10000 (14%)\n","---->Test set 6: Average loss: 0.0031, Accuracy: 1217/10000 (12%)\n","---->Test set 7: Average loss: 0.0031, Accuracy: 1165/10000 (12%)\n","---->Test set 8: Average loss: 0.0033, Accuracy: 984/10000 (10%)\n","---->Test set 9: Average loss: 0.0032, Accuracy: 855/10000 (9%)\n","--> Training Task 2:\n","----> Epoch 0:\n","---->[0/60000 (0%)]\tPrecision: 0.093750\tLoss: 3.011666\tSurrogate Loss: 0.000000\tTotal Loss: 3.011666\n","---->[12800/60000 (21%)]\tPrecision: 0.843750\tLoss: 0.797179\tSurrogate Loss: 6.189679\tTotal Loss: 1.416147\n","---->[25600/60000 (43%)]\tPrecision: 0.828125\tLoss: 0.579781\tSurrogate Loss: 3.381188\tTotal Loss: 0.917899\n","---->[38400/60000 (64%)]\tPrecision: 0.953125\tLoss: 0.212340\tSurrogate Loss: 3.076957\tTotal Loss: 0.520036\n","---->[51200/60000 (85%)]\tPrecision: 0.937500\tLoss: 0.191594\tSurrogate Loss: 2.759406\tTotal Loss: 0.467534\n","----> Epoch 1:\n","---->[0/60000 (0%)]\tPrecision: 0.890625\tLoss: 0.188883\tSurrogate Loss: 2.950094\tTotal Loss: 0.483893\n","---->[12800/60000 (21%)]\tPrecision: 0.921875\tLoss: 0.282800\tSurrogate Loss: 4.290951\tTotal Loss: 0.711895\n","---->[25600/60000 (43%)]\tPrecision: 0.937500\tLoss: 0.243259\tSurrogate Loss: 2.206352\tTotal Loss: 0.463894\n","---->[38400/60000 (64%)]\tPrecision: 0.921875\tLoss: 0.298597\tSurrogate Loss: 5.170120\tTotal Loss: 0.815609\n","---->[51200/60000 (85%)]\tPrecision: 0.921875\tLoss: 0.359871\tSurrogate Loss: 2.972075\tTotal Loss: 0.657079\n","[3.0116658210754395, 0.7971788644790649, 0.5797805786132812, 0.2123398780822754, 0.19159384071826935, 0.18888311088085175, 0.2828003764152527, 0.24325907230377197, 0.29859739542007446, 0.35987094044685364]\n","[3.0116658210754395, 1.4161467552185059, 0.9178993701934814, 0.5200355052947998, 0.46753448247909546, 0.483892560005188, 0.7118954658508301, 0.4638943076133728, 0.8156094551086426, 0.6570785045623779]\n","--> Finished Training Task 2. Starting Test phase:\n","---->Test set 0: Average loss: 0.0003, Accuracy: 9216/10000 (92%)\n","---->Test set 1: Average loss: 0.0003, Accuracy: 9327/10000 (93%)\n","---->Test set 2: Average loss: 0.0003, Accuracy: 9235/10000 (92%)\n","---->Test set 3: Average loss: 0.0054, Accuracy: 775/10000 (8%)\n","---->Test set 4: Average loss: 0.0030, Accuracy: 1325/10000 (13%)\n","---->Test set 5: Average loss: 0.0030, Accuracy: 834/10000 (8%)\n","---->Test set 6: Average loss: 0.0032, Accuracy: 977/10000 (10%)\n","---->Test set 7: Average loss: 0.0029, Accuracy: 1093/10000 (11%)\n","---->Test set 8: Average loss: 0.0031, Accuracy: 888/10000 (9%)\n","---->Test set 9: Average loss: 0.0033, Accuracy: 564/10000 (6%)\n","--> Training Task 3:\n","----> Epoch 0:\n","---->[0/60000 (0%)]\tPrecision: 0.093750\tLoss: 4.936688\tSurrogate Loss: 0.000000\tTotal Loss: 4.936688\n","---->[12800/60000 (21%)]\tPrecision: 0.906250\tLoss: 0.646032\tSurrogate Loss: 7.741037\tTotal Loss: 1.420136\n","---->[25600/60000 (43%)]\tPrecision: 0.812500\tLoss: 0.649817\tSurrogate Loss: 5.561268\tTotal Loss: 1.205944\n","---->[38400/60000 (64%)]\tPrecision: 0.906250\tLoss: 0.291562\tSurrogate Loss: 3.939320\tTotal Loss: 0.685494\n","---->[51200/60000 (85%)]\tPrecision: 0.937500\tLoss: 0.270137\tSurrogate Loss: 3.255161\tTotal Loss: 0.595653\n","----> Epoch 1:\n","---->[0/60000 (0%)]\tPrecision: 0.937500\tLoss: 0.278591\tSurrogate Loss: 3.051631\tTotal Loss: 0.583755\n","---->[12800/60000 (21%)]\tPrecision: 0.984375\tLoss: 0.069525\tSurrogate Loss: 3.271923\tTotal Loss: 0.396718\n","---->[25600/60000 (43%)]\tPrecision: 0.875000\tLoss: 0.464460\tSurrogate Loss: 6.861908\tTotal Loss: 1.150651\n","---->[38400/60000 (64%)]\tPrecision: 0.968750\tLoss: 0.108073\tSurrogate Loss: 3.006951\tTotal Loss: 0.408769\n","---->[51200/60000 (85%)]\tPrecision: 0.890625\tLoss: 0.240409\tSurrogate Loss: 2.836195\tTotal Loss: 0.524028\n","[4.93668794631958, 0.6460321545600891, 0.6498167514801025, 0.29156219959259033, 0.27013731002807617, 0.27859142422676086, 0.06952549517154694, 0.46445995569229126, 0.10807338356971741, 0.24040868878364563]\n","[4.93668794631958, 1.4201359748840332, 1.2059435844421387, 0.6854941844940186, 0.5956534147262573, 0.5837545394897461, 0.39671778678894043, 1.1506507396697998, 0.40876850485801697, 0.5240281820297241]\n","--> Finished Training Task 3. Starting Test phase:\n","---->Test set 0: Average loss: 0.0003, Accuracy: 9131/10000 (91%)\n","---->Test set 1: Average loss: 0.0003, Accuracy: 9322/10000 (93%)\n","---->Test set 2: Average loss: 0.0003, Accuracy: 9188/10000 (92%)\n","---->Test set 3: Average loss: 0.0002, Accuracy: 9424/10000 (94%)\n","---->Test set 4: Average loss: 0.0028, Accuracy: 1402/10000 (14%)\n","---->Test set 5: Average loss: 0.0028, Accuracy: 859/10000 (9%)\n","---->Test set 6: Average loss: 0.0029, Accuracy: 978/10000 (10%)\n","---->Test set 7: Average loss: 0.0027, Accuracy: 1159/10000 (12%)\n","---->Test set 8: Average loss: 0.0030, Accuracy: 1033/10000 (10%)\n","---->Test set 9: Average loss: 0.0031, Accuracy: 836/10000 (8%)\n","--> Training Task 4:\n","----> Epoch 0:\n","---->[0/60000 (0%)]\tPrecision: 0.187500\tLoss: 2.701632\tSurrogate Loss: 0.000000\tTotal Loss: 2.701632\n","---->[12800/60000 (21%)]\tPrecision: 0.984375\tLoss: 0.221880\tSurrogate Loss: 6.625996\tTotal Loss: 0.884480\n","---->[25600/60000 (43%)]\tPrecision: 0.968750\tLoss: 0.153915\tSurrogate Loss: 6.059962\tTotal Loss: 0.759911\n","---->[38400/60000 (64%)]\tPrecision: 0.859375\tLoss: 0.725534\tSurrogate Loss: 6.270594\tTotal Loss: 1.352593\n","---->[51200/60000 (85%)]\tPrecision: 0.984375\tLoss: 0.065725\tSurrogate Loss: 5.654055\tTotal Loss: 0.631130\n","----> Epoch 1:\n","---->[0/60000 (0%)]\tPrecision: 0.921875\tLoss: 0.262932\tSurrogate Loss: 4.895267\tTotal Loss: 0.752459\n","---->[12800/60000 (21%)]\tPrecision: 0.921875\tLoss: 0.232662\tSurrogate Loss: 4.098392\tTotal Loss: 0.642501\n","---->[25600/60000 (43%)]\tPrecision: 0.921875\tLoss: 0.180997\tSurrogate Loss: 6.812363\tTotal Loss: 0.862233\n","---->[38400/60000 (64%)]\tPrecision: 0.937500\tLoss: 0.252868\tSurrogate Loss: 4.292089\tTotal Loss: 0.682077\n","---->[51200/60000 (85%)]\tPrecision: 0.937500\tLoss: 0.348244\tSurrogate Loss: 8.399290\tTotal Loss: 1.188173\n","[2.701632499694824, 0.22188010811805725, 0.15391521155834198, 0.7255337238311768, 0.06572488695383072, 0.26293236017227173, 0.2326621562242508, 0.1809970587491989, 0.2528684437274933, 0.34824374318122864]\n","[2.701632499694824, 0.8844796419143677, 0.7599114179611206, 1.352593183517456, 0.6311303973197937, 0.7524590492248535, 0.6425014138221741, 0.8622333407402039, 0.6820772886276245, 1.1881728172302246]\n","--> Finished Training Task 4. Starting Test phase:\n","---->Test set 0: Average loss: 0.0003, Accuracy: 8980/10000 (90%)\n","---->Test set 1: Average loss: 0.0004, Accuracy: 8998/10000 (90%)\n","---->Test set 2: Average loss: 0.0005, Accuracy: 8650/10000 (86%)\n","---->Test set 3: Average loss: 0.0003, Accuracy: 9204/10000 (92%)\n","---->Test set 4: Average loss: 0.0003, Accuracy: 9160/10000 (92%)\n","---->Test set 5: Average loss: 0.0029, Accuracy: 925/10000 (9%)\n","---->Test set 6: Average loss: 0.0032, Accuracy: 760/10000 (8%)\n","---->Test set 7: Average loss: 0.0030, Accuracy: 1147/10000 (11%)\n","---->Test set 8: Average loss: 0.0034, Accuracy: 1168/10000 (12%)\n","---->Test set 9: Average loss: 0.0034, Accuracy: 1061/10000 (11%)\n","--> Training Task 5:\n","----> Epoch 0:\n","---->[0/60000 (0%)]\tPrecision: 0.062500\tLoss: 3.325927\tSurrogate Loss: 0.000000\tTotal Loss: 3.325927\n","---->[12800/60000 (21%)]\tPrecision: 0.812500\tLoss: 0.494114\tSurrogate Loss: 11.272932\tTotal Loss: 1.621407\n","---->[25600/60000 (43%)]\tPrecision: 0.890625\tLoss: 0.329906\tSurrogate Loss: 7.324329\tTotal Loss: 1.062339\n","---->[38400/60000 (64%)]\tPrecision: 0.953125\tLoss: 0.215230\tSurrogate Loss: 6.607822\tTotal Loss: 0.876012\n","---->[51200/60000 (85%)]\tPrecision: 0.875000\tLoss: 0.336198\tSurrogate Loss: 5.777205\tTotal Loss: 0.913918\n","----> Epoch 1:\n","---->[0/60000 (0%)]\tPrecision: 0.890625\tLoss: 0.459801\tSurrogate Loss: 6.931818\tTotal Loss: 1.152983\n","---->[12800/60000 (21%)]\tPrecision: 0.953125\tLoss: 0.106698\tSurrogate Loss: 4.631574\tTotal Loss: 0.569855\n","---->[25600/60000 (43%)]\tPrecision: 0.937500\tLoss: 0.160776\tSurrogate Loss: 9.112947\tTotal Loss: 1.072071\n","---->[38400/60000 (64%)]\tPrecision: 0.906250\tLoss: 0.248766\tSurrogate Loss: 11.929500\tTotal Loss: 1.441716\n","---->[51200/60000 (85%)]\tPrecision: 0.921875\tLoss: 0.219070\tSurrogate Loss: 5.337971\tTotal Loss: 0.752867\n","[3.325927257537842, 0.49411389231681824, 0.3299058675765991, 0.21523015201091766, 0.33619755506515503, 0.4598011374473572, 0.10669773072004318, 0.16077636182308197, 0.24876603484153748, 0.2190699726343155]\n","[3.325927257537842, 1.621407151222229, 1.0623388290405273, 0.8760124444961548, 0.9139180183410645, 1.1529829502105713, 0.5698550939559937, 1.0720710754394531, 1.4417160749435425, 0.7528670430183411]\n","--> Finished Training Task 5. Starting Test phase:\n","---->Test set 0: Average loss: 0.0004, Accuracy: 8918/10000 (89%)\n","---->Test set 1: Average loss: 0.0003, Accuracy: 9233/10000 (92%)\n","---->Test set 2: Average loss: 0.0004, Accuracy: 8858/10000 (89%)\n","---->Test set 3: Average loss: 0.0002, Accuracy: 9347/10000 (93%)\n","---->Test set 4: Average loss: 0.0003, Accuracy: 9205/10000 (92%)\n","---->Test set 5: Average loss: 0.0002, Accuracy: 9315/10000 (93%)\n","---->Test set 6: Average loss: 0.0029, Accuracy: 916/10000 (9%)\n","---->Test set 7: Average loss: 0.0028, Accuracy: 1147/10000 (11%)\n","---->Test set 8: Average loss: 0.0032, Accuracy: 1036/10000 (10%)\n","---->Test set 9: Average loss: 0.0032, Accuracy: 768/10000 (8%)\n","--> Training Task 6:\n","----> Epoch 0:\n","---->[0/60000 (0%)]\tPrecision: 0.109375\tLoss: 3.087404\tSurrogate Loss: 0.000000\tTotal Loss: 3.087404\n","---->[12800/60000 (21%)]\tPrecision: 0.937500\tLoss: 0.316824\tSurrogate Loss: 12.652139\tTotal Loss: 1.582038\n","---->[25600/60000 (43%)]\tPrecision: 0.906250\tLoss: 0.367668\tSurrogate Loss: 9.288736\tTotal Loss: 1.296542\n","---->[38400/60000 (64%)]\tPrecision: 0.843750\tLoss: 0.576680\tSurrogate Loss: 6.644433\tTotal Loss: 1.241123\n","---->[51200/60000 (85%)]\tPrecision: 0.937500\tLoss: 0.191775\tSurrogate Loss: 6.231060\tTotal Loss: 0.814881\n","----> Epoch 1:\n","---->[0/60000 (0%)]\tPrecision: 0.890625\tLoss: 0.393377\tSurrogate Loss: 7.379997\tTotal Loss: 1.131377\n","---->[12800/60000 (21%)]\tPrecision: 0.890625\tLoss: 0.556064\tSurrogate Loss: 6.720446\tTotal Loss: 1.228109\n","---->[25600/60000 (43%)]\tPrecision: 0.921875\tLoss: 0.272459\tSurrogate Loss: 6.723041\tTotal Loss: 0.944763\n","---->[38400/60000 (64%)]\tPrecision: 0.921875\tLoss: 0.210756\tSurrogate Loss: 4.816431\tTotal Loss: 0.692399\n","---->[51200/60000 (85%)]\tPrecision: 0.921875\tLoss: 0.245196\tSurrogate Loss: 5.387130\tTotal Loss: 0.783909\n","[3.0874040126800537, 0.316824346780777, 0.3676682710647583, 0.5766798853874207, 0.19177505373954773, 0.39337679743766785, 0.5560639500617981, 0.2724590301513672, 0.2107556313276291, 0.24519583582878113]\n","[3.0874040126800537, 1.582038164138794, 1.2965419292449951, 1.2411231994628906, 0.8148810863494873, 1.1313765048980713, 1.2281086444854736, 0.9447631239891052, 0.692398726940155, 0.7839088439941406]\n","--> Finished Training Task 6. Starting Test phase:\n","---->Test set 0: Average loss: 0.0004, Accuracy: 8680/10000 (87%)\n","---->Test set 1: Average loss: 0.0003, Accuracy: 9098/10000 (91%)\n","---->Test set 2: Average loss: 0.0005, Accuracy: 8641/10000 (86%)\n","---->Test set 3: Average loss: 0.0003, Accuracy: 9282/10000 (93%)\n","---->Test set 4: Average loss: 0.0003, Accuracy: 9130/10000 (91%)\n","---->Test set 5: Average loss: 0.0003, Accuracy: 9140/10000 (91%)\n","---->Test set 6: Average loss: 0.0002, Accuracy: 9296/10000 (93%)\n","---->Test set 7: Average loss: 0.0031, Accuracy: 1104/10000 (11%)\n","---->Test set 8: Average loss: 0.0032, Accuracy: 1141/10000 (11%)\n","---->Test set 9: Average loss: 0.0032, Accuracy: 948/10000 (9%)\n","--> Training Task 7:\n","----> Epoch 0:\n","---->[0/60000 (0%)]\tPrecision: 0.156250\tLoss: 2.643715\tSurrogate Loss: 0.000000\tTotal Loss: 2.643715\n","---->[12800/60000 (21%)]\tPrecision: 0.843750\tLoss: 0.466464\tSurrogate Loss: 15.332489\tTotal Loss: 1.999713\n","---->[25600/60000 (43%)]\tPrecision: 0.875000\tLoss: 0.487526\tSurrogate Loss: 6.579214\tTotal Loss: 1.145448\n","---->[38400/60000 (64%)]\tPrecision: 0.906250\tLoss: 0.236901\tSurrogate Loss: 7.473656\tTotal Loss: 0.984267\n","---->[51200/60000 (85%)]\tPrecision: 0.984375\tLoss: 0.082296\tSurrogate Loss: 6.925763\tTotal Loss: 0.774873\n","----> Epoch 1:\n","---->[0/60000 (0%)]\tPrecision: 0.921875\tLoss: 0.248326\tSurrogate Loss: 6.735068\tTotal Loss: 0.921833\n","---->[12800/60000 (21%)]\tPrecision: 0.906250\tLoss: 0.377155\tSurrogate Loss: 6.816177\tTotal Loss: 1.058773\n","---->[25600/60000 (43%)]\tPrecision: 0.906250\tLoss: 0.279006\tSurrogate Loss: 5.009562\tTotal Loss: 0.779962\n","---->[38400/60000 (64%)]\tPrecision: 0.859375\tLoss: 0.373007\tSurrogate Loss: 6.966457\tTotal Loss: 1.069653\n","---->[51200/60000 (85%)]\tPrecision: 0.875000\tLoss: 0.329593\tSurrogate Loss: 5.199820\tTotal Loss: 0.849575\n","[2.643714666366577, 0.46646377444267273, 0.4875262975692749, 0.23690131306648254, 0.08229625225067139, 0.24832569062709808, 0.37715548276901245, 0.27900591492652893, 0.3730071485042572, 0.32959306240081787]\n","[2.643714666366577, 1.9997127056121826, 1.1454477310180664, 0.984266996383667, 0.7748726010322571, 0.9218325018882751, 1.058773159980774, 0.7799620628356934, 1.069652795791626, 0.8495750427246094]\n","--> Finished Training Task 7. Starting Test phase:\n","---->Test set 0: Average loss: 0.0005, Accuracy: 8727/10000 (87%)\n","---->Test set 1: Average loss: 0.0003, Accuracy: 9046/10000 (90%)\n","---->Test set 2: Average loss: 0.0004, Accuracy: 8593/10000 (86%)\n","---->Test set 3: Average loss: 0.0003, Accuracy: 9215/10000 (92%)\n","---->Test set 4: Average loss: 0.0004, Accuracy: 8725/10000 (87%)\n","---->Test set 5: Average loss: 0.0004, Accuracy: 8958/10000 (90%)\n","---->Test set 6: Average loss: 0.0002, Accuracy: 9274/10000 (93%)\n","---->Test set 7: Average loss: 0.0003, Accuracy: 9074/10000 (91%)\n","---->Test set 8: Average loss: 0.0035, Accuracy: 1052/10000 (11%)\n","---->Test set 9: Average loss: 0.0030, Accuracy: 981/10000 (10%)\n","--> Training Task 8:\n","----> Epoch 0:\n","---->[0/60000 (0%)]\tPrecision: 0.078125\tLoss: 3.487931\tSurrogate Loss: 0.000000\tTotal Loss: 3.487931\n","---->[12800/60000 (21%)]\tPrecision: 0.828125\tLoss: 0.431268\tSurrogate Loss: 11.808634\tTotal Loss: 1.612131\n","---->[25600/60000 (43%)]\tPrecision: 0.843750\tLoss: 0.611385\tSurrogate Loss: 7.474110\tTotal Loss: 1.358796\n","---->[38400/60000 (64%)]\tPrecision: 0.890625\tLoss: 0.439565\tSurrogate Loss: 7.375833\tTotal Loss: 1.177148\n","---->[51200/60000 (85%)]\tPrecision: 0.890625\tLoss: 0.515341\tSurrogate Loss: 5.628717\tTotal Loss: 1.078213\n","----> Epoch 1:\n","---->[0/60000 (0%)]\tPrecision: 0.968750\tLoss: 0.293865\tSurrogate Loss: 5.447309\tTotal Loss: 0.838596\n","---->[12800/60000 (21%)]\tPrecision: 0.828125\tLoss: 0.434288\tSurrogate Loss: 6.551314\tTotal Loss: 1.089420\n","---->[25600/60000 (43%)]\tPrecision: 0.906250\tLoss: 0.341249\tSurrogate Loss: 9.594638\tTotal Loss: 1.300713\n","---->[38400/60000 (64%)]\tPrecision: 0.843750\tLoss: 0.421263\tSurrogate Loss: 4.159258\tTotal Loss: 0.837189\n","---->[51200/60000 (85%)]\tPrecision: 0.906250\tLoss: 0.295936\tSurrogate Loss: 4.703024\tTotal Loss: 0.766238\n","[3.487931489944458, 0.4312676787376404, 0.6113847494125366, 0.4395650625228882, 0.5153412222862244, 0.29386523365974426, 0.4342881143093109, 0.3412489891052246, 0.4212634265422821, 0.29593563079833984]\n","[3.487931489944458, 1.612131118774414, 1.3587957620620728, 1.1771483421325684, 1.0782129764556885, 0.8385962247848511, 1.0894196033477783, 1.3007128238677979, 0.837189257144928, 0.7662380933761597]\n","--> Finished Training Task 8. Starting Test phase:\n","---->Test set 0: Average loss: 0.0004, Accuracy: 9034/10000 (90%)\n","---->Test set 1: Average loss: 0.0004, Accuracy: 8951/10000 (90%)\n","---->Test set 2: Average loss: 0.0006, Accuracy: 8240/10000 (82%)\n","---->Test set 3: Average loss: 0.0004, Accuracy: 8973/10000 (90%)\n","---->Test set 4: Average loss: 0.0004, Accuracy: 8691/10000 (87%)\n","---->Test set 5: Average loss: 0.0005, Accuracy: 8668/10000 (87%)\n","---->Test set 6: Average loss: 0.0003, Accuracy: 9200/10000 (92%)\n","---->Test set 7: Average loss: 0.0003, Accuracy: 8981/10000 (90%)\n","---->Test set 8: Average loss: 0.0003, Accuracy: 9150/10000 (92%)\n","---->Test set 9: Average loss: 0.0030, Accuracy: 1118/10000 (11%)\n","--> Training Task 9:\n","----> Epoch 0:\n","---->[0/60000 (0%)]\tPrecision: 0.093750\tLoss: 3.268819\tSurrogate Loss: 0.000000\tTotal Loss: 3.268819\n","---->[12800/60000 (21%)]\tPrecision: 0.875000\tLoss: 0.492611\tSurrogate Loss: 10.153133\tTotal Loss: 1.507925\n","---->[25600/60000 (43%)]\tPrecision: 0.843750\tLoss: 0.470317\tSurrogate Loss: 8.456570\tTotal Loss: 1.315974\n","---->[38400/60000 (64%)]\tPrecision: 0.859375\tLoss: 0.555987\tSurrogate Loss: 7.026231\tTotal Loss: 1.258610\n","---->[51200/60000 (85%)]\tPrecision: 0.921875\tLoss: 0.314350\tSurrogate Loss: 7.127448\tTotal Loss: 1.027095\n","----> Epoch 1:\n","---->[0/60000 (0%)]\tPrecision: 0.968750\tLoss: 0.112914\tSurrogate Loss: 5.762595\tTotal Loss: 0.689174\n","---->[12800/60000 (21%)]\tPrecision: 0.906250\tLoss: 0.476577\tSurrogate Loss: 5.800932\tTotal Loss: 1.056670\n","---->[25600/60000 (43%)]\tPrecision: 0.859375\tLoss: 0.420861\tSurrogate Loss: 7.150650\tTotal Loss: 1.135926\n","---->[38400/60000 (64%)]\tPrecision: 0.921875\tLoss: 0.312149\tSurrogate Loss: 5.148036\tTotal Loss: 0.826953\n","---->[51200/60000 (85%)]\tPrecision: 0.828125\tLoss: 0.452082\tSurrogate Loss: 5.270539\tTotal Loss: 0.979136\n","[3.2688190937042236, 0.4926114082336426, 0.47031688690185547, 0.5559871196746826, 0.31435030698776245, 0.11291421204805374, 0.4765767455101013, 0.4208609163761139, 0.3121492564678192, 0.4520816206932068]\n","[3.2688190937042236, 1.5079247951507568, 1.3159738779067993, 1.2586102485656738, 1.027095079421997, 0.6891737580299377, 1.0566699504852295, 1.1359258890151978, 0.8269528150558472, 0.9791355729103088]\n","--> Finished Training Task 9. Starting Test phase:\n","---->Test set 0: Average loss: 0.0004, Accuracy: 8751/10000 (88%)\n","---->Test set 1: Average loss: 0.0004, Accuracy: 8834/10000 (88%)\n","---->Test set 2: Average loss: 0.0005, Accuracy: 8356/10000 (84%)\n","---->Test set 3: Average loss: 0.0003, Accuracy: 8947/10000 (89%)\n","---->Test set 4: Average loss: 0.0004, Accuracy: 8790/10000 (88%)\n","---->Test set 5: Average loss: 0.0005, Accuracy: 8393/10000 (84%)\n","---->Test set 6: Average loss: 0.0003, Accuracy: 9128/10000 (91%)\n","---->Test set 7: Average loss: 0.0003, Accuracy: 9067/10000 (91%)\n","---->Test set 8: Average loss: 0.0003, Accuracy: 9015/10000 (90%)\n","---->Test set 9: Average loss: 0.0003, Accuracy: 9233/10000 (92%)\n","Accuracy: 0.88514\n","Confusion matrix:\n","0.0953,0.1111,0.0658,0.0714,0.1305,0.1419,0.155,0.0637,0.0813,0.1407\n","0.9563,0.0633,0.102,0.1157,0.0842,0.139,0.1039,0.1344,0.1009,0.0761\n","0.9404,0.948,0.1179,0.0829,0.1406,0.1378,0.1217,0.1165,0.0984,0.0855\n","0.9216,0.9327,0.9235,0.0775,0.1325,0.0834,0.0977,0.1093,0.0888,0.0564\n","0.9131,0.9322,0.9188,0.9424,0.1402,0.0859,0.0978,0.1159,0.1033,0.0836\n","0.898,0.8998,0.865,0.9204,0.916,0.0925,0.076,0.1147,0.1168,0.1061\n","0.8918,0.9233,0.8858,0.9347,0.9205,0.9315,0.0916,0.1147,0.1036,0.0768\n","0.868,0.9098,0.8641,0.9282,0.913,0.914,0.9296,0.1104,0.1141,0.0948\n","0.8727,0.9046,0.8593,0.9215,0.8725,0.8958,0.9274,0.9074,0.1052,0.0981\n","0.9034,0.8951,0.824,0.8973,0.8691,0.8668,0.92,0.8981,0.915,0.1118\n","0.8751,0.8834,0.8356,0.8947,0.879,0.8393,0.9128,0.9067,0.9015,0.9233\n","--> Training:\n","---->Test set 0: Average loss: 0.1878, Accuracy: 1223/10000 (12%)\n","---->Test set 1: Average loss: 0.1665, Accuracy: 1060/10000 (11%)\n","---->Test set 2: Average loss: 0.1677, Accuracy: 1062/10000 (11%)\n","---->Test set 3: Average loss: 0.1677, Accuracy: 1240/10000 (12%)\n","---->Test set 4: Average loss: 0.1868, Accuracy: 1104/10000 (11%)\n","---->Test set 5: Average loss: 0.1743, Accuracy: 1095/10000 (11%)\n","---->Test set 6: Average loss: 0.1940, Accuracy: 1599/10000 (16%)\n","---->Test set 7: Average loss: 0.1660, Accuracy: 786/10000 (8%)\n","---->Test set 8: Average loss: 0.1773, Accuracy: 1017/10000 (10%)\n","---->Test set 9: Average loss: 0.1516, Accuracy: 1779/10000 (18%)\n","--> Training Task 0:\n","----> Epoch 0:\n","---->[0/60000 (0%)]\tPrecision: 0.171875\tLoss: 187.349854\tSurrogate Loss: 0.000000\tTotal Loss: 187.349854\n","---->[12800/60000 (21%)]\tPrecision: 0.875000\tLoss: 0.392239\tSurrogate Loss: 0.000000\tTotal Loss: 0.392239\n","---->[25600/60000 (43%)]\tPrecision: 0.875000\tLoss: 0.259206\tSurrogate Loss: 0.000000\tTotal Loss: 0.259206\n","---->[38400/60000 (64%)]\tPrecision: 0.890625\tLoss: 0.295028\tSurrogate Loss: 0.000000\tTotal Loss: 0.295028\n","---->[51200/60000 (85%)]\tPrecision: 0.984375\tLoss: 0.092432\tSurrogate Loss: 0.000000\tTotal Loss: 0.092432\n","----> Epoch 1:\n","---->[0/60000 (0%)]\tPrecision: 0.953125\tLoss: 0.189569\tSurrogate Loss: 0.000000\tTotal Loss: 0.189569\n","---->[12800/60000 (21%)]\tPrecision: 0.968750\tLoss: 0.104317\tSurrogate Loss: 0.000000\tTotal Loss: 0.104317\n","---->[25600/60000 (43%)]\tPrecision: 0.984375\tLoss: 0.120236\tSurrogate Loss: 0.000000\tTotal Loss: 0.120236\n","---->[38400/60000 (64%)]\tPrecision: 0.953125\tLoss: 0.144631\tSurrogate Loss: 0.000000\tTotal Loss: 0.144631\n","---->[51200/60000 (85%)]\tPrecision: 1.000000\tLoss: 0.052905\tSurrogate Loss: 0.000000\tTotal Loss: 0.052905\n","----> Epoch 2:\n","---->[0/60000 (0%)]\tPrecision: 1.000000\tLoss: 0.011917\tSurrogate Loss: 0.000000\tTotal Loss: 0.011917\n","---->[12800/60000 (21%)]\tPrecision: 0.953125\tLoss: 0.192614\tSurrogate Loss: 0.000000\tTotal Loss: 0.192614\n","---->[25600/60000 (43%)]\tPrecision: 0.937500\tLoss: 0.108168\tSurrogate Loss: 0.000000\tTotal Loss: 0.108168\n","---->[38400/60000 (64%)]\tPrecision: 0.937500\tLoss: 0.188930\tSurrogate Loss: 0.000000\tTotal Loss: 0.188930\n","---->[51200/60000 (85%)]\tPrecision: 1.000000\tLoss: 0.013632\tSurrogate Loss: 0.000000\tTotal Loss: 0.013632\n","[187.349853515625, 0.3922387361526489, 0.25920552015304565, 0.29502832889556885, 0.09243155270814896, 0.1895691454410553, 0.10431712865829468, 0.12023565918207169, 0.144630566239357, 0.0529051274061203, 0.01191664021462202, 0.19261440634727478, 0.10816771537065506, 0.18893034756183624, 0.013632056303322315]\n","[187.349853515625, 0.3922387361526489, 0.25920552015304565, 0.29502832889556885, 0.09243155270814896, 0.1895691454410553, 0.10431712865829468, 0.12023565918207169, 0.144630566239357, 0.0529051274061203, 0.01191664021462202, 0.19261440634727478, 0.10816771537065506, 0.18893034756183624, 0.013632056303322315]\n","--> Finished Training Task 0. Starting Test phase:\n","---->Test set 0: Average loss: 0.0002, Accuracy: 9553/10000 (96%)\n","---->Test set 1: Average loss: 0.0046, Accuracy: 516/10000 (5%)\n","---->Test set 2: Average loss: 0.0052, Accuracy: 1355/10000 (14%)\n","---->Test set 3: Average loss: 0.0053, Accuracy: 994/10000 (10%)\n","---->Test set 4: Average loss: 0.0036, Accuracy: 1432/10000 (14%)\n","---->Test set 5: Average loss: 0.0046, Accuracy: 1087/10000 (11%)\n","---->Test set 6: Average loss: 0.0055, Accuracy: 865/10000 (9%)\n","---->Test set 7: Average loss: 0.0039, Accuracy: 1186/10000 (12%)\n","---->Test set 8: Average loss: 0.0048, Accuracy: 630/10000 (6%)\n","---->Test set 9: Average loss: 0.0042, Accuracy: 1124/10000 (11%)\n","--> Training Task 1:\n","----> Epoch 0:\n","---->[0/60000 (0%)]\tPrecision: 0.046875\tLoss: 4.431346\tSurrogate Loss: 0.000000\tTotal Loss: 4.431346\n","---->[12800/60000 (21%)]\tPrecision: 0.781250\tLoss: 0.680168\tSurrogate Loss: 10.075731\tTotal Loss: 1.687741\n","---->[25600/60000 (43%)]\tPrecision: 0.921875\tLoss: 0.367904\tSurrogate Loss: 5.511054\tTotal Loss: 0.919009\n","---->[38400/60000 (64%)]\tPrecision: 0.859375\tLoss: 0.502854\tSurrogate Loss: 5.133686\tTotal Loss: 1.016222\n","---->[51200/60000 (85%)]\tPrecision: 0.843750\tLoss: 0.843259\tSurrogate Loss: 4.267686\tTotal Loss: 1.270027\n","----> Epoch 1:\n","---->[0/60000 (0%)]\tPrecision: 0.921875\tLoss: 0.300387\tSurrogate Loss: 4.173157\tTotal Loss: 0.717703\n","---->[12800/60000 (21%)]\tPrecision: 0.937500\tLoss: 0.353972\tSurrogate Loss: 3.752943\tTotal Loss: 0.729266\n","---->[25600/60000 (43%)]\tPrecision: 0.906250\tLoss: 0.282351\tSurrogate Loss: 3.194720\tTotal Loss: 0.601823\n","---->[38400/60000 (64%)]\tPrecision: 0.921875\tLoss: 0.247070\tSurrogate Loss: 4.012615\tTotal Loss: 0.648331\n","---->[51200/60000 (85%)]\tPrecision: 0.984375\tLoss: 0.039108\tSurrogate Loss: 2.481261\tTotal Loss: 0.287234\n","----> Epoch 2:\n","---->[0/60000 (0%)]\tPrecision: 0.953125\tLoss: 0.135767\tSurrogate Loss: 2.886022\tTotal Loss: 0.424369\n","---->[12800/60000 (21%)]\tPrecision: 0.906250\tLoss: 0.222152\tSurrogate Loss: 3.609754\tTotal Loss: 0.583127\n","---->[25600/60000 (43%)]\tPrecision: 1.000000\tLoss: 0.043424\tSurrogate Loss: 2.934788\tTotal Loss: 0.336903\n","---->[38400/60000 (64%)]\tPrecision: 0.968750\tLoss: 0.159474\tSurrogate Loss: 4.515170\tTotal Loss: 0.610991\n","---->[51200/60000 (85%)]\tPrecision: 0.953125\tLoss: 0.168085\tSurrogate Loss: 7.737401\tTotal Loss: 0.941825\n","[4.4313459396362305, 0.6801676750183105, 0.36790403723716736, 0.5028536319732666, 0.8432586193084717, 0.30038702487945557, 0.3539717197418213, 0.2823508083820343, 0.24706964194774628, 0.03910832852125168, 0.135766863822937, 0.2221517562866211, 0.04342398792505264, 0.15947385132312775, 0.16808515787124634]\n","[4.4313459396362305, 1.6877408027648926, 0.9190094470977783, 1.0162222385406494, 1.2700271606445312, 0.7177027463912964, 0.7292659878730774, 0.6018227338790894, 0.6483311057090759, 0.28723442554473877, 0.42436903715133667, 0.5831272006034851, 0.3369027376174927, 0.6109908223152161, 0.9418252110481262]\n","--> Finished Training Task 1. Starting Test phase:\n","---->Test set 0: Average loss: 0.0002, Accuracy: 9539/10000 (95%)\n","---->Test set 1: Average loss: 0.0002, Accuracy: 9356/10000 (94%)\n","---->Test set 2: Average loss: 0.0040, Accuracy: 1063/10000 (11%)\n","---->Test set 3: Average loss: 0.0048, Accuracy: 1303/10000 (13%)\n","---->Test set 4: Average loss: 0.0040, Accuracy: 1069/10000 (11%)\n","---->Test set 5: Average loss: 0.0042, Accuracy: 912/10000 (9%)\n","---->Test set 6: Average loss: 0.0045, Accuracy: 1115/10000 (11%)\n","---->Test set 7: Average loss: 0.0037, Accuracy: 1348/10000 (13%)\n","---->Test set 8: Average loss: 0.0048, Accuracy: 993/10000 (10%)\n","---->Test set 9: Average loss: 0.0038, Accuracy: 1325/10000 (13%)\n","--> Training Task 2:\n","----> Epoch 0:\n","---->[0/60000 (0%)]\tPrecision: 0.062500\tLoss: 4.419391\tSurrogate Loss: 0.000000\tTotal Loss: 4.419391\n","---->[12800/60000 (21%)]\tPrecision: 0.921875\tLoss: 0.318063\tSurrogate Loss: 10.832463\tTotal Loss: 1.401309\n","---->[25600/60000 (43%)]\tPrecision: 0.875000\tLoss: 0.735734\tSurrogate Loss: 8.778928\tTotal Loss: 1.613627\n","---->[38400/60000 (64%)]\tPrecision: 1.000000\tLoss: 0.085942\tSurrogate Loss: 5.144870\tTotal Loss: 0.600429\n","---->[51200/60000 (85%)]\tPrecision: 0.921875\tLoss: 0.236350\tSurrogate Loss: 5.726360\tTotal Loss: 0.808986\n","----> Epoch 1:\n","---->[0/60000 (0%)]\tPrecision: 0.968750\tLoss: 0.139300\tSurrogate Loss: 4.438258\tTotal Loss: 0.583126\n","---->[12800/60000 (21%)]\tPrecision: 0.984375\tLoss: 0.117932\tSurrogate Loss: 5.472169\tTotal Loss: 0.665149\n","---->[25600/60000 (43%)]\tPrecision: 0.968750\tLoss: 0.112849\tSurrogate Loss: 3.760387\tTotal Loss: 0.488887\n","---->[38400/60000 (64%)]\tPrecision: 0.984375\tLoss: 0.084356\tSurrogate Loss: 4.209519\tTotal Loss: 0.505308\n","---->[51200/60000 (85%)]\tPrecision: 0.875000\tLoss: 0.503040\tSurrogate Loss: 8.340837\tTotal Loss: 1.337124\n","----> Epoch 2:\n","---->[0/60000 (0%)]\tPrecision: 0.937500\tLoss: 0.217849\tSurrogate Loss: 4.015473\tTotal Loss: 0.619396\n","---->[12800/60000 (21%)]\tPrecision: 0.953125\tLoss: 0.176104\tSurrogate Loss: 3.468721\tTotal Loss: 0.522976\n","---->[25600/60000 (43%)]\tPrecision: 0.953125\tLoss: 0.153417\tSurrogate Loss: 3.055686\tTotal Loss: 0.458986\n","---->[38400/60000 (64%)]\tPrecision: 0.890625\tLoss: 0.303132\tSurrogate Loss: 4.812644\tTotal Loss: 0.784397\n","---->[51200/60000 (85%)]\tPrecision: 0.937500\tLoss: 0.634379\tSurrogate Loss: 3.559180\tTotal Loss: 0.990297\n","[4.41939115524292, 0.3180626630783081, 0.7357339859008789, 0.08594217151403427, 0.236349955201149, 0.13930006325244904, 0.11793222278356552, 0.11284864693880081, 0.08435562252998352, 0.5030398368835449, 0.21784894168376923, 0.17610377073287964, 0.15341708064079285, 0.3031323552131653, 0.6343786120414734]\n","[4.41939115524292, 1.4013090133666992, 1.6136267185211182, 0.600429117679596, 0.8089859485626221, 0.5831258296966553, 0.6651490926742554, 0.48888736963272095, 0.5053075551986694, 1.3371236324310303, 0.6193962693214417, 0.5229759216308594, 0.45898571610450745, 0.7843967080116272, 0.9902966022491455]\n","--> Finished Training Task 2. Starting Test phase:\n","---->Test set 0: Average loss: 0.0002, Accuracy: 9504/10000 (95%)\n","---->Test set 1: Average loss: 0.0002, Accuracy: 9416/10000 (94%)\n","---->Test set 2: Average loss: 0.0003, Accuracy: 9300/10000 (93%)\n","---->Test set 3: Average loss: 0.0072, Accuracy: 1110/10000 (11%)\n","---->Test set 4: Average loss: 0.0038, Accuracy: 1235/10000 (12%)\n","---->Test set 5: Average loss: 0.0037, Accuracy: 855/10000 (9%)\n","---->Test set 6: Average loss: 0.0049, Accuracy: 1106/10000 (11%)\n","---->Test set 7: Average loss: 0.0037, Accuracy: 1471/10000 (15%)\n","---->Test set 8: Average loss: 0.0039, Accuracy: 1111/10000 (11%)\n","---->Test set 9: Average loss: 0.0039, Accuracy: 1277/10000 (13%)\n","--> Training Task 3:\n","----> Epoch 0:\n","---->[0/60000 (0%)]\tPrecision: 0.203125\tLoss: 5.855038\tSurrogate Loss: 0.000000\tTotal Loss: 5.855038\n","---->[12800/60000 (21%)]\tPrecision: 0.843750\tLoss: 0.537497\tSurrogate Loss: 9.866670\tTotal Loss: 1.524164\n","---->[25600/60000 (43%)]\tPrecision: 0.937500\tLoss: 0.233305\tSurrogate Loss: 7.065293\tTotal Loss: 0.939835\n","---->[38400/60000 (64%)]\tPrecision: 0.890625\tLoss: 0.319153\tSurrogate Loss: 6.349282\tTotal Loss: 0.954081\n","---->[51200/60000 (85%)]\tPrecision: 0.937500\tLoss: 0.277464\tSurrogate Loss: 3.905538\tTotal Loss: 0.668018\n","----> Epoch 1:\n","---->[0/60000 (0%)]\tPrecision: 0.875000\tLoss: 0.344043\tSurrogate Loss: 5.739000\tTotal Loss: 0.917943\n","---->[12800/60000 (21%)]\tPrecision: 0.906250\tLoss: 0.340974\tSurrogate Loss: 3.268270\tTotal Loss: 0.667801\n","---->[25600/60000 (43%)]\tPrecision: 0.968750\tLoss: 0.163436\tSurrogate Loss: 5.019317\tTotal Loss: 0.665368\n","---->[38400/60000 (64%)]\tPrecision: 0.937500\tLoss: 0.322717\tSurrogate Loss: 7.426157\tTotal Loss: 1.065332\n","---->[51200/60000 (85%)]\tPrecision: 0.906250\tLoss: 0.441092\tSurrogate Loss: 7.224209\tTotal Loss: 1.163513\n","----> Epoch 2:\n","---->[0/60000 (0%)]\tPrecision: 0.906250\tLoss: 0.492235\tSurrogate Loss: 7.518514\tTotal Loss: 1.244087\n","---->[12800/60000 (21%)]\tPrecision: 0.906250\tLoss: 0.647108\tSurrogate Loss: 5.076926\tTotal Loss: 1.154800\n","---->[25600/60000 (43%)]\tPrecision: 0.859375\tLoss: 0.609772\tSurrogate Loss: 5.856762\tTotal Loss: 1.195448\n","---->[38400/60000 (64%)]\tPrecision: 0.906250\tLoss: 0.369680\tSurrogate Loss: 4.775552\tTotal Loss: 0.847235\n","---->[51200/60000 (85%)]\tPrecision: 0.937500\tLoss: 0.148319\tSurrogate Loss: 4.695295\tTotal Loss: 0.617848\n","[5.855037689208984, 0.5374974608421326, 0.2333052158355713, 0.3191532790660858, 0.27746421098709106, 0.34404289722442627, 0.34097424149513245, 0.163436159491539, 0.3227165937423706, 0.44109195470809937, 0.4922351837158203, 0.647107720375061, 0.6097718477249146, 0.3696799874305725, 0.14831864833831787]\n","[5.855037689208984, 1.5241644382476807, 0.9398345351219177, 0.9540814161300659, 0.6680180430412292, 0.9179429411888123, 0.6678012013435364, 0.665367841720581, 1.0653321743011475, 1.163512945175171, 1.2440866231918335, 1.1548004150390625, 1.1954481601715088, 0.8472352027893066, 0.6178481578826904]\n","--> Finished Training Task 3. Starting Test phase:\n","---->Test set 0: Average loss: 0.0002, Accuracy: 9415/10000 (94%)\n","---->Test set 1: Average loss: 0.0003, Accuracy: 9192/10000 (92%)\n","---->Test set 2: Average loss: 0.0003, Accuracy: 9183/10000 (92%)\n","---->Test set 3: Average loss: 0.0003, Accuracy: 9280/10000 (93%)\n","---->Test set 4: Average loss: 0.0041, Accuracy: 1472/10000 (15%)\n","---->Test set 5: Average loss: 0.0044, Accuracy: 869/10000 (9%)\n","---->Test set 6: Average loss: 0.0043, Accuracy: 1204/10000 (12%)\n","---->Test set 7: Average loss: 0.0044, Accuracy: 1104/10000 (11%)\n","---->Test set 8: Average loss: 0.0047, Accuracy: 1129/10000 (11%)\n","---->Test set 9: Average loss: 0.0046, Accuracy: 1277/10000 (13%)\n","--> Training Task 4:\n","----> Epoch 0:\n","---->[0/60000 (0%)]\tPrecision: 0.109375\tLoss: 4.135140\tSurrogate Loss: 0.000000\tTotal Loss: 4.135140\n","---->[12800/60000 (21%)]\tPrecision: 0.906250\tLoss: 0.349842\tSurrogate Loss: 11.660485\tTotal Loss: 1.515890\n","---->[25600/60000 (43%)]\tPrecision: 0.843750\tLoss: 0.520738\tSurrogate Loss: 8.618371\tTotal Loss: 1.382576\n","---->[38400/60000 (64%)]\tPrecision: 0.859375\tLoss: 0.346108\tSurrogate Loss: 7.977551\tTotal Loss: 1.143863\n","---->[51200/60000 (85%)]\tPrecision: 0.906250\tLoss: 0.321305\tSurrogate Loss: 8.901537\tTotal Loss: 1.211458\n","----> Epoch 1:\n","---->[0/60000 (0%)]\tPrecision: 0.859375\tLoss: 0.497805\tSurrogate Loss: 6.860735\tTotal Loss: 1.183879\n","---->[12800/60000 (21%)]\tPrecision: 0.859375\tLoss: 0.330541\tSurrogate Loss: 5.164989\tTotal Loss: 0.847039\n","---->[25600/60000 (43%)]\tPrecision: 0.937500\tLoss: 0.249670\tSurrogate Loss: 5.095185\tTotal Loss: 0.759188\n","---->[38400/60000 (64%)]\tPrecision: 0.906250\tLoss: 0.246634\tSurrogate Loss: 6.100980\tTotal Loss: 0.856732\n","---->[51200/60000 (85%)]\tPrecision: 0.906250\tLoss: 0.269645\tSurrogate Loss: 6.856138\tTotal Loss: 0.955259\n","----> Epoch 2:\n","---->[0/60000 (0%)]\tPrecision: 0.953125\tLoss: 0.239260\tSurrogate Loss: 8.005301\tTotal Loss: 1.039790\n","---->[12800/60000 (21%)]\tPrecision: 0.921875\tLoss: 0.299436\tSurrogate Loss: 5.564879\tTotal Loss: 0.855924\n","---->[25600/60000 (43%)]\tPrecision: 0.890625\tLoss: 0.310256\tSurrogate Loss: 5.809607\tTotal Loss: 0.891217\n","---->[38400/60000 (64%)]\tPrecision: 0.843750\tLoss: 0.532700\tSurrogate Loss: 6.966536\tTotal Loss: 1.229354\n","---->[51200/60000 (85%)]\tPrecision: 0.921875\tLoss: 0.278144\tSurrogate Loss: 5.139717\tTotal Loss: 0.792116\n","[4.1351399421691895, 0.34984180331230164, 0.5207383632659912, 0.3461078405380249, 0.3213045299053192, 0.49780526757240295, 0.33054062724113464, 0.24966958165168762, 0.24663393199443817, 0.26964542269706726, 0.23925966024398804, 0.2994355261325836, 0.3102562427520752, 0.5327004194259644, 0.27814406156539917]\n","[4.1351399421691895, 1.51589035987854, 1.382575511932373, 1.1438629627227783, 1.2114582061767578, 1.183878779411316, 0.8470394611358643, 0.759188175201416, 0.8567320108413696, 0.9552592039108276, 1.0397896766662598, 0.8559235334396362, 0.8912169337272644, 1.229353904724121, 0.7921157479286194]\n","--> Finished Training Task 4. Starting Test phase:\n","---->Test set 0: Average loss: 0.0002, Accuracy: 9421/10000 (94%)\n","---->Test set 1: Average loss: 0.0003, Accuracy: 9175/10000 (92%)\n","---->Test set 2: Average loss: 0.0003, Accuracy: 9258/10000 (93%)\n","---->Test set 3: Average loss: 0.0003, Accuracy: 9322/10000 (93%)\n","---->Test set 4: Average loss: 0.0002, Accuracy: 9353/10000 (94%)\n","---->Test set 5: Average loss: 0.0038, Accuracy: 1015/10000 (10%)\n","---->Test set 6: Average loss: 0.0046, Accuracy: 1054/10000 (11%)\n","---->Test set 7: Average loss: 0.0037, Accuracy: 1452/10000 (15%)\n","---->Test set 8: Average loss: 0.0043, Accuracy: 1172/10000 (12%)\n","---->Test set 9: Average loss: 0.0044, Accuracy: 1102/10000 (11%)\n","--> Training Task 5:\n","----> Epoch 0:\n","---->[0/60000 (0%)]\tPrecision: 0.125000\tLoss: 3.859271\tSurrogate Loss: 0.000000\tTotal Loss: 3.859271\n","---->[12800/60000 (21%)]\tPrecision: 0.859375\tLoss: 0.514642\tSurrogate Loss: 10.498118\tTotal Loss: 1.564454\n","---->[25600/60000 (43%)]\tPrecision: 0.906250\tLoss: 0.698920\tSurrogate Loss: 13.094659\tTotal Loss: 2.008386\n","---->[38400/60000 (64%)]\tPrecision: 0.921875\tLoss: 0.329781\tSurrogate Loss: 7.666306\tTotal Loss: 1.096411\n","---->[51200/60000 (85%)]\tPrecision: 0.984375\tLoss: 0.121265\tSurrogate Loss: 7.489689\tTotal Loss: 0.870234\n","----> Epoch 1:\n","---->[0/60000 (0%)]\tPrecision: 0.890625\tLoss: 0.247510\tSurrogate Loss: 4.923562\tTotal Loss: 0.739867\n","---->[12800/60000 (21%)]\tPrecision: 0.968750\tLoss: 0.116554\tSurrogate Loss: 5.834162\tTotal Loss: 0.699970\n","---->[25600/60000 (43%)]\tPrecision: 0.843750\tLoss: 0.674655\tSurrogate Loss: 6.898939\tTotal Loss: 1.364549\n","---->[38400/60000 (64%)]\tPrecision: 0.921875\tLoss: 0.217644\tSurrogate Loss: 5.866728\tTotal Loss: 0.804317\n","---->[51200/60000 (85%)]\tPrecision: 0.921875\tLoss: 0.244724\tSurrogate Loss: 4.132724\tTotal Loss: 0.657996\n","----> Epoch 2:\n","---->[0/60000 (0%)]\tPrecision: 0.906250\tLoss: 0.364559\tSurrogate Loss: 5.925958\tTotal Loss: 0.957155\n","---->[12800/60000 (21%)]\tPrecision: 0.890625\tLoss: 0.395022\tSurrogate Loss: 4.604000\tTotal Loss: 0.855422\n","---->[25600/60000 (43%)]\tPrecision: 0.984375\tLoss: 0.093513\tSurrogate Loss: 5.713251\tTotal Loss: 0.664838\n","---->[38400/60000 (64%)]\tPrecision: 0.937500\tLoss: 0.163874\tSurrogate Loss: 6.207648\tTotal Loss: 0.784638\n","---->[51200/60000 (85%)]\tPrecision: 0.921875\tLoss: 0.408811\tSurrogate Loss: 4.124449\tTotal Loss: 0.821256\n","[3.8592705726623535, 0.5146417021751404, 0.6989202499389648, 0.3297806978225708, 0.12126517295837402, 0.2475103884935379, 0.11655403673648834, 0.6746553778648376, 0.2176438868045807, 0.24472369253635406, 0.3645593523979187, 0.39502182602882385, 0.09351255744695663, 0.16387353837490082, 0.40881088376045227]\n","[3.8592705726623535, 1.5644536018371582, 2.0083861351013184, 1.0964112281799316, 0.8702341318130493, 0.7398665547370911, 0.6999701857566833, 1.3645492792129517, 0.8043166399002075, 0.6579960584640503, 0.9571551084518433, 0.8554218411445618, 0.664837658405304, 0.7846384048461914, 0.8212558031082153]\n","--> Finished Training Task 5. Starting Test phase:\n","---->Test set 0: Average loss: 0.0002, Accuracy: 9431/10000 (94%)\n","---->Test set 1: Average loss: 0.0003, Accuracy: 9225/10000 (92%)\n","---->Test set 2: Average loss: 0.0003, Accuracy: 9198/10000 (92%)\n","---->Test set 3: Average loss: 0.0003, Accuracy: 9250/10000 (92%)\n","---->Test set 4: Average loss: 0.0003, Accuracy: 9291/10000 (93%)\n","---->Test set 5: Average loss: 0.0003, Accuracy: 9280/10000 (93%)\n","---->Test set 6: Average loss: 0.0043, Accuracy: 1073/10000 (11%)\n","---->Test set 7: Average loss: 0.0033, Accuracy: 1778/10000 (18%)\n","---->Test set 8: Average loss: 0.0045, Accuracy: 1143/10000 (11%)\n","---->Test set 9: Average loss: 0.0042, Accuracy: 1072/10000 (11%)\n","--> Training Task 6:\n","----> Epoch 0:\n","---->[0/60000 (0%)]\tPrecision: 0.093750\tLoss: 3.704225\tSurrogate Loss: 0.000000\tTotal Loss: 3.704225\n","---->[12800/60000 (21%)]\tPrecision: 0.875000\tLoss: 0.450158\tSurrogate Loss: 12.183268\tTotal Loss: 1.668485\n","---->[25600/60000 (43%)]\tPrecision: 0.906250\tLoss: 0.274269\tSurrogate Loss: 9.244475\tTotal Loss: 1.198716\n","---->[38400/60000 (64%)]\tPrecision: 0.812500\tLoss: 0.659480\tSurrogate Loss: 10.604392\tTotal Loss: 1.719920\n","---->[51200/60000 (85%)]\tPrecision: 0.859375\tLoss: 0.393284\tSurrogate Loss: 9.388699\tTotal Loss: 1.332154\n","----> Epoch 1:\n","---->[0/60000 (0%)]\tPrecision: 0.859375\tLoss: 0.507024\tSurrogate Loss: 8.146140\tTotal Loss: 1.321638\n","---->[12800/60000 (21%)]\tPrecision: 0.890625\tLoss: 0.244837\tSurrogate Loss: 6.477148\tTotal Loss: 0.892552\n","---->[25600/60000 (43%)]\tPrecision: 0.968750\tLoss: 0.345457\tSurrogate Loss: 10.172609\tTotal Loss: 1.362718\n","---->[38400/60000 (64%)]\tPrecision: 0.937500\tLoss: 0.419821\tSurrogate Loss: 7.403854\tTotal Loss: 1.160206\n","---->[51200/60000 (85%)]\tPrecision: 0.953125\tLoss: 0.169254\tSurrogate Loss: 12.683740\tTotal Loss: 1.437628\n","----> Epoch 2:\n","---->[0/60000 (0%)]\tPrecision: 0.875000\tLoss: 0.506912\tSurrogate Loss: 7.782733\tTotal Loss: 1.285185\n","---->[12800/60000 (21%)]\tPrecision: 0.937500\tLoss: 0.316845\tSurrogate Loss: 9.562576\tTotal Loss: 1.273102\n","---->[25600/60000 (43%)]\tPrecision: 0.921875\tLoss: 0.237692\tSurrogate Loss: 5.593699\tTotal Loss: 0.797062\n","---->[38400/60000 (64%)]\tPrecision: 0.906250\tLoss: 0.271606\tSurrogate Loss: 7.650338\tTotal Loss: 1.036640\n","---->[51200/60000 (85%)]\tPrecision: 0.890625\tLoss: 0.430764\tSurrogate Loss: 8.913572\tTotal Loss: 1.322121\n","[3.7042250633239746, 0.45015814900398254, 0.2742687463760376, 0.6594804525375366, 0.3932836055755615, 0.5070241689682007, 0.24483732879161835, 0.3454570174217224, 0.419820636510849, 0.16925403475761414, 0.50691157579422, 0.316844642162323, 0.23769229650497437, 0.2716064155101776, 0.430763840675354]\n","[3.7042250633239746, 1.6684849262237549, 1.1987162828445435, 1.7199196815490723, 1.332153558731079, 1.3216381072998047, 0.8925521373748779, 1.362717866897583, 1.1602060794830322, 1.4376280307769775, 1.2851848602294922, 1.2731022834777832, 0.7970622777938843, 1.0366401672363281, 1.3221211433410645]\n","--> Finished Training Task 6. Starting Test phase:\n","---->Test set 0: Average loss: 0.0002, Accuracy: 9386/10000 (94%)\n","---->Test set 1: Average loss: 0.0004, Accuracy: 9061/10000 (91%)\n","---->Test set 2: Average loss: 0.0003, Accuracy: 9029/10000 (90%)\n","---->Test set 3: Average loss: 0.0003, Accuracy: 9171/10000 (92%)\n","---->Test set 4: Average loss: 0.0003, Accuracy: 9001/10000 (90%)\n","---->Test set 5: Average loss: 0.0003, Accuracy: 9053/10000 (91%)\n","---->Test set 6: Average loss: 0.0003, Accuracy: 9244/10000 (92%)\n","---->Test set 7: Average loss: 0.0033, Accuracy: 1373/10000 (14%)\n","---->Test set 8: Average loss: 0.0036, Accuracy: 1373/10000 (14%)\n","---->Test set 9: Average loss: 0.0035, Accuracy: 1002/10000 (10%)\n","--> Training Task 7:\n","----> Epoch 0:\n","---->[0/60000 (0%)]\tPrecision: 0.140625\tLoss: 3.151500\tSurrogate Loss: 0.000000\tTotal Loss: 3.151500\n","---->[12800/60000 (21%)]\tPrecision: 0.859375\tLoss: 0.564873\tSurrogate Loss: 13.384575\tTotal Loss: 1.903331\n","---->[25600/60000 (43%)]\tPrecision: 0.906250\tLoss: 0.340556\tSurrogate Loss: 8.678467\tTotal Loss: 1.208402\n","---->[38400/60000 (64%)]\tPrecision: 0.906250\tLoss: 0.426646\tSurrogate Loss: 6.202152\tTotal Loss: 1.046862\n","---->[51200/60000 (85%)]\tPrecision: 0.890625\tLoss: 0.319886\tSurrogate Loss: 7.933729\tTotal Loss: 1.113259\n","----> Epoch 1:\n","---->[0/60000 (0%)]\tPrecision: 0.921875\tLoss: 0.223787\tSurrogate Loss: 6.406581\tTotal Loss: 0.864445\n","---->[12800/60000 (21%)]\tPrecision: 0.906250\tLoss: 0.270825\tSurrogate Loss: 6.878825\tTotal Loss: 0.958708\n","---->[25600/60000 (43%)]\tPrecision: 0.890625\tLoss: 0.575756\tSurrogate Loss: 5.662646\tTotal Loss: 1.142020\n","---->[38400/60000 (64%)]\tPrecision: 0.906250\tLoss: 0.438171\tSurrogate Loss: 11.614611\tTotal Loss: 1.599632\n","---->[51200/60000 (85%)]\tPrecision: 0.921875\tLoss: 0.345206\tSurrogate Loss: 6.032795\tTotal Loss: 0.948486\n","----> Epoch 2:\n","---->[0/60000 (0%)]\tPrecision: 0.890625\tLoss: 0.242404\tSurrogate Loss: 5.290102\tTotal Loss: 0.771415\n","---->[12800/60000 (21%)]\tPrecision: 0.968750\tLoss: 0.109067\tSurrogate Loss: 6.037319\tTotal Loss: 0.712799\n","---->[25600/60000 (43%)]\tPrecision: 0.937500\tLoss: 0.223119\tSurrogate Loss: 6.382874\tTotal Loss: 0.861407\n","---->[38400/60000 (64%)]\tPrecision: 0.906250\tLoss: 0.263263\tSurrogate Loss: 5.940609\tTotal Loss: 0.857323\n","---->[51200/60000 (85%)]\tPrecision: 0.937500\tLoss: 0.301612\tSurrogate Loss: 6.235876\tTotal Loss: 0.925200\n","[3.1515002250671387, 0.5648732781410217, 0.3405556380748749, 0.4266464114189148, 0.31988608837127686, 0.22378680109977722, 0.270825058221817, 0.5757558345794678, 0.4381711184978485, 0.3452061116695404, 0.24240431189537048, 0.10906734317541122, 0.22311939299106598, 0.2632625102996826, 0.30161210894584656]\n","[3.1515002250671387, 1.9033308029174805, 1.2084022760391235, 1.0468616485595703, 1.113258957862854, 0.8644449710845947, 0.9587075710296631, 1.1420204639434814, 1.5996322631835938, 0.9484856128692627, 0.7714145183563232, 0.7127991914749146, 0.8614068627357483, 0.8573234677314758, 0.9251997470855713]\n","--> Finished Training Task 7. Starting Test phase:\n","---->Test set 0: Average loss: 0.0002, Accuracy: 9327/10000 (93%)\n","---->Test set 1: Average loss: 0.0004, Accuracy: 8890/10000 (89%)\n","---->Test set 2: Average loss: 0.0005, Accuracy: 8620/10000 (86%)\n","---->Test set 3: Average loss: 0.0003, Accuracy: 9037/10000 (90%)\n","---->Test set 4: Average loss: 0.0004, Accuracy: 8734/10000 (87%)\n","---->Test set 5: Average loss: 0.0004, Accuracy: 8876/10000 (89%)\n","---->Test set 6: Average loss: 0.0003, Accuracy: 9183/10000 (92%)\n","---->Test set 7: Average loss: 0.0003, Accuracy: 9106/10000 (91%)\n","---->Test set 8: Average loss: 0.0035, Accuracy: 1310/10000 (13%)\n","---->Test set 9: Average loss: 0.0033, Accuracy: 1028/10000 (10%)\n","--> Training Task 8:\n","----> Epoch 0:\n","---->[0/60000 (0%)]\tPrecision: 0.093750\tLoss: 3.888070\tSurrogate Loss: 0.000000\tTotal Loss: 3.888070\n","---->[12800/60000 (21%)]\tPrecision: 0.734375\tLoss: 0.811029\tSurrogate Loss: 14.595548\tTotal Loss: 2.270583\n","---->[25600/60000 (43%)]\tPrecision: 0.890625\tLoss: 0.254517\tSurrogate Loss: 10.243288\tTotal Loss: 1.278846\n","---->[38400/60000 (64%)]\tPrecision: 0.906250\tLoss: 0.254208\tSurrogate Loss: 10.088184\tTotal Loss: 1.263027\n","---->[51200/60000 (85%)]\tPrecision: 0.843750\tLoss: 0.341256\tSurrogate Loss: 9.884053\tTotal Loss: 1.329661\n","----> Epoch 1:\n","---->[0/60000 (0%)]\tPrecision: 0.921875\tLoss: 0.221631\tSurrogate Loss: 8.956963\tTotal Loss: 1.117328\n","---->[12800/60000 (21%)]\tPrecision: 0.859375\tLoss: 0.649753\tSurrogate Loss: 5.806190\tTotal Loss: 1.230372\n","---->[25600/60000 (43%)]\tPrecision: 0.906250\tLoss: 0.457375\tSurrogate Loss: 7.403941\tTotal Loss: 1.197769\n","---->[38400/60000 (64%)]\tPrecision: 0.968750\tLoss: 0.183986\tSurrogate Loss: 6.995310\tTotal Loss: 0.883517\n","---->[51200/60000 (85%)]\tPrecision: 0.937500\tLoss: 0.235812\tSurrogate Loss: 6.508687\tTotal Loss: 0.886681\n","----> Epoch 2:\n","---->[0/60000 (0%)]\tPrecision: 0.921875\tLoss: 0.198682\tSurrogate Loss: 6.495170\tTotal Loss: 0.848199\n","---->[12800/60000 (21%)]\tPrecision: 0.921875\tLoss: 0.157176\tSurrogate Loss: 6.232505\tTotal Loss: 0.780426\n","---->[25600/60000 (43%)]\tPrecision: 0.984375\tLoss: 0.211793\tSurrogate Loss: 7.767482\tTotal Loss: 0.988541\n","---->[38400/60000 (64%)]\tPrecision: 0.781250\tLoss: 1.008950\tSurrogate Loss: 7.566445\tTotal Loss: 1.765594\n","---->[51200/60000 (85%)]\tPrecision: 0.937500\tLoss: 0.182955\tSurrogate Loss: 6.805624\tTotal Loss: 0.863518\n","[3.8880696296691895, 0.8110286593437195, 0.2545170783996582, 0.2542082369327545, 0.34125569462776184, 0.2216314971446991, 0.6497526168823242, 0.45737484097480774, 0.183986097574234, 0.2358122169971466, 0.19868203997612, 0.15717589855194092, 0.21179302036762238, 1.0089499950408936, 0.1829553097486496]\n","[3.8880696296691895, 2.270583391189575, 1.2788459062576294, 1.2630267143249512, 1.3296610116958618, 1.1173278093338013, 1.2303717136383057, 1.1977689266204834, 0.8835171461105347, 0.8866809606552124, 0.8481990098953247, 0.7804263830184937, 0.9885411858558655, 1.765594482421875, 0.863517701625824]\n","--> Finished Training Task 8. Starting Test phase:\n","---->Test set 0: Average loss: 0.0003, Accuracy: 9238/10000 (92%)\n","---->Test set 1: Average loss: 0.0006, Accuracy: 8312/10000 (83%)\n","---->Test set 2: Average loss: 0.0006, Accuracy: 8053/10000 (81%)\n","---->Test set 3: Average loss: 0.0005, Accuracy: 8417/10000 (84%)\n","---->Test set 4: Average loss: 0.0005, Accuracy: 8435/10000 (84%)\n","---->Test set 5: Average loss: 0.0006, Accuracy: 8207/10000 (82%)\n","---->Test set 6: Average loss: 0.0003, Accuracy: 9046/10000 (90%)\n","---->Test set 7: Average loss: 0.0004, Accuracy: 8879/10000 (89%)\n","---->Test set 8: Average loss: 0.0003, Accuracy: 9164/10000 (92%)\n","---->Test set 9: Average loss: 0.0034, Accuracy: 1001/10000 (10%)\n","--> Training Task 9:\n","----> Epoch 0:\n","---->[0/60000 (0%)]\tPrecision: 0.140625\tLoss: 3.066564\tSurrogate Loss: 0.000000\tTotal Loss: 3.066564\n","---->[12800/60000 (21%)]\tPrecision: 0.656250\tLoss: 0.985276\tSurrogate Loss: 21.122141\tTotal Loss: 3.097490\n","---->[25600/60000 (43%)]\tPrecision: 0.765625\tLoss: 1.128932\tSurrogate Loss: 16.041012\tTotal Loss: 2.733034\n","---->[38400/60000 (64%)]\tPrecision: 0.781250\tLoss: 0.583906\tSurrogate Loss: 11.814348\tTotal Loss: 1.765341\n","---->[51200/60000 (85%)]\tPrecision: 0.796875\tLoss: 0.542233\tSurrogate Loss: 8.396715\tTotal Loss: 1.381904\n","----> Epoch 1:\n","---->[0/60000 (0%)]\tPrecision: 0.875000\tLoss: 0.387582\tSurrogate Loss: 6.898834\tTotal Loss: 1.077465\n","---->[12800/60000 (21%)]\tPrecision: 0.968750\tLoss: 0.135947\tSurrogate Loss: 9.757393\tTotal Loss: 1.111686\n","---->[25600/60000 (43%)]\tPrecision: 0.953125\tLoss: 0.116773\tSurrogate Loss: 5.909986\tTotal Loss: 0.707771\n","---->[38400/60000 (64%)]\tPrecision: 0.906250\tLoss: 0.351509\tSurrogate Loss: 7.431184\tTotal Loss: 1.094627\n","---->[51200/60000 (85%)]\tPrecision: 0.906250\tLoss: 0.276451\tSurrogate Loss: 7.748141\tTotal Loss: 1.051265\n","----> Epoch 2:\n","---->[0/60000 (0%)]\tPrecision: 0.937500\tLoss: 0.411477\tSurrogate Loss: 7.837351\tTotal Loss: 1.195212\n","---->[12800/60000 (21%)]\tPrecision: 0.921875\tLoss: 0.418893\tSurrogate Loss: 5.689038\tTotal Loss: 0.987797\n","---->[25600/60000 (43%)]\tPrecision: 0.875000\tLoss: 0.674933\tSurrogate Loss: 11.847895\tTotal Loss: 1.859723\n","---->[38400/60000 (64%)]\tPrecision: 0.875000\tLoss: 0.362746\tSurrogate Loss: 6.404377\tTotal Loss: 1.003184\n","---->[51200/60000 (85%)]\tPrecision: 0.937500\tLoss: 0.245075\tSurrogate Loss: 7.884958\tTotal Loss: 1.033571\n","[3.066563606262207, 0.9852761030197144, 1.1289324760437012, 0.5839064121246338, 0.542232871055603, 0.3875815272331238, 0.13594657182693481, 0.11677265912294388, 0.35150864720344543, 0.27645131945610046, 0.4114767611026764, 0.4188932180404663, 0.6749330759048462, 0.36274635791778564, 0.24507547914981842]\n","[3.066563606262207, 3.0974903106689453, 2.7330336570739746, 1.7653412818908691, 1.3819043636322021, 1.0774649381637573, 1.111685872077942, 0.7077712416648865, 1.094627022743225, 1.0512653589248657, 1.1952118873596191, 0.9877970814704895, 1.85972261428833, 1.0031840801239014, 1.0335713624954224]\n","--> Finished Training Task 9. Starting Test phase:\n","---->Test set 0: Average loss: 0.0003, Accuracy: 9250/10000 (92%)\n","---->Test set 1: Average loss: 0.0005, Accuracy: 8560/10000 (86%)\n","---->Test set 2: Average loss: 0.0014, Accuracy: 6674/10000 (67%)\n","---->Test set 3: Average loss: 0.0006, Accuracy: 8311/10000 (83%)\n","---->Test set 4: Average loss: 0.0007, Accuracy: 7906/10000 (79%)\n","---->Test set 5: Average loss: 0.0006, Accuracy: 8364/10000 (84%)\n","---->Test set 6: Average loss: 0.0005, Accuracy: 8504/10000 (85%)\n","---->Test set 7: Average loss: 0.0005, Accuracy: 8536/10000 (85%)\n","---->Test set 8: Average loss: 0.0005, Accuracy: 8664/10000 (87%)\n","---->Test set 9: Average loss: 0.0003, Accuracy: 9154/10000 (92%)\n","Accuracy: 0.8392299999999999\n","Confusion matrix:\n","0.1223,0.106,0.1062,0.124,0.1104,0.1095,0.1599,0.0786,0.1017,0.1779\n","0.9553,0.0516,0.1355,0.0994,0.1432,0.1087,0.0865,0.1186,0.063,0.1124\n","0.9539,0.9356,0.1063,0.1303,0.1069,0.0912,0.1115,0.1348,0.0993,0.1325\n","0.9504,0.9416,0.93,0.111,0.1235,0.0855,0.1106,0.1471,0.1111,0.1277\n","0.9415,0.9192,0.9183,0.928,0.1472,0.0869,0.1204,0.1104,0.1129,0.1277\n","0.9421,0.9175,0.9258,0.9322,0.9353,0.1015,0.1054,0.1452,0.1172,0.1102\n","0.9431,0.9225,0.9198,0.925,0.9291,0.928,0.1073,0.1778,0.1143,0.1072\n","0.9386,0.9061,0.9029,0.9171,0.9001,0.9053,0.9244,0.1373,0.1373,0.1002\n","0.9327,0.889,0.862,0.9037,0.8734,0.8876,0.9183,0.9106,0.131,0.1028\n","0.9238,0.8312,0.8053,0.8417,0.8435,0.8207,0.9046,0.8879,0.9164,0.1001\n","0.925,0.856,0.6674,0.8311,0.7906,0.8364,0.8504,0.8536,0.8664,0.9154\n","--> Training:\n","---->Test set 0: Average loss: 0.1635, Accuracy: 1478/10000 (15%)\n","---->Test set 1: Average loss: 0.2187, Accuracy: 742/10000 (7%)\n","---->Test set 2: Average loss: 0.1876, Accuracy: 618/10000 (6%)\n","---->Test set 3: Average loss: 0.1764, Accuracy: 965/10000 (10%)\n","---->Test set 4: Average loss: 0.2105, Accuracy: 994/10000 (10%)\n","---->Test set 5: Average loss: 0.2153, Accuracy: 497/10000 (5%)\n","---->Test set 6: Average loss: 0.2127, Accuracy: 1025/10000 (10%)\n","---->Test set 7: Average loss: 0.1828, Accuracy: 1166/10000 (12%)\n","---->Test set 8: Average loss: 0.1779, Accuracy: 1126/10000 (11%)\n","---->Test set 9: Average loss: 0.1710, Accuracy: 1161/10000 (12%)\n","--> Training Task 0:\n","----> Epoch 0:\n","---->[0/60000 (0%)]\tPrecision: 0.125000\tLoss: 142.351761\tSurrogate Loss: 0.000000\tTotal Loss: 142.351761\n","---->[12800/60000 (21%)]\tPrecision: 0.906250\tLoss: 0.298576\tSurrogate Loss: 0.000000\tTotal Loss: 0.298576\n","---->[25600/60000 (43%)]\tPrecision: 0.968750\tLoss: 0.118533\tSurrogate Loss: 0.000000\tTotal Loss: 0.118533\n","---->[38400/60000 (64%)]\tPrecision: 0.875000\tLoss: 0.602649\tSurrogate Loss: 0.000000\tTotal Loss: 0.602649\n","---->[51200/60000 (85%)]\tPrecision: 0.953125\tLoss: 0.153937\tSurrogate Loss: 0.000000\tTotal Loss: 0.153937\n","----> Epoch 1:\n","---->[0/60000 (0%)]\tPrecision: 0.968750\tLoss: 0.047355\tSurrogate Loss: 0.000000\tTotal Loss: 0.047355\n","---->[12800/60000 (21%)]\tPrecision: 0.937500\tLoss: 0.176889\tSurrogate Loss: 0.000000\tTotal Loss: 0.176889\n","---->[25600/60000 (43%)]\tPrecision: 0.921875\tLoss: 0.397969\tSurrogate Loss: 0.000000\tTotal Loss: 0.397969\n","---->[38400/60000 (64%)]\tPrecision: 0.968750\tLoss: 0.135787\tSurrogate Loss: 0.000000\tTotal Loss: 0.135787\n","---->[51200/60000 (85%)]\tPrecision: 0.968750\tLoss: 0.081203\tSurrogate Loss: 0.000000\tTotal Loss: 0.081203\n","----> Epoch 2:\n","---->[0/60000 (0%)]\tPrecision: 0.968750\tLoss: 0.062629\tSurrogate Loss: 0.000000\tTotal Loss: 0.062629\n","---->[12800/60000 (21%)]\tPrecision: 1.000000\tLoss: 0.038032\tSurrogate Loss: 0.000000\tTotal Loss: 0.038032\n","---->[25600/60000 (43%)]\tPrecision: 1.000000\tLoss: 0.042092\tSurrogate Loss: 0.000000\tTotal Loss: 0.042092\n","---->[38400/60000 (64%)]\tPrecision: 0.937500\tLoss: 0.260548\tSurrogate Loss: 0.000000\tTotal Loss: 0.260548\n","---->[51200/60000 (85%)]\tPrecision: 0.968750\tLoss: 0.153577\tSurrogate Loss: 0.000000\tTotal Loss: 0.153577\n","----> Epoch 3:\n","---->[0/60000 (0%)]\tPrecision: 0.953125\tLoss: 0.178275\tSurrogate Loss: 0.000000\tTotal Loss: 0.178275\n","---->[12800/60000 (21%)]\tPrecision: 0.921875\tLoss: 0.235839\tSurrogate Loss: 0.000000\tTotal Loss: 0.235839\n","---->[25600/60000 (43%)]\tPrecision: 0.906250\tLoss: 0.424096\tSurrogate Loss: 0.000000\tTotal Loss: 0.424096\n","---->[38400/60000 (64%)]\tPrecision: 1.000000\tLoss: 0.024091\tSurrogate Loss: 0.000000\tTotal Loss: 0.024091\n","---->[51200/60000 (85%)]\tPrecision: 0.906250\tLoss: 0.402843\tSurrogate Loss: 0.000000\tTotal Loss: 0.402843\n","[142.3517608642578, 0.29857611656188965, 0.11853256821632385, 0.6026485562324524, 0.15393690764904022, 0.047354888170957565, 0.17688925564289093, 0.39796900749206543, 0.135787233710289, 0.08120254427194595, 0.06262911111116409, 0.03803197294473648, 0.04209167882800102, 0.2605476379394531, 0.15357697010040283, 0.17827491462230682, 0.23583859205245972, 0.4240962266921997, 0.02409142628312111, 0.40284281969070435]\n","[142.3517608642578, 0.29857611656188965, 0.11853256821632385, 0.6026485562324524, 0.15393690764904022, 0.047354888170957565, 0.17688925564289093, 0.39796900749206543, 0.135787233710289, 0.08120254427194595, 0.06262911111116409, 0.03803197294473648, 0.04209167882800102, 0.2605476379394531, 0.15357697010040283, 0.17827491462230682, 0.23583859205245972, 0.4240962266921997, 0.02409142628312111, 0.40284281969070435]\n","--> Finished Training Task 0. Starting Test phase:\n","---->Test set 0: Average loss: 0.0002, Accuracy: 9553/10000 (96%)\n","---->Test set 1: Average loss: 0.0065, Accuracy: 882/10000 (9%)\n","---->Test set 2: Average loss: 0.0064, Accuracy: 963/10000 (10%)\n","---->Test set 3: Average loss: 0.0063, Accuracy: 810/10000 (8%)\n","---->Test set 4: Average loss: 0.0047, Accuracy: 677/10000 (7%)\n","---->Test set 5: Average loss: 0.0035, Accuracy: 1518/10000 (15%)\n","---->Test set 6: Average loss: 0.0061, Accuracy: 1037/10000 (10%)\n","---->Test set 7: Average loss: 0.0054, Accuracy: 898/10000 (9%)\n","---->Test set 8: Average loss: 0.0080, Accuracy: 942/10000 (9%)\n","---->Test set 9: Average loss: 0.0052, Accuracy: 895/10000 (9%)\n","--> Training Task 1:\n","----> Epoch 0:\n","---->[0/60000 (0%)]\tPrecision: 0.078125\tLoss: 6.299266\tSurrogate Loss: 0.000000\tTotal Loss: 6.299266\n","---->[12800/60000 (21%)]\tPrecision: 0.062500\tLoss: 2.311637\tSurrogate Loss: 10.345747\tTotal Loss: 3.346212\n","---->[25600/60000 (43%)]\tPrecision: 0.140625\tLoss: 2.302699\tSurrogate Loss: 3.820760\tTotal Loss: 2.684775\n","---->[38400/60000 (64%)]\tPrecision: 0.109375\tLoss: 2.305890\tSurrogate Loss: 2.272132\tTotal Loss: 2.533103\n","---->[51200/60000 (85%)]\tPrecision: 0.109375\tLoss: 2.298627\tSurrogate Loss: 3.000598\tTotal Loss: 2.598686\n","----> Epoch 1:\n","---->[0/60000 (0%)]\tPrecision: 0.015625\tLoss: 2.305766\tSurrogate Loss: 2.791417\tTotal Loss: 2.584908\n","---->[12800/60000 (21%)]\tPrecision: 0.109375\tLoss: 2.313330\tSurrogate Loss: 1.913996\tTotal Loss: 2.504729\n","---->[25600/60000 (43%)]\tPrecision: 0.109375\tLoss: 2.296016\tSurrogate Loss: 1.383654\tTotal Loss: 2.434381\n","---->[38400/60000 (64%)]\tPrecision: 0.125000\tLoss: 2.302505\tSurrogate Loss: 1.052510\tTotal Loss: 2.407756\n","---->[51200/60000 (85%)]\tPrecision: 0.109375\tLoss: 2.311867\tSurrogate Loss: 0.835647\tTotal Loss: 2.395432\n","----> Epoch 2:\n","---->[0/60000 (0%)]\tPrecision: 0.140625\tLoss: 2.291997\tSurrogate Loss: 5.957869\tTotal Loss: 2.887784\n","---->[12800/60000 (21%)]\tPrecision: 0.109375\tLoss: 2.305234\tSurrogate Loss: 1.450998\tTotal Loss: 2.450334\n","---->[25600/60000 (43%)]\tPrecision: 0.062500\tLoss: 2.315460\tSurrogate Loss: 1.336350\tTotal Loss: 2.449095\n","---->[38400/60000 (64%)]\tPrecision: 0.093750\tLoss: 2.305134\tSurrogate Loss: 0.998725\tTotal Loss: 2.405007\n","---->[51200/60000 (85%)]\tPrecision: 0.109375\tLoss: 2.302979\tSurrogate Loss: 0.888776\tTotal Loss: 2.391856\n","----> Epoch 3:\n","---->[0/60000 (0%)]\tPrecision: 0.140625\tLoss: 2.306673\tSurrogate Loss: 0.737071\tTotal Loss: 2.380380\n","---->[12800/60000 (21%)]\tPrecision: 0.109375\tLoss: 2.304295\tSurrogate Loss: 0.610131\tTotal Loss: 2.365308\n","---->[25600/60000 (43%)]\tPrecision: 0.093750\tLoss: 2.313334\tSurrogate Loss: 2.303198\tTotal Loss: 2.543653\n","---->[38400/60000 (64%)]\tPrecision: 0.093750\tLoss: 2.300976\tSurrogate Loss: 2.414815\tTotal Loss: 2.542457\n","---->[51200/60000 (85%)]\tPrecision: 0.031250\tLoss: 2.309577\tSurrogate Loss: 7.753530\tTotal Loss: 3.084930\n","[6.2992658615112305, 2.3116371631622314, 2.302699089050293, 2.3058903217315674, 2.298626661300659, 2.3057661056518555, 2.3133296966552734, 2.296015501022339, 2.3025050163269043, 2.3118674755096436, 2.291997194290161, 2.30523419380188, 2.315459728240967, 2.3051342964172363, 2.302978754043579, 2.306673049926758, 2.3042948246002197, 2.313333749771118, 2.300975799560547, 2.309577465057373]\n","[6.2992658615112305, 3.3462119102478027, 2.6847751140594482, 2.5331034660339355, 2.598686456680298, 2.5849077701568604, 2.5047292709350586, 2.4343810081481934, 2.4077560901641846, 2.395432233810425, 2.887784004211426, 2.450334072113037, 2.449094772338867, 2.4050066471099854, 2.3918564319610596, 2.380380153656006, 2.3653078079223633, 2.5436534881591797, 2.542457342147827, 3.084930419921875]\n","--> Finished Training Task 1. Starting Test phase:\n","---->Test set 0: Average loss: 0.0002, Accuracy: 9450/10000 (94%)\n","---->Test set 1: Average loss: 0.0023, Accuracy: 1009/10000 (10%)\n","---->Test set 2: Average loss: 0.0023, Accuracy: 1007/10000 (10%)\n","---->Test set 3: Average loss: 0.0023, Accuracy: 1010/10000 (10%)\n","---->Test set 4: Average loss: 0.0023, Accuracy: 1009/10000 (10%)\n","---->Test set 5: Average loss: 0.0023, Accuracy: 1009/10000 (10%)\n","---->Test set 6: Average loss: 0.0023, Accuracy: 1009/10000 (10%)\n","---->Test set 7: Average loss: 0.0023, Accuracy: 1008/10000 (10%)\n","---->Test set 8: Average loss: 0.0023, Accuracy: 1009/10000 (10%)\n","---->Test set 9: Average loss: 0.0023, Accuracy: 1009/10000 (10%)\n","--> Training Task 2:\n","----> Epoch 0:\n","---->[0/60000 (0%)]\tPrecision: 0.078125\tLoss: 2.309749\tSurrogate Loss: 0.000000\tTotal Loss: 2.309749\n","---->[12800/60000 (21%)]\tPrecision: 0.093750\tLoss: 2.302689\tSurrogate Loss: 0.053067\tTotal Loss: 2.307995\n","---->[25600/60000 (43%)]\tPrecision: 0.046875\tLoss: 2.301539\tSurrogate Loss: 0.013965\tTotal Loss: 2.302935\n","---->[38400/60000 (64%)]\tPrecision: 0.046875\tLoss: 2.310893\tSurrogate Loss: 0.009338\tTotal Loss: 2.311827\n","---->[51200/60000 (85%)]\tPrecision: 0.125000\tLoss: 2.302920\tSurrogate Loss: 0.006190\tTotal Loss: 2.303539\n","----> Epoch 1:\n","---->[0/60000 (0%)]\tPrecision: 0.062500\tLoss: 2.306899\tSurrogate Loss: 0.004115\tTotal Loss: 2.307310\n","---->[12800/60000 (21%)]\tPrecision: 0.078125\tLoss: 2.300060\tSurrogate Loss: 0.003794\tTotal Loss: 2.300439\n","---->[25600/60000 (43%)]\tPrecision: 0.078125\tLoss: 2.305330\tSurrogate Loss: 0.006252\tTotal Loss: 2.305955\n","---->[38400/60000 (64%)]\tPrecision: 0.093750\tLoss: 2.304932\tSurrogate Loss: 0.003951\tTotal Loss: 2.305327\n","---->[51200/60000 (85%)]\tPrecision: 0.109375\tLoss: 2.302613\tSurrogate Loss: 0.009459\tTotal Loss: 2.303559\n","----> Epoch 2:\n","---->[0/60000 (0%)]\tPrecision: 0.093750\tLoss: 2.305024\tSurrogate Loss: 0.004816\tTotal Loss: 2.305506\n","---->[12800/60000 (21%)]\tPrecision: 0.109375\tLoss: 2.299273\tSurrogate Loss: 0.007571\tTotal Loss: 2.300030\n","---->[25600/60000 (43%)]\tPrecision: 0.171875\tLoss: 2.298264\tSurrogate Loss: 0.007676\tTotal Loss: 2.299031\n","---->[38400/60000 (64%)]\tPrecision: 0.109375\tLoss: 2.294220\tSurrogate Loss: 0.004752\tTotal Loss: 2.294695\n","---->[51200/60000 (85%)]\tPrecision: 0.125000\tLoss: 2.301347\tSurrogate Loss: 0.005102\tTotal Loss: 2.301857\n","----> Epoch 3:\n","---->[0/60000 (0%)]\tPrecision: 0.046875\tLoss: 2.301815\tSurrogate Loss: 0.006204\tTotal Loss: 2.302435\n","---->[12800/60000 (21%)]\tPrecision: 0.078125\tLoss: 2.305155\tSurrogate Loss: 0.009150\tTotal Loss: 2.306070\n","---->[25600/60000 (43%)]\tPrecision: 0.171875\tLoss: 2.300648\tSurrogate Loss: 0.004308\tTotal Loss: 2.301079\n","---->[38400/60000 (64%)]\tPrecision: 0.125000\tLoss: 2.305213\tSurrogate Loss: 0.006020\tTotal Loss: 2.305815\n","---->[51200/60000 (85%)]\tPrecision: 0.093750\tLoss: 2.305438\tSurrogate Loss: 0.005847\tTotal Loss: 2.306022\n","[2.309749126434326, 2.3026885986328125, 2.3015387058258057, 2.3108928203582764, 2.302919626235962, 2.306898832321167, 2.3000595569610596, 2.3053295612335205, 2.3049323558807373, 2.302612543106079, 2.3050241470336914, 2.2992734909057617, 2.2982635498046875, 2.294219732284546, 2.301346778869629, 2.3018150329589844, 2.305155038833618, 2.3006482124328613, 2.305213451385498, 2.3054375648498535]\n","[2.309749126434326, 2.307995319366455, 2.3029351234436035, 2.311826705932617, 2.3035385608673096, 2.3073103427886963, 2.30043888092041, 2.305954694747925, 2.3053274154663086, 2.303558588027954, 2.3055057525634766, 2.300030469894409, 2.2990312576293945, 2.2946949005126953, 2.3018569946289062, 2.3024353981018066, 2.30607008934021, 2.301079034805298, 2.3058154582977295, 2.3060221672058105]\n","--> Finished Training Task 2. Starting Test phase:\n","---->Test set 0: Average loss: 0.0002, Accuracy: 9440/10000 (94%)\n","---->Test set 1: Average loss: 0.0023, Accuracy: 1009/10000 (10%)\n","---->Test set 2: Average loss: 0.0023, Accuracy: 1009/10000 (10%)\n","---->Test set 3: Average loss: 0.0023, Accuracy: 1009/10000 (10%)\n","---->Test set 4: Average loss: 0.0023, Accuracy: 1009/10000 (10%)\n","---->Test set 5: Average loss: 0.0023, Accuracy: 1009/10000 (10%)\n","---->Test set 6: Average loss: 0.0023, Accuracy: 1009/10000 (10%)\n","---->Test set 7: Average loss: 0.0023, Accuracy: 1008/10000 (10%)\n","---->Test set 8: Average loss: 0.0023, Accuracy: 1009/10000 (10%)\n","---->Test set 9: Average loss: 0.0023, Accuracy: 1009/10000 (10%)\n","--> Training Task 3:\n","----> Epoch 0:\n","---->[0/60000 (0%)]\tPrecision: 0.093750\tLoss: 2.302221\tSurrogate Loss: 0.000000\tTotal Loss: 2.302221\n","---->[12800/60000 (21%)]\tPrecision: 0.078125\tLoss: 2.301428\tSurrogate Loss: 0.002217\tTotal Loss: 2.301650\n","---->[25600/60000 (43%)]\tPrecision: 0.125000\tLoss: 2.305990\tSurrogate Loss: 0.003584\tTotal Loss: 2.306348\n","---->[38400/60000 (64%)]\tPrecision: 0.125000\tLoss: 2.300884\tSurrogate Loss: 0.004756\tTotal Loss: 2.301360\n","---->[51200/60000 (85%)]\tPrecision: 0.062500\tLoss: 2.302706\tSurrogate Loss: 0.002887\tTotal Loss: 2.302995\n","----> Epoch 1:\n","---->[0/60000 (0%)]\tPrecision: 0.078125\tLoss: 2.302223\tSurrogate Loss: 0.002496\tTotal Loss: 2.302473\n","---->[12800/60000 (21%)]\tPrecision: 0.093750\tLoss: 2.305405\tSurrogate Loss: 0.002714\tTotal Loss: 2.305676\n","---->[25600/60000 (43%)]\tPrecision: 0.140625\tLoss: 2.295542\tSurrogate Loss: 0.003161\tTotal Loss: 2.295858\n","---->[38400/60000 (64%)]\tPrecision: 0.062500\tLoss: 2.304340\tSurrogate Loss: 0.002190\tTotal Loss: 2.304559\n","---->[51200/60000 (85%)]\tPrecision: 0.109375\tLoss: 2.300561\tSurrogate Loss: 0.000705\tTotal Loss: 2.300632\n","----> Epoch 2:\n","---->[0/60000 (0%)]\tPrecision: 0.093750\tLoss: 2.300371\tSurrogate Loss: 0.002028\tTotal Loss: 2.300574\n","---->[12800/60000 (21%)]\tPrecision: 0.140625\tLoss: 2.298237\tSurrogate Loss: 0.003059\tTotal Loss: 2.298542\n","---->[25600/60000 (43%)]\tPrecision: 0.046875\tLoss: 2.302721\tSurrogate Loss: 0.005483\tTotal Loss: 2.303269\n","---->[38400/60000 (64%)]\tPrecision: 0.078125\tLoss: 2.301233\tSurrogate Loss: 0.002680\tTotal Loss: 2.301501\n","---->[51200/60000 (85%)]\tPrecision: 0.203125\tLoss: 2.297765\tSurrogate Loss: 0.001710\tTotal Loss: 2.297936\n","----> Epoch 3:\n","---->[0/60000 (0%)]\tPrecision: 0.093750\tLoss: 2.301430\tSurrogate Loss: 0.004780\tTotal Loss: 2.301908\n","---->[12800/60000 (21%)]\tPrecision: 0.078125\tLoss: 2.302712\tSurrogate Loss: 0.002699\tTotal Loss: 2.302982\n","---->[25600/60000 (43%)]\tPrecision: 0.093750\tLoss: 2.300875\tSurrogate Loss: 0.004370\tTotal Loss: 2.301312\n","---->[38400/60000 (64%)]\tPrecision: 0.109375\tLoss: 2.305594\tSurrogate Loss: 0.002458\tTotal Loss: 2.305840\n","---->[51200/60000 (85%)]\tPrecision: 0.031250\tLoss: 2.303550\tSurrogate Loss: 0.002062\tTotal Loss: 2.303756\n","[2.3022210597991943, 2.3014280796051025, 2.3059895038604736, 2.3008840084075928, 2.302706241607666, 2.302222967147827, 2.3054049015045166, 2.295542001724243, 2.30433988571167, 2.3005614280700684, 2.3003714084625244, 2.298236608505249, 2.302720546722412, 2.3012325763702393, 2.2977652549743652, 2.3014297485351562, 2.3027124404907227, 2.300875186920166, 2.305593729019165, 2.3035502433776855]\n","[2.3022210597991943, 2.301649808883667, 2.3063478469848633, 2.3013596534729004, 2.302994966506958, 2.3024725914001465, 2.305676221847534, 2.295858144760132, 2.304558753967285, 2.3006319999694824, 2.3005740642547607, 2.2985424995422363, 2.3032689094543457, 2.3015005588531494, 2.297936201095581, 2.301907777786255, 2.3029823303222656, 2.301312208175659, 2.3058395385742188, 2.3037564754486084]\n","--> Finished Training Task 3. Starting Test phase:\n","---->Test set 0: Average loss: 0.0002, Accuracy: 9435/10000 (94%)\n","---->Test set 1: Average loss: 0.0023, Accuracy: 1009/10000 (10%)\n","---->Test set 2: Average loss: 0.0023, Accuracy: 1009/10000 (10%)\n","---->Test set 3: Average loss: 0.0023, Accuracy: 1009/10000 (10%)\n","---->Test set 4: Average loss: 0.0023, Accuracy: 1009/10000 (10%)\n","---->Test set 5: Average loss: 0.0023, Accuracy: 1009/10000 (10%)\n","---->Test set 6: Average loss: 0.0023, Accuracy: 1009/10000 (10%)\n","---->Test set 7: Average loss: 0.0023, Accuracy: 1008/10000 (10%)\n","---->Test set 8: Average loss: 0.0023, Accuracy: 1009/10000 (10%)\n","---->Test set 9: Average loss: 0.0023, Accuracy: 1009/10000 (10%)\n","--> Training Task 4:\n","----> Epoch 0:\n","---->[0/60000 (0%)]\tPrecision: 0.093750\tLoss: 2.302247\tSurrogate Loss: 0.000000\tTotal Loss: 2.302247\n","---->[12800/60000 (21%)]\tPrecision: 0.109375\tLoss: 2.298364\tSurrogate Loss: 6.414198\tTotal Loss: 2.939784\n","---->[25600/60000 (43%)]\tPrecision: 0.078125\tLoss: 2.303929\tSurrogate Loss: 1.484369\tTotal Loss: 2.452365\n","---->[38400/60000 (64%)]\tPrecision: 0.125000\tLoss: 2.299883\tSurrogate Loss: 0.655582\tTotal Loss: 2.365441\n","---->[51200/60000 (85%)]\tPrecision: 0.125000\tLoss: 2.300735\tSurrogate Loss: 0.410234\tTotal Loss: 2.341758\n","----> Epoch 1:\n","---->[0/60000 (0%)]\tPrecision: 0.078125\tLoss: 2.304543\tSurrogate Loss: 0.315863\tTotal Loss: 2.336130\n","---->[12800/60000 (21%)]\tPrecision: 0.171875\tLoss: 2.297779\tSurrogate Loss: 0.224378\tTotal Loss: 2.320217\n","---->[25600/60000 (43%)]\tPrecision: 0.093750\tLoss: 2.300552\tSurrogate Loss: 0.166878\tTotal Loss: 2.317240\n","---->[38400/60000 (64%)]\tPrecision: 0.109375\tLoss: 2.302443\tSurrogate Loss: 0.127810\tTotal Loss: 2.315224\n","---->[51200/60000 (85%)]\tPrecision: 0.078125\tLoss: 2.304410\tSurrogate Loss: 0.102141\tTotal Loss: 2.314624\n","----> Epoch 2:\n","---->[0/60000 (0%)]\tPrecision: 0.078125\tLoss: 2.301126\tSurrogate Loss: 0.089980\tTotal Loss: 2.310124\n","---->[12800/60000 (21%)]\tPrecision: 0.109375\tLoss: 2.304078\tSurrogate Loss: 0.066641\tTotal Loss: 2.310742\n","---->[25600/60000 (43%)]\tPrecision: 0.078125\tLoss: 2.297354\tSurrogate Loss: 0.054086\tTotal Loss: 2.302763\n","---->[38400/60000 (64%)]\tPrecision: 0.062500\tLoss: 2.300711\tSurrogate Loss: 0.044959\tTotal Loss: 2.305207\n","---->[51200/60000 (85%)]\tPrecision: 0.046875\tLoss: 2.300949\tSurrogate Loss: 0.039463\tTotal Loss: 2.304895\n","----> Epoch 3:\n","---->[0/60000 (0%)]\tPrecision: 0.140625\tLoss: 2.302442\tSurrogate Loss: 0.033874\tTotal Loss: 2.305829\n","---->[12800/60000 (21%)]\tPrecision: 0.078125\tLoss: 2.304354\tSurrogate Loss: 0.029453\tTotal Loss: 2.307299\n","---->[25600/60000 (43%)]\tPrecision: 0.093750\tLoss: 2.303807\tSurrogate Loss: 0.026036\tTotal Loss: 2.306411\n","---->[38400/60000 (64%)]\tPrecision: 0.109375\tLoss: 2.303125\tSurrogate Loss: 0.022431\tTotal Loss: 2.305368\n","---->[51200/60000 (85%)]\tPrecision: 0.125000\tLoss: 2.302782\tSurrogate Loss: 0.018948\tTotal Loss: 2.304677\n","[2.3022472858428955, 2.29836368560791, 2.3039286136627197, 2.2998831272125244, 2.300734519958496, 2.3045434951782227, 2.2977793216705322, 2.3005518913269043, 2.302443265914917, 2.3044097423553467, 2.301126480102539, 2.3040781021118164, 2.2973544597625732, 2.300711154937744, 2.3009488582611084, 2.3024415969848633, 2.304353713989258, 2.303807020187378, 2.3031246662139893, 2.302781820297241]\n","[2.3022472858428955, 2.9397835731506348, 2.4523653984069824, 2.36544132232666, 2.3417580127716064, 2.336129903793335, 2.3202171325683594, 2.317239761352539, 2.3152244091033936, 2.3146238327026367, 2.310124397277832, 2.310742139816284, 2.302762985229492, 2.3052070140838623, 2.3048951625823975, 2.3058290481567383, 2.3072988986968994, 2.306410551071167, 2.3053677082061768, 2.3046767711639404]\n","--> Finished Training Task 4. Starting Test phase:\n","---->Test set 0: Average loss: 0.0002, Accuracy: 9436/10000 (94%)\n","---->Test set 1: Average loss: 0.0023, Accuracy: 1009/10000 (10%)\n","---->Test set 2: Average loss: 0.0023, Accuracy: 1009/10000 (10%)\n","---->Test set 3: Average loss: 0.0023, Accuracy: 1009/10000 (10%)\n","---->Test set 4: Average loss: 0.0023, Accuracy: 1009/10000 (10%)\n","---->Test set 5: Average loss: 0.0023, Accuracy: 1009/10000 (10%)\n","---->Test set 6: Average loss: 0.0023, Accuracy: 1009/10000 (10%)\n","---->Test set 7: Average loss: 0.0023, Accuracy: 1009/10000 (10%)\n","---->Test set 8: Average loss: 0.0023, Accuracy: 1009/10000 (10%)\n","---->Test set 9: Average loss: 0.0023, Accuracy: 1009/10000 (10%)\n","--> Training Task 5:\n","----> Epoch 0:\n","---->[0/60000 (0%)]\tPrecision: 0.093750\tLoss: 2.304511\tSurrogate Loss: 0.000000\tTotal Loss: 2.304511\n","---->[12800/60000 (21%)]\tPrecision: 0.046875\tLoss: 2.303951\tSurrogate Loss: 0.389452\tTotal Loss: 2.342896\n","---->[25600/60000 (43%)]\tPrecision: 0.140625\tLoss: 2.302563\tSurrogate Loss: 0.009575\tTotal Loss: 2.303520\n","---->[38400/60000 (64%)]\tPrecision: 0.062500\tLoss: 2.304013\tSurrogate Loss: 0.016716\tTotal Loss: 2.305684\n","---->[51200/60000 (85%)]\tPrecision: 0.140625\tLoss: 2.301766\tSurrogate Loss: 0.009202\tTotal Loss: 2.302686\n","----> Epoch 1:\n","---->[0/60000 (0%)]\tPrecision: 0.046875\tLoss: 2.301698\tSurrogate Loss: 0.008246\tTotal Loss: 2.302523\n","---->[12800/60000 (21%)]\tPrecision: 0.062500\tLoss: 2.301647\tSurrogate Loss: 10.078426\tTotal Loss: 3.309490\n","---->[25600/60000 (43%)]\tPrecision: 0.031250\tLoss: 2.309104\tSurrogate Loss: 0.839417\tTotal Loss: 2.393045\n","---->[38400/60000 (64%)]\tPrecision: 0.078125\tLoss: 2.303313\tSurrogate Loss: 0.429663\tTotal Loss: 2.346279\n","---->[51200/60000 (85%)]\tPrecision: 0.187500\tLoss: 2.302685\tSurrogate Loss: 0.318828\tTotal Loss: 2.334568\n","----> Epoch 2:\n","---->[0/60000 (0%)]\tPrecision: 0.046875\tLoss: 2.306598\tSurrogate Loss: 0.209125\tTotal Loss: 2.327511\n","---->[12800/60000 (21%)]\tPrecision: 0.171875\tLoss: 2.302423\tSurrogate Loss: 0.151487\tTotal Loss: 2.317572\n","---->[25600/60000 (43%)]\tPrecision: 0.078125\tLoss: 2.301712\tSurrogate Loss: 0.126084\tTotal Loss: 2.314321\n","---->[38400/60000 (64%)]\tPrecision: 0.140625\tLoss: 2.299721\tSurrogate Loss: 0.128127\tTotal Loss: 2.312533\n","---->[51200/60000 (85%)]\tPrecision: 0.078125\tLoss: 2.302446\tSurrogate Loss: 0.979160\tTotal Loss: 2.400362\n","----> Epoch 3:\n","---->[0/60000 (0%)]\tPrecision: 0.078125\tLoss: 2.301099\tSurrogate Loss: 0.471845\tTotal Loss: 2.348284\n","---->[12800/60000 (21%)]\tPrecision: 0.046875\tLoss: 2.305548\tSurrogate Loss: 3.779776\tTotal Loss: 2.683526\n","---->[25600/60000 (43%)]\tPrecision: 0.062500\tLoss: 2.303799\tSurrogate Loss: 1.063003\tTotal Loss: 2.410100\n","---->[38400/60000 (64%)]\tPrecision: 0.093750\tLoss: 2.301945\tSurrogate Loss: 0.657808\tTotal Loss: 2.367726\n","---->[51200/60000 (85%)]\tPrecision: 0.093750\tLoss: 2.301787\tSurrogate Loss: 1.110158\tTotal Loss: 2.412803\n","[2.304511070251465, 2.3039512634277344, 2.302562952041626, 2.3040125370025635, 2.3017656803131104, 2.3016979694366455, 2.301647424697876, 2.3091037273406982, 2.3033130168914795, 2.302684783935547, 2.306597948074341, 2.3024232387542725, 2.3017122745513916, 2.2997207641601562, 2.302445888519287, 2.3010993003845215, 2.3055479526519775, 2.3037993907928467, 2.3019449710845947, 2.3017868995666504]\n","[2.304511070251465, 2.3428964614868164, 2.303520441055298, 2.3056840896606445, 2.3026859760284424, 2.302522659301758, 3.309490203857422, 2.393045425415039, 2.3462793827056885, 2.3345675468444824, 2.3275105953216553, 2.3175718784332275, 2.3143205642700195, 2.312533378601074, 2.400362014770508, 2.3482837677001953, 2.683525562286377, 2.410099744796753, 2.3677258491516113, 2.4128026962280273]\n","--> Finished Training Task 5. Starting Test phase:\n","---->Test set 0: Average loss: 0.0002, Accuracy: 9433/10000 (94%)\n","---->Test set 1: Average loss: 0.0023, Accuracy: 1135/10000 (11%)\n","---->Test set 2: Average loss: 0.0023, Accuracy: 1135/10000 (11%)\n","---->Test set 3: Average loss: 0.0023, Accuracy: 1135/10000 (11%)\n","---->Test set 4: Average loss: 0.0023, Accuracy: 1135/10000 (11%)\n","---->Test set 5: Average loss: 0.0023, Accuracy: 1149/10000 (11%)\n","---->Test set 6: Average loss: 0.0023, Accuracy: 1089/10000 (11%)\n","---->Test set 7: Average loss: 0.0023, Accuracy: 1122/10000 (11%)\n","---->Test set 8: Average loss: 0.0023, Accuracy: 1135/10000 (11%)\n","---->Test set 9: Average loss: 0.0023, Accuracy: 1135/10000 (11%)\n","--> Training Task 6:\n","----> Epoch 0:\n","---->[0/60000 (0%)]\tPrecision: 0.093750\tLoss: 2.302234\tSurrogate Loss: 0.000000\tTotal Loss: 2.302234\n","---->[12800/60000 (21%)]\tPrecision: 0.125000\tLoss: 2.301874\tSurrogate Loss: 0.003245\tTotal Loss: 2.302198\n","---->[25600/60000 (43%)]\tPrecision: 0.093750\tLoss: 2.303612\tSurrogate Loss: 0.003331\tTotal Loss: 2.303945\n","---->[38400/60000 (64%)]\tPrecision: 0.140625\tLoss: 2.299445\tSurrogate Loss: 0.003171\tTotal Loss: 2.299762\n","---->[51200/60000 (85%)]\tPrecision: 0.125000\tLoss: 2.301587\tSurrogate Loss: 0.004805\tTotal Loss: 2.302068\n","----> Epoch 1:\n","---->[0/60000 (0%)]\tPrecision: 0.031250\tLoss: 2.301641\tSurrogate Loss: 0.004585\tTotal Loss: 2.302100\n","---->[12800/60000 (21%)]\tPrecision: 0.140625\tLoss: 2.299937\tSurrogate Loss: 0.001643\tTotal Loss: 2.300101\n","---->[25600/60000 (43%)]\tPrecision: 0.078125\tLoss: 2.303683\tSurrogate Loss: 0.003864\tTotal Loss: 2.304069\n","---->[38400/60000 (64%)]\tPrecision: 0.078125\tLoss: 2.301267\tSurrogate Loss: 0.003239\tTotal Loss: 2.301591\n","---->[51200/60000 (85%)]\tPrecision: 0.062500\tLoss: 2.302293\tSurrogate Loss: 0.002410\tTotal Loss: 2.302534\n","----> Epoch 2:\n","---->[0/60000 (0%)]\tPrecision: 0.109375\tLoss: 2.300588\tSurrogate Loss: 0.002063\tTotal Loss: 2.300794\n","---->[12800/60000 (21%)]\tPrecision: 0.078125\tLoss: 2.305533\tSurrogate Loss: 0.003224\tTotal Loss: 2.305856\n","---->[25600/60000 (43%)]\tPrecision: 0.125000\tLoss: 2.303442\tSurrogate Loss: 0.002439\tTotal Loss: 2.303686\n","---->[38400/60000 (64%)]\tPrecision: 0.031250\tLoss: 2.306175\tSurrogate Loss: 0.004921\tTotal Loss: 2.306667\n","---->[51200/60000 (85%)]\tPrecision: 0.062500\tLoss: 2.304620\tSurrogate Loss: 0.004826\tTotal Loss: 2.305102\n","----> Epoch 3:\n","---->[0/60000 (0%)]\tPrecision: 0.125000\tLoss: 2.299845\tSurrogate Loss: 0.004341\tTotal Loss: 2.300279\n","---->[12800/60000 (21%)]\tPrecision: 0.140625\tLoss: 2.301148\tSurrogate Loss: 0.003180\tTotal Loss: 2.301466\n","---->[25600/60000 (43%)]\tPrecision: 0.125000\tLoss: 2.302534\tSurrogate Loss: 0.004831\tTotal Loss: 2.303017\n","---->[38400/60000 (64%)]\tPrecision: 0.109375\tLoss: 2.299604\tSurrogate Loss: 0.002155\tTotal Loss: 2.299819\n","---->[51200/60000 (85%)]\tPrecision: 0.125000\tLoss: 2.299698\tSurrogate Loss: 0.023056\tTotal Loss: 2.302004\n","[2.302234172821045, 2.3018736839294434, 2.3036117553710938, 2.299445152282715, 2.3015871047973633, 2.3016414642333984, 2.2999367713928223, 2.303683042526245, 2.301266670227051, 2.302292823791504, 2.3005876541137695, 2.3055334091186523, 2.3034424781799316, 2.3061752319335938, 2.304619789123535, 2.299844980239868, 2.301147937774658, 2.3025341033935547, 2.2996037006378174, 2.2996983528137207]\n","[2.302234172821045, 2.3021981716156006, 2.3039448261260986, 2.29976224899292, 2.302067518234253, 2.3020999431610107, 2.3001010417938232, 2.3040692806243896, 2.30159068107605, 2.3025338649749756, 2.3007938861846924, 2.3058557510375977, 2.3036863803863525, 2.3066673278808594, 2.3051023483276367, 2.300279140472412, 2.3014659881591797, 2.3030171394348145, 2.299819231033325, 2.302004098892212]\n","--> Finished Training Task 6. Starting Test phase:\n","---->Test set 0: Average loss: 0.0002, Accuracy: 9435/10000 (94%)\n","---->Test set 1: Average loss: 0.0023, Accuracy: 1135/10000 (11%)\n","---->Test set 2: Average loss: 0.0023, Accuracy: 1135/10000 (11%)\n","---->Test set 3: Average loss: 0.0023, Accuracy: 1135/10000 (11%)\n","---->Test set 4: Average loss: 0.0023, Accuracy: 1135/10000 (11%)\n","---->Test set 5: Average loss: 0.0023, Accuracy: 1149/10000 (11%)\n","---->Test set 6: Average loss: 0.0023, Accuracy: 1134/10000 (11%)\n","---->Test set 7: Average loss: 0.0023, Accuracy: 1124/10000 (11%)\n","---->Test set 8: Average loss: 0.0023, Accuracy: 1135/10000 (11%)\n","---->Test set 9: Average loss: 0.0023, Accuracy: 1135/10000 (11%)\n","--> Training Task 7:\n","----> Epoch 0:\n","---->[0/60000 (0%)]\tPrecision: 0.171875\tLoss: 2.298439\tSurrogate Loss: 0.000000\tTotal Loss: 2.298439\n","---->[12800/60000 (21%)]\tPrecision: 0.140625\tLoss: 2.299008\tSurrogate Loss: 0.035523\tTotal Loss: 2.302561\n","---->[25600/60000 (43%)]\tPrecision: 0.062500\tLoss: 2.303133\tSurrogate Loss: 0.004360\tTotal Loss: 2.303569\n","---->[38400/60000 (64%)]\tPrecision: 0.125000\tLoss: 2.302020\tSurrogate Loss: 0.002887\tTotal Loss: 2.302308\n","---->[51200/60000 (85%)]\tPrecision: 0.062500\tLoss: 2.306028\tSurrogate Loss: 0.001761\tTotal Loss: 2.306204\n","----> Epoch 1:\n","---->[0/60000 (0%)]\tPrecision: 0.093750\tLoss: 2.304516\tSurrogate Loss: 0.003883\tTotal Loss: 2.304904\n","---->[12800/60000 (21%)]\tPrecision: 0.109375\tLoss: 2.299622\tSurrogate Loss: 0.005932\tTotal Loss: 2.300215\n","---->[25600/60000 (43%)]\tPrecision: 0.218750\tLoss: 2.297112\tSurrogate Loss: 0.002708\tTotal Loss: 2.297383\n","---->[38400/60000 (64%)]\tPrecision: 0.093750\tLoss: 2.302547\tSurrogate Loss: 0.002936\tTotal Loss: 2.302841\n","---->[51200/60000 (85%)]\tPrecision: 0.187500\tLoss: 2.298975\tSurrogate Loss: 0.002044\tTotal Loss: 2.299180\n","----> Epoch 2:\n","---->[0/60000 (0%)]\tPrecision: 0.109375\tLoss: 2.301189\tSurrogate Loss: 0.003105\tTotal Loss: 2.301499\n","---->[12800/60000 (21%)]\tPrecision: 0.093750\tLoss: 2.301816\tSurrogate Loss: 0.002992\tTotal Loss: 2.302115\n","---->[25600/60000 (43%)]\tPrecision: 0.109375\tLoss: 2.300718\tSurrogate Loss: 0.004616\tTotal Loss: 2.301179\n","---->[38400/60000 (64%)]\tPrecision: 0.156250\tLoss: 2.300122\tSurrogate Loss: 0.002143\tTotal Loss: 2.300337\n","---->[51200/60000 (85%)]\tPrecision: 0.156250\tLoss: 2.300234\tSurrogate Loss: 0.004096\tTotal Loss: 2.300644\n","----> Epoch 3:\n","---->[0/60000 (0%)]\tPrecision: 0.078125\tLoss: 2.304275\tSurrogate Loss: 0.002025\tTotal Loss: 2.304477\n","---->[12800/60000 (21%)]\tPrecision: 0.203125\tLoss: 2.297056\tSurrogate Loss: 0.001136\tTotal Loss: 2.297170\n","---->[25600/60000 (43%)]\tPrecision: 0.109375\tLoss: 2.301764\tSurrogate Loss: 0.001730\tTotal Loss: 2.301937\n","---->[38400/60000 (64%)]\tPrecision: 0.093750\tLoss: 2.305071\tSurrogate Loss: 0.001760\tTotal Loss: 2.305247\n","---->[51200/60000 (85%)]\tPrecision: 0.140625\tLoss: 2.299877\tSurrogate Loss: 0.005934\tTotal Loss: 2.300470\n","[2.298438787460327, 2.299008369445801, 2.3031325340270996, 2.3020195960998535, 2.306027889251709, 2.304515838623047, 2.299622058868408, 2.297112226486206, 2.3025472164154053, 2.2989752292633057, 2.3011887073516846, 2.3018157482147217, 2.30071759223938, 2.3001222610473633, 2.300234317779541, 2.304274559020996, 2.297056198120117, 2.301764488220215, 2.3050713539123535, 2.2998769283294678]\n","[2.298438787460327, 2.302560806274414, 2.3035686016082764, 2.3023083209991455, 2.306204080581665, 2.3049042224884033, 2.300215244293213, 2.2973830699920654, 2.3028409481048584, 2.2991795539855957, 2.301499128341675, 2.302114963531494, 2.3011791706085205, 2.3003365993499756, 2.3006439208984375, 2.3044772148132324, 2.2971699237823486, 2.3019373416900635, 2.3052473068237305, 2.3004703521728516]\n","--> Finished Training Task 7. Starting Test phase:\n","---->Test set 0: Average loss: 0.0002, Accuracy: 9433/10000 (94%)\n","---->Test set 1: Average loss: 0.0023, Accuracy: 1135/10000 (11%)\n","---->Test set 2: Average loss: 0.0023, Accuracy: 1135/10000 (11%)\n","---->Test set 3: Average loss: 0.0023, Accuracy: 1135/10000 (11%)\n","---->Test set 4: Average loss: 0.0023, Accuracy: 1135/10000 (11%)\n","---->Test set 5: Average loss: 0.0023, Accuracy: 1149/10000 (11%)\n","---->Test set 6: Average loss: 0.0023, Accuracy: 1134/10000 (11%)\n","---->Test set 7: Average loss: 0.0023, Accuracy: 1135/10000 (11%)\n","---->Test set 8: Average loss: 0.0023, Accuracy: 1135/10000 (11%)\n","---->Test set 9: Average loss: 0.0023, Accuracy: 1135/10000 (11%)\n","--> Training Task 8:\n","----> Epoch 0:\n","---->[0/60000 (0%)]\tPrecision: 0.078125\tLoss: 2.304705\tSurrogate Loss: 0.000000\tTotal Loss: 2.304705\n","---->[12800/60000 (21%)]\tPrecision: 0.062500\tLoss: 2.307127\tSurrogate Loss: 0.001640\tTotal Loss: 2.307291\n","---->[25600/60000 (43%)]\tPrecision: 0.156250\tLoss: 2.302557\tSurrogate Loss: 0.003208\tTotal Loss: 2.302878\n","---->[38400/60000 (64%)]\tPrecision: 0.031250\tLoss: 2.306485\tSurrogate Loss: 0.002436\tTotal Loss: 2.306729\n","---->[51200/60000 (85%)]\tPrecision: 0.078125\tLoss: 2.306453\tSurrogate Loss: 0.002542\tTotal Loss: 2.306707\n","----> Epoch 1:\n","---->[0/60000 (0%)]\tPrecision: 0.156250\tLoss: 2.299968\tSurrogate Loss: 0.002095\tTotal Loss: 2.300178\n","---->[12800/60000 (21%)]\tPrecision: 0.125000\tLoss: 2.301800\tSurrogate Loss: 0.003320\tTotal Loss: 2.302132\n","---->[25600/60000 (43%)]\tPrecision: 0.109375\tLoss: 2.301769\tSurrogate Loss: 0.003744\tTotal Loss: 2.302144\n","---->[38400/60000 (64%)]\tPrecision: 0.109375\tLoss: 2.300091\tSurrogate Loss: 0.002299\tTotal Loss: 2.300321\n","---->[51200/60000 (85%)]\tPrecision: 0.093750\tLoss: 2.302482\tSurrogate Loss: 0.002510\tTotal Loss: 2.302733\n","----> Epoch 2:\n","---->[0/60000 (0%)]\tPrecision: 0.187500\tLoss: 2.297786\tSurrogate Loss: 0.004014\tTotal Loss: 2.298187\n","---->[12800/60000 (21%)]\tPrecision: 0.171875\tLoss: 2.297927\tSurrogate Loss: 0.002358\tTotal Loss: 2.298163\n","---->[25600/60000 (43%)]\tPrecision: 0.140625\tLoss: 2.301201\tSurrogate Loss: 0.002336\tTotal Loss: 2.301435\n","---->[38400/60000 (64%)]\tPrecision: 0.062500\tLoss: 2.307002\tSurrogate Loss: 0.001843\tTotal Loss: 2.307186\n","---->[51200/60000 (85%)]\tPrecision: 0.062500\tLoss: 2.301579\tSurrogate Loss: 0.004750\tTotal Loss: 2.302054\n","----> Epoch 3:\n","---->[0/60000 (0%)]\tPrecision: 0.171875\tLoss: 2.298324\tSurrogate Loss: 0.004152\tTotal Loss: 2.298739\n","---->[12800/60000 (21%)]\tPrecision: 0.140625\tLoss: 2.298973\tSurrogate Loss: 0.003396\tTotal Loss: 2.299312\n","---->[25600/60000 (43%)]\tPrecision: 0.109375\tLoss: 2.303859\tSurrogate Loss: 0.002259\tTotal Loss: 2.304085\n","---->[38400/60000 (64%)]\tPrecision: 0.140625\tLoss: 2.298133\tSurrogate Loss: 0.001318\tTotal Loss: 2.298265\n","---->[51200/60000 (85%)]\tPrecision: 0.093750\tLoss: 2.304973\tSurrogate Loss: 0.003451\tTotal Loss: 2.305318\n","[2.3047049045562744, 2.307126760482788, 2.3025574684143066, 2.306485414505005, 2.306453227996826, 2.2999682426452637, 2.301799774169922, 2.301769256591797, 2.30009126663208, 2.3024818897247314, 2.297785758972168, 2.297926902770996, 2.301201343536377, 2.3070015907287598, 2.3015785217285156, 2.2983238697052, 2.2989726066589355, 2.303858757019043, 2.2981326580047607, 2.3049733638763428]\n","[2.3047049045562744, 2.30729079246521, 2.3028783798217773, 2.3067290782928467, 2.3067073822021484, 2.300177812576294, 2.3021316528320312, 2.3021435737609863, 2.300321102142334, 2.3027329444885254, 2.298187017440796, 2.2981626987457275, 2.3014349937438965, 2.3071858882904053, 2.302053689956665, 2.298738956451416, 2.2993123531341553, 2.304084539413452, 2.298264503479004, 2.3053183555603027]\n","--> Finished Training Task 8. Starting Test phase:\n","---->Test set 0: Average loss: 0.0002, Accuracy: 9429/10000 (94%)\n","---->Test set 1: Average loss: 0.0023, Accuracy: 1135/10000 (11%)\n","---->Test set 2: Average loss: 0.0023, Accuracy: 1135/10000 (11%)\n","---->Test set 3: Average loss: 0.0023, Accuracy: 1135/10000 (11%)\n","---->Test set 4: Average loss: 0.0023, Accuracy: 1135/10000 (11%)\n","---->Test set 5: Average loss: 0.0023, Accuracy: 1149/10000 (11%)\n","---->Test set 6: Average loss: 0.0023, Accuracy: 1134/10000 (11%)\n","---->Test set 7: Average loss: 0.0023, Accuracy: 1135/10000 (11%)\n","---->Test set 8: Average loss: 0.0023, Accuracy: 1135/10000 (11%)\n","---->Test set 9: Average loss: 0.0023, Accuracy: 1135/10000 (11%)\n","--> Training Task 9:\n","----> Epoch 0:\n","---->[0/60000 (0%)]\tPrecision: 0.156250\tLoss: 2.298820\tSurrogate Loss: 0.000000\tTotal Loss: 2.298820\n","---->[12800/60000 (21%)]\tPrecision: 0.140625\tLoss: 2.300149\tSurrogate Loss: 0.003357\tTotal Loss: 2.300485\n","---->[25600/60000 (43%)]\tPrecision: 0.140625\tLoss: 2.300118\tSurrogate Loss: 0.452455\tTotal Loss: 2.345364\n","---->[38400/60000 (64%)]\tPrecision: 0.171875\tLoss: 2.299076\tSurrogate Loss: 0.005602\tTotal Loss: 2.299636\n","---->[51200/60000 (85%)]\tPrecision: 0.078125\tLoss: 2.305503\tSurrogate Loss: 0.003214\tTotal Loss: 2.305825\n","----> Epoch 1:\n","---->[0/60000 (0%)]\tPrecision: 0.062500\tLoss: 2.306409\tSurrogate Loss: 0.003958\tTotal Loss: 2.306805\n","---->[12800/60000 (21%)]\tPrecision: 0.046875\tLoss: 2.307842\tSurrogate Loss: 0.003714\tTotal Loss: 2.308213\n","---->[25600/60000 (43%)]\tPrecision: 0.109375\tLoss: 2.301586\tSurrogate Loss: 0.001999\tTotal Loss: 2.301786\n","---->[38400/60000 (64%)]\tPrecision: 0.156250\tLoss: 2.303491\tSurrogate Loss: 0.001643\tTotal Loss: 2.303656\n","---->[51200/60000 (85%)]\tPrecision: 0.031250\tLoss: 2.305950\tSurrogate Loss: 0.002013\tTotal Loss: 2.306151\n","----> Epoch 2:\n","---->[0/60000 (0%)]\tPrecision: 0.031250\tLoss: 2.306766\tSurrogate Loss: 0.004120\tTotal Loss: 2.307178\n","---->[12800/60000 (21%)]\tPrecision: 0.046875\tLoss: 2.303699\tSurrogate Loss: 0.001755\tTotal Loss: 2.303875\n","---->[25600/60000 (43%)]\tPrecision: 0.093750\tLoss: 2.302853\tSurrogate Loss: 0.002743\tTotal Loss: 2.303127\n","---->[38400/60000 (64%)]\tPrecision: 0.093750\tLoss: 2.304551\tSurrogate Loss: 0.006799\tTotal Loss: 2.305231\n","---->[51200/60000 (85%)]\tPrecision: 0.171875\tLoss: 2.297730\tSurrogate Loss: 0.002043\tTotal Loss: 2.297935\n","----> Epoch 3:\n","---->[0/60000 (0%)]\tPrecision: 0.109375\tLoss: 2.305049\tSurrogate Loss: 0.002593\tTotal Loss: 2.305308\n","---->[12800/60000 (21%)]\tPrecision: 0.093750\tLoss: 2.302849\tSurrogate Loss: 0.002224\tTotal Loss: 2.303072\n","---->[25600/60000 (43%)]\tPrecision: 0.046875\tLoss: 2.306850\tSurrogate Loss: 0.003546\tTotal Loss: 2.307205\n","---->[38400/60000 (64%)]\tPrecision: 0.046875\tLoss: 2.305717\tSurrogate Loss: 0.002551\tTotal Loss: 2.305972\n","---->[51200/60000 (85%)]\tPrecision: 0.125000\tLoss: 2.301723\tSurrogate Loss: 0.006511\tTotal Loss: 2.302374\n","[2.2988204956054688, 2.3001492023468018, 2.3001184463500977, 2.2990760803222656, 2.3055033683776855, 2.3064093589782715, 2.3078420162200928, 2.3015859127044678, 2.3034913539886475, 2.305950164794922, 2.306765556335449, 2.303699254989624, 2.3028526306152344, 2.3045506477355957, 2.2977304458618164, 2.305048942565918, 2.302849292755127, 2.3068504333496094, 2.3057169914245605, 2.301723003387451]\n","[2.2988204956054688, 2.3004848957061768, 2.3453640937805176, 2.2996363639831543, 2.3058247566223145, 2.30680513381958, 2.308213472366333, 2.301785707473755, 2.3036556243896484, 2.3061513900756836, 2.3071775436401367, 2.3038747310638428, 2.303126811981201, 2.3052306175231934, 2.2979347705841064, 2.3053081035614014, 2.3030717372894287, 2.3072049617767334, 2.305972099304199, 2.3023741245269775]\n","--> Finished Training Task 9. Starting Test phase:\n","---->Test set 0: Average loss: 0.0002, Accuracy: 9431/10000 (94%)\n","---->Test set 1: Average loss: 0.0023, Accuracy: 1135/10000 (11%)\n","---->Test set 2: Average loss: 0.0023, Accuracy: 1135/10000 (11%)\n","---->Test set 3: Average loss: 0.0023, Accuracy: 1135/10000 (11%)\n","---->Test set 4: Average loss: 0.0023, Accuracy: 1135/10000 (11%)\n","---->Test set 5: Average loss: 0.0023, Accuracy: 1149/10000 (11%)\n","---->Test set 6: Average loss: 0.0023, Accuracy: 1134/10000 (11%)\n","---->Test set 7: Average loss: 0.0023, Accuracy: 1135/10000 (11%)\n","---->Test set 8: Average loss: 0.0023, Accuracy: 1135/10000 (11%)\n","---->Test set 9: Average loss: 0.0023, Accuracy: 1135/10000 (11%)\n","Accuracy: 0.19659\n","Confusion matrix:\n","0.1478,0.0742,0.0618,0.0965,0.0994,0.0497,0.1025,0.1166,0.1126,0.1161\n","0.9553,0.0882,0.0963,0.081,0.0677,0.1518,0.1037,0.0898,0.0942,0.0895\n","0.945,0.1009,0.1007,0.101,0.1009,0.1009,0.1009,0.1008,0.1009,0.1009\n","0.944,0.1009,0.1009,0.1009,0.1009,0.1009,0.1009,0.1008,0.1009,0.1009\n","0.9435,0.1009,0.1009,0.1009,0.1009,0.1009,0.1009,0.1008,0.1009,0.1009\n","0.9436,0.1009,0.1009,0.1009,0.1009,0.1009,0.1009,0.1009,0.1009,0.1009\n","0.9433,0.1135,0.1135,0.1135,0.1135,0.1149,0.1089,0.1122,0.1135,0.1135\n","0.9435,0.1135,0.1135,0.1135,0.1135,0.1149,0.1134,0.1124,0.1135,0.1135\n","0.9433,0.1135,0.1135,0.1135,0.1135,0.1149,0.1134,0.1135,0.1135,0.1135\n","0.9429,0.1135,0.1135,0.1135,0.1135,0.1149,0.1134,0.1135,0.1135,0.1135\n","0.9431,0.1135,0.1135,0.1135,0.1135,0.1149,0.1134,0.1135,0.1135,0.1135\n","--> Training:\n","---->Test set 0: Average loss: 0.1569, Accuracy: 1443/10000 (14%)\n","---->Test set 1: Average loss: 0.1613, Accuracy: 1361/10000 (14%)\n","---->Test set 2: Average loss: 0.1583, Accuracy: 818/10000 (8%)\n","---->Test set 3: Average loss: 0.1990, Accuracy: 870/10000 (9%)\n","---->Test set 4: Average loss: 0.1733, Accuracy: 1144/10000 (11%)\n","---->Test set 5: Average loss: 0.1616, Accuracy: 1113/10000 (11%)\n","---->Test set 6: Average loss: 0.1923, Accuracy: 1121/10000 (11%)\n","---->Test set 7: Average loss: 0.1659, Accuracy: 1069/10000 (11%)\n","---->Test set 8: Average loss: 0.1578, Accuracy: 582/10000 (6%)\n","---->Test set 9: Average loss: 0.1733, Accuracy: 1120/10000 (11%)\n","--> Training Task 0:\n","----> Epoch 0:\n","---->[0/60000 (0%)]\tPrecision: 0.234375\tLoss: 132.111603\tSurrogate Loss: 0.000000\tTotal Loss: 132.111603\n","---->[12800/60000 (21%)]\tPrecision: 0.890625\tLoss: 0.562879\tSurrogate Loss: 0.000000\tTotal Loss: 0.562879\n","---->[25600/60000 (43%)]\tPrecision: 0.828125\tLoss: 0.654198\tSurrogate Loss: 0.000000\tTotal Loss: 0.654198\n","---->[38400/60000 (64%)]\tPrecision: 0.937500\tLoss: 0.217006\tSurrogate Loss: 0.000000\tTotal Loss: 0.217006\n","---->[51200/60000 (85%)]\tPrecision: 0.968750\tLoss: 0.111688\tSurrogate Loss: 0.000000\tTotal Loss: 0.111688\n","----> Epoch 1:\n","---->[0/60000 (0%)]\tPrecision: 0.921875\tLoss: 0.241800\tSurrogate Loss: 0.000000\tTotal Loss: 0.241800\n","---->[12800/60000 (21%)]\tPrecision: 0.984375\tLoss: 0.047919\tSurrogate Loss: 0.000000\tTotal Loss: 0.047919\n","---->[25600/60000 (43%)]\tPrecision: 0.921875\tLoss: 0.175120\tSurrogate Loss: 0.000000\tTotal Loss: 0.175120\n","---->[38400/60000 (64%)]\tPrecision: 0.953125\tLoss: 0.152956\tSurrogate Loss: 0.000000\tTotal Loss: 0.152956\n","---->[51200/60000 (85%)]\tPrecision: 0.953125\tLoss: 0.171019\tSurrogate Loss: 0.000000\tTotal Loss: 0.171019\n","----> Epoch 2:\n","---->[0/60000 (0%)]\tPrecision: 0.921875\tLoss: 0.183122\tSurrogate Loss: 0.000000\tTotal Loss: 0.183122\n","---->[12800/60000 (21%)]\tPrecision: 0.968750\tLoss: 0.058860\tSurrogate Loss: 0.000000\tTotal Loss: 0.058860\n","---->[25600/60000 (43%)]\tPrecision: 0.953125\tLoss: 0.148238\tSurrogate Loss: 0.000000\tTotal Loss: 0.148238\n","---->[38400/60000 (64%)]\tPrecision: 0.921875\tLoss: 0.402374\tSurrogate Loss: 0.000000\tTotal Loss: 0.402374\n","---->[51200/60000 (85%)]\tPrecision: 0.968750\tLoss: 0.077067\tSurrogate Loss: 0.000000\tTotal Loss: 0.077067\n","----> Epoch 3:\n","---->[0/60000 (0%)]\tPrecision: 0.984375\tLoss: 0.040790\tSurrogate Loss: 0.000000\tTotal Loss: 0.040790\n","---->[12800/60000 (21%)]\tPrecision: 1.000000\tLoss: 0.010602\tSurrogate Loss: 0.000000\tTotal Loss: 0.010602\n","---->[25600/60000 (43%)]\tPrecision: 0.953125\tLoss: 0.085308\tSurrogate Loss: 0.000000\tTotal Loss: 0.085308\n","---->[38400/60000 (64%)]\tPrecision: 0.953125\tLoss: 0.155319\tSurrogate Loss: 0.000000\tTotal Loss: 0.155319\n","---->[51200/60000 (85%)]\tPrecision: 0.984375\tLoss: 0.348009\tSurrogate Loss: 0.000000\tTotal Loss: 0.348009\n","----> Epoch 4:\n","---->[0/60000 (0%)]\tPrecision: 0.921875\tLoss: 0.214960\tSurrogate Loss: 0.000000\tTotal Loss: 0.214960\n","---->[12800/60000 (21%)]\tPrecision: 0.953125\tLoss: 0.108775\tSurrogate Loss: 0.000000\tTotal Loss: 0.108775\n","---->[25600/60000 (43%)]\tPrecision: 0.921875\tLoss: 0.198983\tSurrogate Loss: 0.000000\tTotal Loss: 0.198983\n","---->[38400/60000 (64%)]\tPrecision: 0.937500\tLoss: 0.189630\tSurrogate Loss: 0.000000\tTotal Loss: 0.189630\n","---->[51200/60000 (85%)]\tPrecision: 0.953125\tLoss: 0.079414\tSurrogate Loss: 0.000000\tTotal Loss: 0.079414\n","[132.11160278320312, 0.5628787279129028, 0.6541977524757385, 0.21700598299503326, 0.11168766021728516, 0.24180006980895996, 0.04791903868317604, 0.1751195341348648, 0.15295635163784027, 0.17101942002773285, 0.18312175571918488, 0.05886020138859749, 0.14823833107948303, 0.40237390995025635, 0.07706739753484726, 0.040790122002363205, 0.0106017105281353, 0.08530791848897934, 0.15531890094280243, 0.348008930683136, 0.2149600237607956, 0.10877460986375809, 0.1989830732345581, 0.18962961435317993, 0.07941435277462006]\n","[132.11160278320312, 0.5628787279129028, 0.6541977524757385, 0.21700598299503326, 0.11168766021728516, 0.24180006980895996, 0.04791903868317604, 0.1751195341348648, 0.15295635163784027, 0.17101942002773285, 0.18312175571918488, 0.05886020138859749, 0.14823833107948303, 0.40237390995025635, 0.07706739753484726, 0.040790122002363205, 0.0106017105281353, 0.08530791848897934, 0.15531890094280243, 0.348008930683136, 0.2149600237607956, 0.10877460986375809, 0.1989830732345581, 0.18962961435317993, 0.07941435277462006]\n","--> Finished Training Task 0. Starting Test phase:\n","---->Test set 0: Average loss: 0.0002, Accuracy: 9542/10000 (95%)\n","---->Test set 1: Average loss: 0.0084, Accuracy: 925/10000 (9%)\n","---->Test set 2: Average loss: 0.0063, Accuracy: 1035/10000 (10%)\n","---->Test set 3: Average loss: 0.0037, Accuracy: 1022/10000 (10%)\n","---->Test set 4: Average loss: 0.0034, Accuracy: 849/10000 (8%)\n","---->Test set 5: Average loss: 0.0038, Accuracy: 1180/10000 (12%)\n","---->Test set 6: Average loss: 0.0040, Accuracy: 1060/10000 (11%)\n","---->Test set 7: Average loss: 0.0051, Accuracy: 1140/10000 (11%)\n","---->Test set 8: Average loss: 0.0037, Accuracy: 1238/10000 (12%)\n","---->Test set 9: Average loss: 0.0049, Accuracy: 773/10000 (8%)\n","--> Training Task 1:\n","----> Epoch 0:\n","---->[0/60000 (0%)]\tPrecision: 0.156250\tLoss: 6.056439\tSurrogate Loss: 0.000000\tTotal Loss: 6.056439\n","---->[12800/60000 (21%)]\tPrecision: 0.140625\tLoss: 2.290646\tSurrogate Loss: 9.664870\tTotal Loss: 3.257133\n","---->[25600/60000 (43%)]\tPrecision: 0.093750\tLoss: 2.310923\tSurrogate Loss: 4.073316\tTotal Loss: 2.718254\n","---->[38400/60000 (64%)]\tPrecision: 0.078125\tLoss: 2.310014\tSurrogate Loss: 2.431656\tTotal Loss: 2.553180\n","---->[51200/60000 (85%)]\tPrecision: 0.046875\tLoss: 2.306828\tSurrogate Loss: 2.629100\tTotal Loss: 2.569738\n","----> Epoch 1:\n","---->[0/60000 (0%)]\tPrecision: 0.093750\tLoss: 2.311424\tSurrogate Loss: 5.092760\tTotal Loss: 2.820700\n","---->[12800/60000 (21%)]\tPrecision: 0.187500\tLoss: 2.300061\tSurrogate Loss: 4.595284\tTotal Loss: 2.759589\n","---->[25600/60000 (43%)]\tPrecision: 0.078125\tLoss: 2.307959\tSurrogate Loss: 2.622377\tTotal Loss: 2.570197\n","---->[38400/60000 (64%)]\tPrecision: 0.078125\tLoss: 2.293990\tSurrogate Loss: 1.865246\tTotal Loss: 2.480515\n","---->[51200/60000 (85%)]\tPrecision: 0.093750\tLoss: 2.297332\tSurrogate Loss: 1.687765\tTotal Loss: 2.466109\n","----> Epoch 2:\n","---->[0/60000 (0%)]\tPrecision: 0.046875\tLoss: 2.308588\tSurrogate Loss: 2.571923\tTotal Loss: 2.565780\n","---->[12800/60000 (21%)]\tPrecision: 0.093750\tLoss: 2.296164\tSurrogate Loss: 1.582453\tTotal Loss: 2.454409\n","---->[25600/60000 (43%)]\tPrecision: 0.078125\tLoss: 2.300166\tSurrogate Loss: 1.689978\tTotal Loss: 2.469163\n","---->[38400/60000 (64%)]\tPrecision: 0.156250\tLoss: 2.301272\tSurrogate Loss: 1.160135\tTotal Loss: 2.417286\n","---->[51200/60000 (85%)]\tPrecision: 0.125000\tLoss: 2.299795\tSurrogate Loss: 0.871283\tTotal Loss: 2.386923\n","----> Epoch 3:\n","---->[0/60000 (0%)]\tPrecision: 0.281250\tLoss: 1.974427\tSurrogate Loss: 1.240086\tTotal Loss: 2.098436\n","---->[12800/60000 (21%)]\tPrecision: 0.265625\tLoss: 2.018342\tSurrogate Loss: 2.880182\tTotal Loss: 2.306360\n","---->[25600/60000 (43%)]\tPrecision: 0.312500\tLoss: 1.748250\tSurrogate Loss: 2.420360\tTotal Loss: 1.990286\n","---->[38400/60000 (64%)]\tPrecision: 0.703125\tLoss: 0.718588\tSurrogate Loss: 5.537189\tTotal Loss: 1.272307\n","---->[51200/60000 (85%)]\tPrecision: 0.875000\tLoss: 0.938306\tSurrogate Loss: 6.518160\tTotal Loss: 1.590122\n","----> Epoch 4:\n","---->[0/60000 (0%)]\tPrecision: 0.937500\tLoss: 0.149998\tSurrogate Loss: 3.964702\tTotal Loss: 0.546468\n","---->[12800/60000 (21%)]\tPrecision: 0.875000\tLoss: 0.327633\tSurrogate Loss: 3.622327\tTotal Loss: 0.689866\n","---->[25600/60000 (43%)]\tPrecision: 0.859375\tLoss: 0.574002\tSurrogate Loss: 7.843209\tTotal Loss: 1.358323\n","---->[38400/60000 (64%)]\tPrecision: 0.968750\tLoss: 0.127155\tSurrogate Loss: 2.987063\tTotal Loss: 0.425861\n","---->[51200/60000 (85%)]\tPrecision: 0.906250\tLoss: 0.285213\tSurrogate Loss: 3.279548\tTotal Loss: 0.613168\n","[6.05643892288208, 2.2906455993652344, 2.310922622680664, 2.310014009475708, 2.306828498840332, 2.3114240169525146, 2.300060510635376, 2.3079590797424316, 2.29399037361145, 2.2973320484161377, 2.3085875511169434, 2.296164035797119, 2.3001656532287598, 2.30127215385437, 2.299794912338257, 1.9744272232055664, 2.018341541290283, 1.7482504844665527, 0.7185878753662109, 0.9383060932159424, 0.149998277425766, 0.3276333510875702, 0.5740016102790833, 0.12715478241443634, 0.28521302342414856]\n","[6.05643892288208, 3.2571325302124023, 2.718254327774048, 2.5531795024871826, 2.5697383880615234, 2.820700168609619, 2.7595889568328857, 2.5701966285705566, 2.4805150032043457, 2.466108560562134, 2.5657799243927, 2.45440936088562, 2.469163417816162, 2.417285680770874, 2.38692307472229, 2.098435878753662, 2.3063597679138184, 1.9902864694595337, 1.2723069190979004, 1.590122103691101, 0.5464684963226318, 0.6898660063743591, 1.3583226203918457, 0.425861120223999, 0.6131678819656372]\n","--> Finished Training Task 1. Starting Test phase:\n","---->Test set 0: Average loss: 0.0002, Accuracy: 9402/10000 (94%)\n","---->Test set 1: Average loss: 0.0002, Accuracy: 9445/10000 (94%)\n","---->Test set 2: Average loss: 0.0042, Accuracy: 871/10000 (9%)\n","---->Test set 3: Average loss: 0.0033, Accuracy: 1105/10000 (11%)\n","---->Test set 4: Average loss: 0.0046, Accuracy: 657/10000 (7%)\n","---->Test set 5: Average loss: 0.0036, Accuracy: 808/10000 (8%)\n","---->Test set 6: Average loss: 0.0047, Accuracy: 828/10000 (8%)\n","---->Test set 7: Average loss: 0.0033, Accuracy: 818/10000 (8%)\n","---->Test set 8: Average loss: 0.0049, Accuracy: 1271/10000 (13%)\n","---->Test set 9: Average loss: 0.0042, Accuracy: 905/10000 (9%)\n","--> Training Task 2:\n","----> Epoch 0:\n","---->[0/60000 (0%)]\tPrecision: 0.093750\tLoss: 3.913081\tSurrogate Loss: 0.000000\tTotal Loss: 3.913081\n","---->[12800/60000 (21%)]\tPrecision: 0.515625\tLoss: 1.493878\tSurrogate Loss: 6.183213\tTotal Loss: 2.112199\n","---->[25600/60000 (43%)]\tPrecision: 0.828125\tLoss: 0.582935\tSurrogate Loss: 7.469920\tTotal Loss: 1.329927\n","---->[38400/60000 (64%)]\tPrecision: 0.843750\tLoss: 0.479766\tSurrogate Loss: 5.471209\tTotal Loss: 1.026886\n","---->[51200/60000 (85%)]\tPrecision: 0.890625\tLoss: 0.356194\tSurrogate Loss: 4.259540\tTotal Loss: 0.782148\n","----> Epoch 1:\n","---->[0/60000 (0%)]\tPrecision: 0.968750\tLoss: 0.105533\tSurrogate Loss: 4.047904\tTotal Loss: 0.510323\n","---->[12800/60000 (21%)]\tPrecision: 0.937500\tLoss: 0.158440\tSurrogate Loss: 3.564043\tTotal Loss: 0.514845\n","---->[25600/60000 (43%)]\tPrecision: 0.906250\tLoss: 0.338439\tSurrogate Loss: 4.392994\tTotal Loss: 0.777738\n","---->[38400/60000 (64%)]\tPrecision: 0.906250\tLoss: 0.642187\tSurrogate Loss: 3.583144\tTotal Loss: 1.000502\n","---->[51200/60000 (85%)]\tPrecision: 0.937500\tLoss: 0.305975\tSurrogate Loss: 5.418427\tTotal Loss: 0.847818\n","----> Epoch 2:\n","---->[0/60000 (0%)]\tPrecision: 0.968750\tLoss: 0.096976\tSurrogate Loss: 3.024441\tTotal Loss: 0.399421\n","---->[12800/60000 (21%)]\tPrecision: 0.921875\tLoss: 0.283418\tSurrogate Loss: 3.762994\tTotal Loss: 0.659718\n","---->[25600/60000 (43%)]\tPrecision: 0.921875\tLoss: 0.207517\tSurrogate Loss: 5.166488\tTotal Loss: 0.724166\n","---->[38400/60000 (64%)]\tPrecision: 0.937500\tLoss: 0.230447\tSurrogate Loss: 3.009953\tTotal Loss: 0.531442\n","---->[51200/60000 (85%)]\tPrecision: 0.921875\tLoss: 0.388123\tSurrogate Loss: 9.061349\tTotal Loss: 1.294258\n","----> Epoch 3:\n","---->[0/60000 (0%)]\tPrecision: 0.953125\tLoss: 0.113037\tSurrogate Loss: 2.844026\tTotal Loss: 0.397439\n","---->[12800/60000 (21%)]\tPrecision: 0.906250\tLoss: 0.401741\tSurrogate Loss: 3.948392\tTotal Loss: 0.796580\n","---->[25600/60000 (43%)]\tPrecision: 0.906250\tLoss: 0.374258\tSurrogate Loss: 3.277130\tTotal Loss: 0.701971\n","---->[38400/60000 (64%)]\tPrecision: 0.937500\tLoss: 0.234006\tSurrogate Loss: 5.149490\tTotal Loss: 0.748955\n","---->[51200/60000 (85%)]\tPrecision: 0.890625\tLoss: 0.344077\tSurrogate Loss: 5.042793\tTotal Loss: 0.848357\n","----> Epoch 4:\n","---->[0/60000 (0%)]\tPrecision: 0.968750\tLoss: 0.159735\tSurrogate Loss: 3.169843\tTotal Loss: 0.476719\n","---->[12800/60000 (21%)]\tPrecision: 0.968750\tLoss: 0.098734\tSurrogate Loss: 2.507330\tTotal Loss: 0.349467\n","---->[25600/60000 (43%)]\tPrecision: 0.906250\tLoss: 0.219564\tSurrogate Loss: 4.438746\tTotal Loss: 0.663439\n","---->[38400/60000 (64%)]\tPrecision: 0.921875\tLoss: 0.159286\tSurrogate Loss: 2.577859\tTotal Loss: 0.417071\n","---->[51200/60000 (85%)]\tPrecision: 0.953125\tLoss: 0.182750\tSurrogate Loss: 2.378471\tTotal Loss: 0.420597\n","[3.9130806922912598, 1.49387788772583, 0.5829349756240845, 0.47976553440093994, 0.35619381070137024, 0.10553259402513504, 0.15844036638736725, 0.33843865990638733, 0.6421874761581421, 0.30597537755966187, 0.09697645902633667, 0.2834184169769287, 0.20751698315143585, 0.23044712841510773, 0.3881230056285858, 0.11303666234016418, 0.401741087436676, 0.3742583394050598, 0.23400573432445526, 0.34407737851142883, 0.15973463654518127, 0.09873402863740921, 0.21956415474414825, 0.1592855304479599, 0.18274962902069092]\n","[3.9130806922912598, 2.112199306488037, 1.3299269676208496, 1.0268864631652832, 0.7821478247642517, 0.5103229880332947, 0.5148447155952454, 0.777738094329834, 1.0005018711090088, 0.8478180766105652, 0.3994206190109253, 0.6597177982330322, 0.7241657972335815, 0.5314424633979797, 1.2942578792572021, 0.39743927121162415, 0.7965803146362305, 0.7019712924957275, 0.7489547729492188, 0.8483567237854004, 0.4767189025878906, 0.349467009305954, 0.6634387373924255, 0.4170714020729065, 0.420596718788147]\n","--> Finished Training Task 2. Starting Test phase:\n","---->Test set 0: Average loss: 0.0002, Accuracy: 9362/10000 (94%)\n","---->Test set 1: Average loss: 0.0003, Accuracy: 9315/10000 (93%)\n","---->Test set 2: Average loss: 0.0002, Accuracy: 9496/10000 (95%)\n","---->Test set 3: Average loss: 0.0053, Accuracy: 1003/10000 (10%)\n","---->Test set 4: Average loss: 0.0063, Accuracy: 975/10000 (10%)\n","---->Test set 5: Average loss: 0.0045, Accuracy: 952/10000 (10%)\n","---->Test set 6: Average loss: 0.0049, Accuracy: 886/10000 (9%)\n","---->Test set 7: Average loss: 0.0039, Accuracy: 889/10000 (9%)\n","---->Test set 8: Average loss: 0.0047, Accuracy: 1083/10000 (11%)\n","---->Test set 9: Average loss: 0.0060, Accuracy: 987/10000 (10%)\n","--> Training Task 3:\n","----> Epoch 0:\n","---->[0/60000 (0%)]\tPrecision: 0.093750\tLoss: 4.932781\tSurrogate Loss: 0.000000\tTotal Loss: 4.932781\n","---->[12800/60000 (21%)]\tPrecision: 0.125000\tLoss: 2.309591\tSurrogate Loss: 5.301517\tTotal Loss: 2.839743\n","---->[25600/60000 (43%)]\tPrecision: 0.109375\tLoss: 2.596067\tSurrogate Loss: 13.517636\tTotal Loss: 3.947831\n","---->[38400/60000 (64%)]\tPrecision: 0.343750\tLoss: 2.000401\tSurrogate Loss: 10.235147\tTotal Loss: 3.023916\n","---->[51200/60000 (85%)]\tPrecision: 0.859375\tLoss: 0.729915\tSurrogate Loss: 10.976157\tTotal Loss: 1.827531\n","----> Epoch 1:\n","---->[0/60000 (0%)]\tPrecision: 0.921875\tLoss: 0.338406\tSurrogate Loss: 12.062569\tTotal Loss: 1.544663\n","---->[12800/60000 (21%)]\tPrecision: 0.875000\tLoss: 0.428079\tSurrogate Loss: 7.977261\tTotal Loss: 1.225806\n","---->[25600/60000 (43%)]\tPrecision: 0.890625\tLoss: 0.601522\tSurrogate Loss: 6.414845\tTotal Loss: 1.243006\n","---->[38400/60000 (64%)]\tPrecision: 0.875000\tLoss: 0.437950\tSurrogate Loss: 6.132874\tTotal Loss: 1.051238\n","---->[51200/60000 (85%)]\tPrecision: 0.921875\tLoss: 0.186834\tSurrogate Loss: 4.806994\tTotal Loss: 0.667533\n","----> Epoch 2:\n","---->[0/60000 (0%)]\tPrecision: 0.937500\tLoss: 0.196253\tSurrogate Loss: 4.643064\tTotal Loss: 0.660559\n","---->[12800/60000 (21%)]\tPrecision: 0.890625\tLoss: 0.344414\tSurrogate Loss: 3.738996\tTotal Loss: 0.718314\n","---->[25600/60000 (43%)]\tPrecision: 0.906250\tLoss: 0.349145\tSurrogate Loss: 3.976696\tTotal Loss: 0.746815\n","---->[38400/60000 (64%)]\tPrecision: 0.906250\tLoss: 0.196458\tSurrogate Loss: 5.058008\tTotal Loss: 0.702258\n","---->[51200/60000 (85%)]\tPrecision: 0.937500\tLoss: 0.226882\tSurrogate Loss: 5.642658\tTotal Loss: 0.791148\n","----> Epoch 3:\n","---->[0/60000 (0%)]\tPrecision: 0.921875\tLoss: 0.279099\tSurrogate Loss: 5.636384\tTotal Loss: 0.842737\n","---->[12800/60000 (21%)]\tPrecision: 0.953125\tLoss: 0.226975\tSurrogate Loss: 4.659694\tTotal Loss: 0.692944\n","---->[25600/60000 (43%)]\tPrecision: 0.937500\tLoss: 0.138631\tSurrogate Loss: 4.231222\tTotal Loss: 0.561753\n","---->[38400/60000 (64%)]\tPrecision: 0.953125\tLoss: 0.174080\tSurrogate Loss: 5.061543\tTotal Loss: 0.680234\n","---->[51200/60000 (85%)]\tPrecision: 0.968750\tLoss: 0.144854\tSurrogate Loss: 3.890749\tTotal Loss: 0.533929\n","----> Epoch 4:\n","---->[0/60000 (0%)]\tPrecision: 0.968750\tLoss: 0.107514\tSurrogate Loss: 3.299887\tTotal Loss: 0.437503\n","---->[12800/60000 (21%)]\tPrecision: 0.953125\tLoss: 0.382796\tSurrogate Loss: 4.168731\tTotal Loss: 0.799669\n","---->[25600/60000 (43%)]\tPrecision: 0.921875\tLoss: 0.354678\tSurrogate Loss: 4.824208\tTotal Loss: 0.837099\n","---->[38400/60000 (64%)]\tPrecision: 0.937500\tLoss: 0.181139\tSurrogate Loss: 4.048091\tTotal Loss: 0.585948\n","---->[51200/60000 (85%)]\tPrecision: 0.968750\tLoss: 0.147550\tSurrogate Loss: 6.909549\tTotal Loss: 0.838505\n","[4.932781219482422, 2.3095908164978027, 2.596067428588867, 2.000401496887207, 0.7299153804779053, 0.33840620517730713, 0.42807936668395996, 0.6015216112136841, 0.4379504919052124, 0.18683406710624695, 0.1962529569864273, 0.3444143235683441, 0.3491452634334564, 0.19645756483078003, 0.22688241302967072, 0.2790989577770233, 0.22697459161281586, 0.1386311948299408, 0.1740795224905014, 0.14485448598861694, 0.10751446336507797, 0.38279637694358826, 0.3546781837940216, 0.18113887310028076, 0.14755001664161682]\n","[4.932781219482422, 2.839742660522461, 3.947831153869629, 3.023916244506836, 1.827531099319458, 1.5446630716323853, 1.2258055210113525, 1.2430061101913452, 1.0512378215789795, 0.6675334572792053, 0.6605592966079712, 0.7183139324188232, 0.7468148469924927, 0.7022584080696106, 0.7911482453346252, 0.8427374362945557, 0.6929439902305603, 0.5617533922195435, 0.6802338361740112, 0.5339293479919434, 0.4375031590461731, 0.7996694445610046, 0.8370989561080933, 0.5859479904174805, 0.8385049104690552]\n","--> Finished Training Task 3. Starting Test phase:\n","---->Test set 0: Average loss: 0.0002, Accuracy: 9374/10000 (94%)\n","---->Test set 1: Average loss: 0.0004, Accuracy: 8966/10000 (90%)\n","---->Test set 2: Average loss: 0.0003, Accuracy: 9165/10000 (92%)\n","---->Test set 3: Average loss: 0.0002, Accuracy: 9467/10000 (95%)\n","---->Test set 4: Average loss: 0.0057, Accuracy: 877/10000 (9%)\n","---->Test set 5: Average loss: 0.0041, Accuracy: 1002/10000 (10%)\n","---->Test set 6: Average loss: 0.0046, Accuracy: 1061/10000 (11%)\n","---->Test set 7: Average loss: 0.0047, Accuracy: 984/10000 (10%)\n","---->Test set 8: Average loss: 0.0050, Accuracy: 1061/10000 (11%)\n","---->Test set 9: Average loss: 0.0051, Accuracy: 846/10000 (8%)\n","--> Training Task 4:\n","----> Epoch 0:\n","---->[0/60000 (0%)]\tPrecision: 0.125000\tLoss: 6.269702\tSurrogate Loss: 0.000000\tTotal Loss: 6.269702\n","---->[12800/60000 (21%)]\tPrecision: 0.781250\tLoss: 1.002177\tSurrogate Loss: 11.996058\tTotal Loss: 2.201782\n","---->[25600/60000 (43%)]\tPrecision: 0.937500\tLoss: 0.269725\tSurrogate Loss: 7.526019\tTotal Loss: 1.022327\n","---->[38400/60000 (64%)]\tPrecision: 0.937500\tLoss: 0.236319\tSurrogate Loss: 7.590497\tTotal Loss: 0.995369\n","---->[51200/60000 (85%)]\tPrecision: 0.859375\tLoss: 0.589682\tSurrogate Loss: 9.751159\tTotal Loss: 1.564798\n","----> Epoch 1:\n","---->[0/60000 (0%)]\tPrecision: 0.906250\tLoss: 0.435856\tSurrogate Loss: 14.663268\tTotal Loss: 1.902182\n","---->[12800/60000 (21%)]\tPrecision: 0.890625\tLoss: 0.401158\tSurrogate Loss: 5.472624\tTotal Loss: 0.948420\n","---->[25600/60000 (43%)]\tPrecision: 0.906250\tLoss: 0.235568\tSurrogate Loss: 6.064842\tTotal Loss: 0.842052\n","---->[38400/60000 (64%)]\tPrecision: 0.937500\tLoss: 0.160885\tSurrogate Loss: 5.732439\tTotal Loss: 0.734129\n","---->[51200/60000 (85%)]\tPrecision: 0.953125\tLoss: 0.145219\tSurrogate Loss: 5.075578\tTotal Loss: 0.652776\n","----> Epoch 2:\n","---->[0/60000 (0%)]\tPrecision: 0.890625\tLoss: 0.360604\tSurrogate Loss: 4.843730\tTotal Loss: 0.844977\n","---->[12800/60000 (21%)]\tPrecision: 0.906250\tLoss: 0.362032\tSurrogate Loss: 5.878536\tTotal Loss: 0.949885\n","---->[25600/60000 (43%)]\tPrecision: 0.843750\tLoss: 0.650798\tSurrogate Loss: 6.574251\tTotal Loss: 1.308223\n","---->[38400/60000 (64%)]\tPrecision: 0.953125\tLoss: 0.118630\tSurrogate Loss: 5.773262\tTotal Loss: 0.695956\n","---->[51200/60000 (85%)]\tPrecision: 0.906250\tLoss: 0.346734\tSurrogate Loss: 6.288301\tTotal Loss: 0.975564\n","----> Epoch 3:\n","---->[0/60000 (0%)]\tPrecision: 1.000000\tLoss: 0.057209\tSurrogate Loss: 7.493004\tTotal Loss: 0.806510\n","---->[12800/60000 (21%)]\tPrecision: 0.937500\tLoss: 0.263579\tSurrogate Loss: 4.284989\tTotal Loss: 0.692078\n","---->[25600/60000 (43%)]\tPrecision: 0.937500\tLoss: 0.175527\tSurrogate Loss: 5.955854\tTotal Loss: 0.771113\n","---->[38400/60000 (64%)]\tPrecision: 0.968750\tLoss: 0.107999\tSurrogate Loss: 6.526648\tTotal Loss: 0.760664\n","---->[51200/60000 (85%)]\tPrecision: 0.875000\tLoss: 0.318278\tSurrogate Loss: 6.694719\tTotal Loss: 0.987750\n","----> Epoch 4:\n","---->[0/60000 (0%)]\tPrecision: 0.984375\tLoss: 0.075186\tSurrogate Loss: 4.327026\tTotal Loss: 0.507888\n","---->[12800/60000 (21%)]\tPrecision: 0.921875\tLoss: 0.185804\tSurrogate Loss: 5.080919\tTotal Loss: 0.693896\n","---->[25600/60000 (43%)]\tPrecision: 0.968750\tLoss: 0.193111\tSurrogate Loss: 6.979942\tTotal Loss: 0.891106\n","---->[38400/60000 (64%)]\tPrecision: 0.953125\tLoss: 0.186364\tSurrogate Loss: 4.563238\tTotal Loss: 0.642688\n","---->[51200/60000 (85%)]\tPrecision: 0.937500\tLoss: 0.323024\tSurrogate Loss: 4.569743\tTotal Loss: 0.779999\n","[6.269702434539795, 1.0021766424179077, 0.26972493529319763, 0.23631912469863892, 0.5896823406219482, 0.4358556568622589, 0.401157945394516, 0.23556779325008392, 0.1608852744102478, 0.14521853625774384, 0.36060360074043274, 0.36203157901763916, 0.6507983207702637, 0.11863003671169281, 0.34673377871513367, 0.057209406048059464, 0.263579398393631, 0.175527423620224, 0.1079990491271019, 0.3182777166366577, 0.07518567889928818, 0.1858036071062088, 0.19311141967773438, 0.18636387586593628, 0.3230244517326355]\n","[6.269702434539795, 2.201782464981079, 1.0223268270492554, 0.9953688979148865, 1.5647982358932495, 1.9021824598312378, 0.9484202861785889, 0.8420519828796387, 0.7341291904449463, 0.6527763605117798, 0.8449766039848328, 0.9498851895332336, 1.3082234859466553, 0.6959562301635742, 0.9755638837814331, 0.8065098524093628, 0.6920783519744873, 0.7711127996444702, 0.7606638073921204, 0.9877496361732483, 0.5078882575035095, 0.6938955187797546, 0.8911056518554688, 0.6426876783370972, 0.779998779296875]\n","--> Finished Training Task 4. Starting Test phase:\n","---->Test set 0: Average loss: 0.0002, Accuracy: 9394/10000 (94%)\n","---->Test set 1: Average loss: 0.0003, Accuracy: 9133/10000 (91%)\n","---->Test set 2: Average loss: 0.0004, Accuracy: 9135/10000 (91%)\n","---->Test set 3: Average loss: 0.0003, Accuracy: 9231/10000 (92%)\n","---->Test set 4: Average loss: 0.0003, Accuracy: 9270/10000 (93%)\n","---->Test set 5: Average loss: 0.0037, Accuracy: 1026/10000 (10%)\n","---->Test set 6: Average loss: 0.0054, Accuracy: 985/10000 (10%)\n","---->Test set 7: Average loss: 0.0047, Accuracy: 868/10000 (9%)\n","---->Test set 8: Average loss: 0.0055, Accuracy: 1061/10000 (11%)\n","---->Test set 9: Average loss: 0.0055, Accuracy: 794/10000 (8%)\n","--> Training Task 5:\n","----> Epoch 0:\n","---->[0/60000 (0%)]\tPrecision: 0.093750\tLoss: 3.935593\tSurrogate Loss: 0.000000\tTotal Loss: 3.935593\n","---->[12800/60000 (21%)]\tPrecision: 0.781250\tLoss: 0.673867\tSurrogate Loss: 11.275668\tTotal Loss: 1.801434\n","---->[25600/60000 (43%)]\tPrecision: 0.906250\tLoss: 0.366531\tSurrogate Loss: 8.218529\tTotal Loss: 1.188384\n","---->[38400/60000 (64%)]\tPrecision: 0.890625\tLoss: 0.414026\tSurrogate Loss: 6.122410\tTotal Loss: 1.026267\n","---->[51200/60000 (85%)]\tPrecision: 0.890625\tLoss: 0.322952\tSurrogate Loss: 8.701035\tTotal Loss: 1.193055\n","----> Epoch 1:\n","---->[0/60000 (0%)]\tPrecision: 0.968750\tLoss: 0.230824\tSurrogate Loss: 8.489164\tTotal Loss: 1.079740\n","---->[12800/60000 (21%)]\tPrecision: 0.906250\tLoss: 0.238105\tSurrogate Loss: 5.323145\tTotal Loss: 0.770419\n","---->[25600/60000 (43%)]\tPrecision: 0.953125\tLoss: 0.153079\tSurrogate Loss: 4.427134\tTotal Loss: 0.595792\n","---->[38400/60000 (64%)]\tPrecision: 0.890625\tLoss: 0.363527\tSurrogate Loss: 5.007006\tTotal Loss: 0.864228\n","---->[51200/60000 (85%)]\tPrecision: 0.937500\tLoss: 0.240638\tSurrogate Loss: 5.344059\tTotal Loss: 0.775044\n","----> Epoch 2:\n","---->[0/60000 (0%)]\tPrecision: 0.921875\tLoss: 0.347931\tSurrogate Loss: 5.818944\tTotal Loss: 0.929825\n","---->[12800/60000 (21%)]\tPrecision: 0.906250\tLoss: 0.255334\tSurrogate Loss: 5.010798\tTotal Loss: 0.756414\n","---->[25600/60000 (43%)]\tPrecision: 0.937500\tLoss: 0.292920\tSurrogate Loss: 5.084118\tTotal Loss: 0.801332\n","---->[38400/60000 (64%)]\tPrecision: 0.953125\tLoss: 0.159023\tSurrogate Loss: 5.164897\tTotal Loss: 0.675513\n","---->[51200/60000 (85%)]\tPrecision: 0.937500\tLoss: 0.261713\tSurrogate Loss: 6.127543\tTotal Loss: 0.874467\n","----> Epoch 3:\n","---->[0/60000 (0%)]\tPrecision: 0.921875\tLoss: 0.364379\tSurrogate Loss: 4.755761\tTotal Loss: 0.839956\n","---->[12800/60000 (21%)]\tPrecision: 0.890625\tLoss: 0.302536\tSurrogate Loss: 7.754574\tTotal Loss: 1.077993\n","---->[25600/60000 (43%)]\tPrecision: 0.937500\tLoss: 0.192971\tSurrogate Loss: 5.474359\tTotal Loss: 0.740407\n","---->[38400/60000 (64%)]\tPrecision: 0.953125\tLoss: 0.140963\tSurrogate Loss: 4.117148\tTotal Loss: 0.552678\n","---->[51200/60000 (85%)]\tPrecision: 0.937500\tLoss: 0.227120\tSurrogate Loss: 5.504552\tTotal Loss: 0.777576\n","----> Epoch 4:\n","---->[0/60000 (0%)]\tPrecision: 0.906250\tLoss: 0.214939\tSurrogate Loss: 4.761692\tTotal Loss: 0.691108\n","---->[12800/60000 (21%)]\tPrecision: 0.937500\tLoss: 0.223229\tSurrogate Loss: 4.623640\tTotal Loss: 0.685593\n","---->[25600/60000 (43%)]\tPrecision: 0.937500\tLoss: 0.169670\tSurrogate Loss: 5.264920\tTotal Loss: 0.696162\n","---->[38400/60000 (64%)]\tPrecision: 0.937500\tLoss: 0.156034\tSurrogate Loss: 4.960781\tTotal Loss: 0.652112\n","---->[51200/60000 (85%)]\tPrecision: 0.875000\tLoss: 0.320846\tSurrogate Loss: 5.852488\tTotal Loss: 0.906095\n","[3.9355928897857666, 0.6738666892051697, 0.3665306866168976, 0.4140262007713318, 0.3229517340660095, 0.23082369565963745, 0.2381046563386917, 0.15307866036891937, 0.3635273277759552, 0.24063833057880402, 0.3479306697845459, 0.2553337514400482, 0.29292047023773193, 0.15902335941791534, 0.2617132067680359, 0.3643794655799866, 0.3025355339050293, 0.19297054409980774, 0.14096345007419586, 0.22712038457393646, 0.21493905782699585, 0.22322925925254822, 0.16966961324214935, 0.15603354573249817, 0.32084590196609497]\n","[3.9355928897857666, 1.8014335632324219, 1.1883835792541504, 1.026267170906067, 1.193055272102356, 1.0797401666641235, 0.7704191207885742, 0.5957919955253601, 0.8642280101776123, 0.7750442028045654, 0.9298251271247864, 0.7564135789871216, 0.8013322949409485, 0.6755130887031555, 0.8744674921035767, 0.8399555683135986, 1.0779929161071777, 0.7404065132141113, 0.5526782870292664, 0.777575671672821, 0.691108226776123, 0.6855932474136353, 0.6961616277694702, 0.6521116495132446, 0.9060946702957153]\n","--> Finished Training Task 5. Starting Test phase:\n","---->Test set 0: Average loss: 0.0002, Accuracy: 9431/10000 (94%)\n","---->Test set 1: Average loss: 0.0005, Accuracy: 8874/10000 (89%)\n","---->Test set 2: Average loss: 0.0004, Accuracy: 8964/10000 (90%)\n","---->Test set 3: Average loss: 0.0010, Accuracy: 7977/10000 (80%)\n","---->Test set 4: Average loss: 0.0003, Accuracy: 9176/10000 (92%)\n","---->Test set 5: Average loss: 0.0003, Accuracy: 9294/10000 (93%)\n","---->Test set 6: Average loss: 0.0051, Accuracy: 868/10000 (9%)\n","---->Test set 7: Average loss: 0.0045, Accuracy: 1112/10000 (11%)\n","---->Test set 8: Average loss: 0.0060, Accuracy: 1038/10000 (10%)\n","---->Test set 9: Average loss: 0.0052, Accuracy: 730/10000 (7%)\n","--> Training Task 6:\n","----> Epoch 0:\n","---->[0/60000 (0%)]\tPrecision: 0.125000\tLoss: 4.636245\tSurrogate Loss: 0.000000\tTotal Loss: 4.636245\n","---->[12800/60000 (21%)]\tPrecision: 0.859375\tLoss: 0.564078\tSurrogate Loss: 13.883509\tTotal Loss: 1.952428\n","---->[25600/60000 (43%)]\tPrecision: 0.859375\tLoss: 0.478050\tSurrogate Loss: 11.816062\tTotal Loss: 1.659656\n","---->[38400/60000 (64%)]\tPrecision: 0.828125\tLoss: 0.585610\tSurrogate Loss: 8.203463\tTotal Loss: 1.405956\n","---->[51200/60000 (85%)]\tPrecision: 0.843750\tLoss: 0.402493\tSurrogate Loss: 8.194977\tTotal Loss: 1.221991\n","----> Epoch 1:\n","---->[0/60000 (0%)]\tPrecision: 0.890625\tLoss: 0.462517\tSurrogate Loss: 5.954070\tTotal Loss: 1.057924\n","---->[12800/60000 (21%)]\tPrecision: 0.906250\tLoss: 0.306684\tSurrogate Loss: 5.898872\tTotal Loss: 0.896571\n","---->[25600/60000 (43%)]\tPrecision: 0.843750\tLoss: 0.507746\tSurrogate Loss: 9.551909\tTotal Loss: 1.462937\n","---->[38400/60000 (64%)]\tPrecision: 0.875000\tLoss: 0.416742\tSurrogate Loss: 7.196141\tTotal Loss: 1.136357\n","---->[51200/60000 (85%)]\tPrecision: 0.937500\tLoss: 0.228171\tSurrogate Loss: 7.108697\tTotal Loss: 0.939040\n","----> Epoch 2:\n","---->[0/60000 (0%)]\tPrecision: 0.937500\tLoss: 0.160231\tSurrogate Loss: 7.426983\tTotal Loss: 0.902929\n","---->[12800/60000 (21%)]\tPrecision: 0.906250\tLoss: 0.275682\tSurrogate Loss: 5.899119\tTotal Loss: 0.865593\n","---->[25600/60000 (43%)]\tPrecision: 0.906250\tLoss: 0.317859\tSurrogate Loss: 7.775054\tTotal Loss: 1.095364\n","---->[38400/60000 (64%)]\tPrecision: 0.937500\tLoss: 0.236004\tSurrogate Loss: 6.884930\tTotal Loss: 0.924497\n","---->[51200/60000 (85%)]\tPrecision: 0.906250\tLoss: 0.277316\tSurrogate Loss: 8.946458\tTotal Loss: 1.171962\n","----> Epoch 3:\n","---->[0/60000 (0%)]\tPrecision: 0.921875\tLoss: 0.354947\tSurrogate Loss: 7.652853\tTotal Loss: 1.120232\n","---->[12800/60000 (21%)]\tPrecision: 0.906250\tLoss: 0.548497\tSurrogate Loss: 5.926862\tTotal Loss: 1.141184\n","---->[25600/60000 (43%)]\tPrecision: 0.937500\tLoss: 0.400715\tSurrogate Loss: 5.181946\tTotal Loss: 0.918909\n","---->[38400/60000 (64%)]\tPrecision: 0.937500\tLoss: 0.216756\tSurrogate Loss: 8.260845\tTotal Loss: 1.042841\n","---->[51200/60000 (85%)]\tPrecision: 0.906250\tLoss: 0.325640\tSurrogate Loss: 5.068835\tTotal Loss: 0.832524\n","----> Epoch 4:\n","---->[0/60000 (0%)]\tPrecision: 0.921875\tLoss: 0.335705\tSurrogate Loss: 7.858577\tTotal Loss: 1.121562\n","---->[12800/60000 (21%)]\tPrecision: 0.906250\tLoss: 0.268634\tSurrogate Loss: 5.784882\tTotal Loss: 0.847122\n","---->[25600/60000 (43%)]\tPrecision: 0.906250\tLoss: 0.304485\tSurrogate Loss: 5.519002\tTotal Loss: 0.856385\n","---->[38400/60000 (64%)]\tPrecision: 0.921875\tLoss: 0.349782\tSurrogate Loss: 8.783134\tTotal Loss: 1.228095\n","---->[51200/60000 (85%)]\tPrecision: 0.890625\tLoss: 0.418497\tSurrogate Loss: 6.469491\tTotal Loss: 1.065446\n","[4.636245250701904, 0.5640775561332703, 0.47804978489875793, 0.5856099128723145, 0.4024929702281952, 0.4625174105167389, 0.30668383836746216, 0.5077459216117859, 0.4167424440383911, 0.22817055881023407, 0.16023065149784088, 0.2756815552711487, 0.31785884499549866, 0.2360035479068756, 0.27731579542160034, 0.354947030544281, 0.5484973788261414, 0.40071454644203186, 0.21675634384155273, 0.32564035058021545, 0.3357046842575073, 0.2686343193054199, 0.3044845461845398, 0.3497816324234009, 0.4184970259666443]\n","[4.636245250701904, 1.9524283409118652, 1.6596559286117554, 1.4059562683105469, 1.221990704536438, 1.0579243898391724, 0.8965710997581482, 1.4629368782043457, 1.1363565921783447, 0.9390403032302856, 0.9029289484024048, 0.8655934929847717, 1.0953642129898071, 0.9244965314865112, 1.1719615459442139, 1.120232343673706, 1.141183614730835, 0.9189091920852661, 1.0428409576416016, 0.8325239419937134, 1.1215623617172241, 0.8471224904060364, 0.8563847541809082, 1.2280950546264648, 1.065446138381958]\n","--> Finished Training Task 6. Starting Test phase:\n","---->Test set 0: Average loss: 0.0002, Accuracy: 9342/10000 (93%)\n","---->Test set 1: Average loss: 0.0006, Accuracy: 8547/10000 (85%)\n","---->Test set 2: Average loss: 0.0011, Accuracy: 7579/10000 (76%)\n","---->Test set 3: Average loss: 0.0009, Accuracy: 8000/10000 (80%)\n","---->Test set 4: Average loss: 0.0004, Accuracy: 8823/10000 (88%)\n","---->Test set 5: Average loss: 0.0003, Accuracy: 9035/10000 (90%)\n","---->Test set 6: Average loss: 0.0004, Accuracy: 9049/10000 (90%)\n","---->Test set 7: Average loss: 0.0043, Accuracy: 1018/10000 (10%)\n","---->Test set 8: Average loss: 0.0046, Accuracy: 1094/10000 (11%)\n","---->Test set 9: Average loss: 0.0045, Accuracy: 847/10000 (8%)\n","--> Training Task 7:\n","----> Epoch 0:\n","---->[0/60000 (0%)]\tPrecision: 0.046875\tLoss: 4.763382\tSurrogate Loss: 0.000000\tTotal Loss: 4.763382\n","---->[12800/60000 (21%)]\tPrecision: 0.796875\tLoss: 0.621628\tSurrogate Loss: 16.029846\tTotal Loss: 2.224612\n","---->[25600/60000 (43%)]\tPrecision: 0.843750\tLoss: 0.752920\tSurrogate Loss: 9.486186\tTotal Loss: 1.701538\n","---->[38400/60000 (64%)]\tPrecision: 0.875000\tLoss: 0.440394\tSurrogate Loss: 7.417480\tTotal Loss: 1.182142\n","---->[51200/60000 (85%)]\tPrecision: 0.812500\tLoss: 0.545380\tSurrogate Loss: 10.668303\tTotal Loss: 1.612211\n","----> Epoch 1:\n","---->[0/60000 (0%)]\tPrecision: 0.875000\tLoss: 0.434975\tSurrogate Loss: 7.848011\tTotal Loss: 1.219776\n","---->[12800/60000 (21%)]\tPrecision: 0.968750\tLoss: 0.158965\tSurrogate Loss: 6.849462\tTotal Loss: 0.843911\n","---->[25600/60000 (43%)]\tPrecision: 0.906250\tLoss: 0.393298\tSurrogate Loss: 6.488688\tTotal Loss: 1.042166\n","---->[38400/60000 (64%)]\tPrecision: 0.890625\tLoss: 0.223562\tSurrogate Loss: 7.152833\tTotal Loss: 0.938845\n","---->[51200/60000 (85%)]\tPrecision: 0.921875\tLoss: 0.327917\tSurrogate Loss: 7.279666\tTotal Loss: 1.055883\n","----> Epoch 2:\n","---->[0/60000 (0%)]\tPrecision: 0.937500\tLoss: 0.255949\tSurrogate Loss: 6.935503\tTotal Loss: 0.949499\n","---->[12800/60000 (21%)]\tPrecision: 0.890625\tLoss: 0.234536\tSurrogate Loss: 7.361394\tTotal Loss: 0.970676\n","---->[25600/60000 (43%)]\tPrecision: 0.921875\tLoss: 0.369169\tSurrogate Loss: 7.123388\tTotal Loss: 1.081508\n","---->[38400/60000 (64%)]\tPrecision: 0.984375\tLoss: 0.055548\tSurrogate Loss: 6.350643\tTotal Loss: 0.690612\n","---->[51200/60000 (85%)]\tPrecision: 0.921875\tLoss: 0.292890\tSurrogate Loss: 8.181533\tTotal Loss: 1.111044\n","----> Epoch 3:\n","---->[0/60000 (0%)]\tPrecision: 0.906250\tLoss: 0.399584\tSurrogate Loss: 9.551161\tTotal Loss: 1.354700\n","---->[12800/60000 (21%)]\tPrecision: 0.859375\tLoss: 0.378883\tSurrogate Loss: 8.418975\tTotal Loss: 1.220780\n","---->[25600/60000 (43%)]\tPrecision: 0.953125\tLoss: 0.249293\tSurrogate Loss: 6.416248\tTotal Loss: 0.890918\n","---->[38400/60000 (64%)]\tPrecision: 0.921875\tLoss: 0.371136\tSurrogate Loss: 5.491437\tTotal Loss: 0.920280\n","---->[51200/60000 (85%)]\tPrecision: 0.875000\tLoss: 0.418957\tSurrogate Loss: 7.082977\tTotal Loss: 1.127254\n","----> Epoch 4:\n","---->[0/60000 (0%)]\tPrecision: 0.937500\tLoss: 0.178296\tSurrogate Loss: 6.072694\tTotal Loss: 0.785565\n","---->[12800/60000 (21%)]\tPrecision: 0.921875\tLoss: 0.350180\tSurrogate Loss: 6.031045\tTotal Loss: 0.953285\n","---->[25600/60000 (43%)]\tPrecision: 0.937500\tLoss: 0.390812\tSurrogate Loss: 6.956858\tTotal Loss: 1.086497\n","---->[38400/60000 (64%)]\tPrecision: 0.937500\tLoss: 0.202449\tSurrogate Loss: 8.986267\tTotal Loss: 1.101075\n","---->[51200/60000 (85%)]\tPrecision: 0.984375\tLoss: 0.145851\tSurrogate Loss: 6.826569\tTotal Loss: 0.828508\n","[4.763382434844971, 0.6216278076171875, 0.7529197931289673, 0.4403941333293915, 0.5453803539276123, 0.4349753260612488, 0.1589650958776474, 0.39329755306243896, 0.2235618382692337, 0.3279167413711548, 0.25594890117645264, 0.23453648388385773, 0.3691687285900116, 0.05554783344268799, 0.292890340089798, 0.399584025144577, 0.37888285517692566, 0.2492930293083191, 0.37113624811172485, 0.41895678639411926, 0.17829611897468567, 0.35018008947372437, 0.3908115327358246, 0.20244860649108887, 0.14585131406784058]\n","[4.763382434844971, 2.2246124744415283, 1.701538324356079, 1.1821421384811401, 1.612210750579834, 1.2197763919830322, 0.843911349773407, 1.0421663522720337, 0.938845157623291, 1.0558834075927734, 0.949499249458313, 0.9706758856773376, 1.0815075635910034, 0.6906121373176575, 1.1110436916351318, 1.3547000885009766, 1.220780372619629, 0.8909178376197815, 0.9202799797058105, 1.1272544860839844, 0.7855654954910278, 0.9532846212387085, 1.0864973068237305, 1.1010754108428955, 0.828508198261261]\n","--> Finished Training Task 7. Starting Test phase:\n","---->Test set 0: Average loss: 0.0002, Accuracy: 9304/10000 (93%)\n","---->Test set 1: Average loss: 0.0006, Accuracy: 8564/10000 (86%)\n","---->Test set 2: Average loss: 0.0010, Accuracy: 7735/10000 (77%)\n","---->Test set 3: Average loss: 0.0011, Accuracy: 7688/10000 (77%)\n","---->Test set 4: Average loss: 0.0006, Accuracy: 8416/10000 (84%)\n","---->Test set 5: Average loss: 0.0004, Accuracy: 8922/10000 (89%)\n","---->Test set 6: Average loss: 0.0004, Accuracy: 8936/10000 (89%)\n","---->Test set 7: Average loss: 0.0003, Accuracy: 9282/10000 (93%)\n","---->Test set 8: Average loss: 0.0048, Accuracy: 1028/10000 (10%)\n","---->Test set 9: Average loss: 0.0049, Accuracy: 856/10000 (9%)\n","--> Training Task 8:\n","----> Epoch 0:\n","---->[0/60000 (0%)]\tPrecision: 0.062500\tLoss: 5.402359\tSurrogate Loss: 0.000000\tTotal Loss: 5.402359\n","---->[12800/60000 (21%)]\tPrecision: 0.750000\tLoss: 0.843507\tSurrogate Loss: 15.954692\tTotal Loss: 2.438976\n","---->[25600/60000 (43%)]\tPrecision: 0.828125\tLoss: 0.514047\tSurrogate Loss: 10.641225\tTotal Loss: 1.578170\n","---->[38400/60000 (64%)]\tPrecision: 0.859375\tLoss: 0.313184\tSurrogate Loss: 8.830873\tTotal Loss: 1.196271\n","---->[51200/60000 (85%)]\tPrecision: 0.906250\tLoss: 0.550322\tSurrogate Loss: 7.788737\tTotal Loss: 1.329196\n","----> Epoch 1:\n","---->[0/60000 (0%)]\tPrecision: 0.906250\tLoss: 0.892435\tSurrogate Loss: 8.101269\tTotal Loss: 1.702562\n","---->[12800/60000 (21%)]\tPrecision: 0.843750\tLoss: 0.417190\tSurrogate Loss: 6.869183\tTotal Loss: 1.104108\n","---->[25600/60000 (43%)]\tPrecision: 0.890625\tLoss: 0.379406\tSurrogate Loss: 6.483876\tTotal Loss: 1.027794\n","---->[38400/60000 (64%)]\tPrecision: 0.921875\tLoss: 0.385003\tSurrogate Loss: 7.395682\tTotal Loss: 1.124571\n","---->[51200/60000 (85%)]\tPrecision: 0.890625\tLoss: 0.573412\tSurrogate Loss: 9.273612\tTotal Loss: 1.500774\n","----> Epoch 2:\n","---->[0/60000 (0%)]\tPrecision: 0.921875\tLoss: 0.341950\tSurrogate Loss: 6.063678\tTotal Loss: 0.948318\n","---->[12800/60000 (21%)]\tPrecision: 0.890625\tLoss: 0.512124\tSurrogate Loss: 6.307968\tTotal Loss: 1.142921\n","---->[25600/60000 (43%)]\tPrecision: 0.953125\tLoss: 0.311893\tSurrogate Loss: 6.443706\tTotal Loss: 0.956263\n","---->[38400/60000 (64%)]\tPrecision: 0.906250\tLoss: 0.339646\tSurrogate Loss: 5.297386\tTotal Loss: 0.869385\n","---->[51200/60000 (85%)]\tPrecision: 0.953125\tLoss: 0.108937\tSurrogate Loss: 6.564373\tTotal Loss: 0.765375\n","----> Epoch 3:\n","---->[0/60000 (0%)]\tPrecision: 0.937500\tLoss: 0.259724\tSurrogate Loss: 6.497211\tTotal Loss: 0.909445\n","---->[12800/60000 (21%)]\tPrecision: 0.890625\tLoss: 0.444857\tSurrogate Loss: 8.037259\tTotal Loss: 1.248583\n","---->[25600/60000 (43%)]\tPrecision: 0.906250\tLoss: 0.307332\tSurrogate Loss: 5.380271\tTotal Loss: 0.845360\n","---->[38400/60000 (64%)]\tPrecision: 0.953125\tLoss: 0.200490\tSurrogate Loss: 8.506909\tTotal Loss: 1.051181\n","---->[51200/60000 (85%)]\tPrecision: 0.968750\tLoss: 0.227370\tSurrogate Loss: 6.226699\tTotal Loss: 0.850039\n","----> Epoch 4:\n","---->[0/60000 (0%)]\tPrecision: 0.921875\tLoss: 0.249412\tSurrogate Loss: 7.553027\tTotal Loss: 1.004715\n","---->[12800/60000 (21%)]\tPrecision: 0.875000\tLoss: 0.388325\tSurrogate Loss: 5.752717\tTotal Loss: 0.963597\n","---->[25600/60000 (43%)]\tPrecision: 0.921875\tLoss: 0.272847\tSurrogate Loss: 5.665617\tTotal Loss: 0.839409\n","---->[38400/60000 (64%)]\tPrecision: 0.937500\tLoss: 0.160813\tSurrogate Loss: 8.041541\tTotal Loss: 0.964967\n","---->[51200/60000 (85%)]\tPrecision: 0.937500\tLoss: 0.280827\tSurrogate Loss: 7.754724\tTotal Loss: 1.056299\n","[5.4023590087890625, 0.8435066342353821, 0.5140470266342163, 0.31318357586860657, 0.5503218770027161, 0.8924350142478943, 0.41719022393226624, 0.3794064521789551, 0.3850032389163971, 0.5734124779701233, 0.341950386762619, 0.5121237635612488, 0.31189262866973877, 0.33964598178863525, 0.1089373230934143, 0.2597235143184662, 0.4448569416999817, 0.3073323965072632, 0.20048969984054565, 0.2273695319890976, 0.24941205978393555, 0.388325035572052, 0.2728467285633087, 0.16081297397613525, 0.2808266282081604]\n","[5.4023590087890625, 2.4389758110046387, 1.578169584274292, 1.1962709426879883, 1.3291956186294556, 1.702561855316162, 1.104108452796936, 1.0277941226959229, 1.124571442604065, 1.5007736682891846, 0.9483181238174438, 1.1429206132888794, 0.9562632441520691, 0.8693845868110657, 0.7653746604919434, 0.9094446897506714, 1.2485828399658203, 0.8453595042228699, 1.0511806011199951, 0.8500394225120544, 1.0047147274017334, 0.9635967016220093, 0.8394085168838501, 0.9649670720100403, 1.0562989711761475]\n","--> Finished Training Task 8. Starting Test phase:\n","---->Test set 0: Average loss: 0.0002, Accuracy: 9358/10000 (94%)\n","---->Test set 1: Average loss: 0.0006, Accuracy: 8566/10000 (86%)\n","---->Test set 2: Average loss: 0.0010, Accuracy: 7687/10000 (77%)\n","---->Test set 3: Average loss: 0.0009, Accuracy: 7991/10000 (80%)\n","---->Test set 4: Average loss: 0.0010, Accuracy: 7614/10000 (76%)\n","---->Test set 5: Average loss: 0.0006, Accuracy: 8449/10000 (84%)\n","---->Test set 6: Average loss: 0.0006, Accuracy: 8457/10000 (85%)\n","---->Test set 7: Average loss: 0.0003, Accuracy: 9226/10000 (92%)\n","---->Test set 8: Average loss: 0.0003, Accuracy: 9307/10000 (93%)\n","---->Test set 9: Average loss: 0.0054, Accuracy: 934/10000 (9%)\n","--> Training Task 9:\n","----> Epoch 0:\n","---->[0/60000 (0%)]\tPrecision: 0.093750\tLoss: 5.114141\tSurrogate Loss: 0.000000\tTotal Loss: 5.114141\n","---->[12800/60000 (21%)]\tPrecision: 0.718750\tLoss: 1.149539\tSurrogate Loss: 13.970322\tTotal Loss: 2.546571\n","---->[25600/60000 (43%)]\tPrecision: 0.875000\tLoss: 1.058660\tSurrogate Loss: 13.331970\tTotal Loss: 2.391857\n","---->[38400/60000 (64%)]\tPrecision: 0.843750\tLoss: 0.850663\tSurrogate Loss: 10.458334\tTotal Loss: 1.896497\n","---->[51200/60000 (85%)]\tPrecision: 0.937500\tLoss: 0.226731\tSurrogate Loss: 10.295196\tTotal Loss: 1.256251\n","----> Epoch 1:\n","---->[0/60000 (0%)]\tPrecision: 0.843750\tLoss: 0.498788\tSurrogate Loss: 10.653665\tTotal Loss: 1.564155\n","---->[12800/60000 (21%)]\tPrecision: 0.906250\tLoss: 0.333035\tSurrogate Loss: 8.031701\tTotal Loss: 1.136205\n","---->[25600/60000 (43%)]\tPrecision: 0.921875\tLoss: 0.242921\tSurrogate Loss: 8.529425\tTotal Loss: 1.095863\n","---->[38400/60000 (64%)]\tPrecision: 0.890625\tLoss: 0.434631\tSurrogate Loss: 7.073635\tTotal Loss: 1.141994\n","---->[51200/60000 (85%)]\tPrecision: 0.906250\tLoss: 0.378341\tSurrogate Loss: 6.954377\tTotal Loss: 1.073779\n","----> Epoch 2:\n","---->[0/60000 (0%)]\tPrecision: 0.875000\tLoss: 0.431307\tSurrogate Loss: 8.826230\tTotal Loss: 1.313930\n","---->[12800/60000 (21%)]\tPrecision: 0.890625\tLoss: 0.268474\tSurrogate Loss: 7.375344\tTotal Loss: 1.006009\n","---->[25600/60000 (43%)]\tPrecision: 0.906250\tLoss: 0.462729\tSurrogate Loss: 6.887138\tTotal Loss: 1.151443\n","---->[38400/60000 (64%)]\tPrecision: 0.968750\tLoss: 0.225865\tSurrogate Loss: 6.913116\tTotal Loss: 0.917176\n","---->[51200/60000 (85%)]\tPrecision: 0.937500\tLoss: 0.216980\tSurrogate Loss: 7.078179\tTotal Loss: 0.924798\n","----> Epoch 3:\n","---->[0/60000 (0%)]\tPrecision: 0.968750\tLoss: 0.097360\tSurrogate Loss: 7.814502\tTotal Loss: 0.878810\n","---->[12800/60000 (21%)]\tPrecision: 0.843750\tLoss: 0.409569\tSurrogate Loss: 6.336066\tTotal Loss: 1.043175\n","---->[25600/60000 (43%)]\tPrecision: 0.859375\tLoss: 0.544118\tSurrogate Loss: 5.824893\tTotal Loss: 1.126608\n","---->[38400/60000 (64%)]\tPrecision: 0.906250\tLoss: 0.612294\tSurrogate Loss: 8.797328\tTotal Loss: 1.492027\n","---->[51200/60000 (85%)]\tPrecision: 0.921875\tLoss: 0.332304\tSurrogate Loss: 7.613201\tTotal Loss: 1.093624\n","----> Epoch 4:\n","---->[0/60000 (0%)]\tPrecision: 0.890625\tLoss: 0.362297\tSurrogate Loss: 8.062371\tTotal Loss: 1.168535\n","---->[12800/60000 (21%)]\tPrecision: 0.937500\tLoss: 0.233514\tSurrogate Loss: 6.015395\tTotal Loss: 0.835054\n","---->[25600/60000 (43%)]\tPrecision: 0.906250\tLoss: 0.385453\tSurrogate Loss: 6.394402\tTotal Loss: 1.024894\n","---->[38400/60000 (64%)]\tPrecision: 0.953125\tLoss: 0.182248\tSurrogate Loss: 5.729219\tTotal Loss: 0.755170\n","---->[51200/60000 (85%)]\tPrecision: 0.937500\tLoss: 0.271314\tSurrogate Loss: 8.872814\tTotal Loss: 1.158595\n","[5.114140510559082, 1.1495386362075806, 1.0586601495742798, 0.850663423538208, 0.2267313152551651, 0.4987880289554596, 0.3330349922180176, 0.24292075634002686, 0.43463072180747986, 0.3783412575721741, 0.43130701780319214, 0.2684742212295532, 0.4627290964126587, 0.22586491703987122, 0.216980442404747, 0.09735996276140213, 0.4095686674118042, 0.5441184639930725, 0.6122944951057434, 0.332303911447525, 0.3622974157333374, 0.23351438343524933, 0.38545337319374084, 0.18224799633026123, 0.2713140547275543]\n","[5.114140510559082, 2.5465707778930664, 2.391857147216797, 1.8964968919754028, 1.2562508583068848, 1.5641545057296753, 1.1362051963806152, 1.0958632230758667, 1.1419942378997803, 1.0737789869308472, 1.3139300346374512, 1.0060086250305176, 1.1514430046081543, 0.9171764850616455, 0.9247983694076538, 0.8788101673126221, 1.043175220489502, 1.1266077756881714, 1.4920272827148438, 1.0936239957809448, 1.168534517288208, 0.8350539207458496, 1.0248935222625732, 0.7551699280738831, 1.1585954427719116]\n","--> Finished Training Task 9. Starting Test phase:\n","---->Test set 0: Average loss: 0.0002, Accuracy: 9392/10000 (94%)\n","---->Test set 1: Average loss: 0.0006, Accuracy: 8598/10000 (86%)\n","---->Test set 2: Average loss: 0.0012, Accuracy: 7364/10000 (74%)\n","---->Test set 3: Average loss: 0.0009, Accuracy: 7933/10000 (79%)\n","---->Test set 4: Average loss: 0.0007, Accuracy: 8015/10000 (80%)\n","---->Test set 5: Average loss: 0.0010, Accuracy: 7846/10000 (78%)\n","---->Test set 6: Average loss: 0.0008, Accuracy: 8044/10000 (80%)\n","---->Test set 7: Average loss: 0.0004, Accuracy: 8985/10000 (90%)\n","---->Test set 8: Average loss: 0.0003, Accuracy: 9208/10000 (92%)\n","---->Test set 9: Average loss: 0.0003, Accuracy: 9232/10000 (92%)\n","Accuracy: 0.8461700000000001\n","Confusion matrix:\n","0.1443,0.1361,0.0818,0.087,0.1144,0.1113,0.1121,0.1069,0.0582,0.112\n","0.9542,0.0925,0.1035,0.1022,0.0849,0.118,0.106,0.114,0.1238,0.0773\n","0.9402,0.9445,0.0871,0.1105,0.0657,0.0808,0.0828,0.0818,0.1271,0.0905\n","0.9362,0.9315,0.9496,0.1003,0.0975,0.0952,0.0886,0.0889,0.1083,0.0987\n","0.9374,0.8966,0.9165,0.9467,0.0877,0.1002,0.1061,0.0984,0.1061,0.0846\n","0.9394,0.9133,0.9135,0.9231,0.927,0.1026,0.0985,0.0868,0.1061,0.0794\n","0.9431,0.8874,0.8964,0.7977,0.9176,0.9294,0.0868,0.1112,0.1038,0.073\n","0.9342,0.8547,0.7579,0.8,0.8823,0.9035,0.9049,0.1018,0.1094,0.0847\n","0.9304,0.8564,0.7735,0.7688,0.8416,0.8922,0.8936,0.9282,0.1028,0.0856\n","0.9358,0.8566,0.7687,0.7991,0.7614,0.8449,0.8457,0.9226,0.9307,0.0934\n","0.9392,0.8598,0.7364,0.7933,0.8015,0.7846,0.8044,0.8985,0.9208,0.9232\n","--> Training:\n","---->Test set 0: Average loss: 0.2165, Accuracy: 1208/10000 (12%)\n","---->Test set 1: Average loss: 0.1776, Accuracy: 1215/10000 (12%)\n","---->Test set 2: Average loss: 0.2578, Accuracy: 953/10000 (10%)\n","---->Test set 3: Average loss: 0.1757, Accuracy: 1154/10000 (12%)\n","---->Test set 4: Average loss: 0.2083, Accuracy: 1035/10000 (10%)\n","---->Test set 5: Average loss: 0.1848, Accuracy: 1098/10000 (11%)\n","---->Test set 6: Average loss: 0.2138, Accuracy: 1053/10000 (11%)\n","---->Test set 7: Average loss: 0.1587, Accuracy: 724/10000 (7%)\n","---->Test set 8: Average loss: 0.1424, Accuracy: 780/10000 (8%)\n","---->Test set 9: Average loss: 0.1985, Accuracy: 1012/10000 (10%)\n","--> Training Task 0:\n","----> Epoch 0:\n","---->[0/60000 (0%)]\tPrecision: 0.078125\tLoss: 214.152313\tSurrogate Loss: 0.000000\tTotal Loss: 214.152313\n","---->[12800/60000 (21%)]\tPrecision: 0.953125\tLoss: 0.159161\tSurrogate Loss: 0.000000\tTotal Loss: 0.159161\n","---->[25600/60000 (43%)]\tPrecision: 0.906250\tLoss: 0.272648\tSurrogate Loss: 0.000000\tTotal Loss: 0.272648\n","---->[38400/60000 (64%)]\tPrecision: 0.937500\tLoss: 0.314856\tSurrogate Loss: 0.000000\tTotal Loss: 0.314856\n","---->[51200/60000 (85%)]\tPrecision: 0.953125\tLoss: 0.299227\tSurrogate Loss: 0.000000\tTotal Loss: 0.299227\n","----> Epoch 1:\n","---->[0/60000 (0%)]\tPrecision: 0.875000\tLoss: 0.617381\tSurrogate Loss: 0.000000\tTotal Loss: 0.617381\n","---->[12800/60000 (21%)]\tPrecision: 0.937500\tLoss: 0.468316\tSurrogate Loss: 0.000000\tTotal Loss: 0.468316\n","---->[25600/60000 (43%)]\tPrecision: 0.968750\tLoss: 0.098228\tSurrogate Loss: 0.000000\tTotal Loss: 0.098228\n","---->[38400/60000 (64%)]\tPrecision: 0.921875\tLoss: 0.242561\tSurrogate Loss: 0.000000\tTotal Loss: 0.242561\n","---->[51200/60000 (85%)]\tPrecision: 0.937500\tLoss: 0.136003\tSurrogate Loss: 0.000000\tTotal Loss: 0.136003\n","----> Epoch 2:\n","---->[0/60000 (0%)]\tPrecision: 0.937500\tLoss: 0.245846\tSurrogate Loss: 0.000000\tTotal Loss: 0.245846\n","---->[12800/60000 (21%)]\tPrecision: 0.968750\tLoss: 0.075225\tSurrogate Loss: 0.000000\tTotal Loss: 0.075225\n","---->[25600/60000 (43%)]\tPrecision: 0.921875\tLoss: 0.584409\tSurrogate Loss: 0.000000\tTotal Loss: 0.584409\n","---->[38400/60000 (64%)]\tPrecision: 1.000000\tLoss: 0.033568\tSurrogate Loss: 0.000000\tTotal Loss: 0.033568\n","---->[51200/60000 (85%)]\tPrecision: 0.906250\tLoss: 0.426925\tSurrogate Loss: 0.000000\tTotal Loss: 0.426925\n","----> Epoch 3:\n","---->[0/60000 (0%)]\tPrecision: 0.968750\tLoss: 0.120830\tSurrogate Loss: 0.000000\tTotal Loss: 0.120830\n","---->[12800/60000 (21%)]\tPrecision: 0.968750\tLoss: 0.102162\tSurrogate Loss: 0.000000\tTotal Loss: 0.102162\n","---->[25600/60000 (43%)]\tPrecision: 0.906250\tLoss: 0.335102\tSurrogate Loss: 0.000000\tTotal Loss: 0.335102\n","---->[38400/60000 (64%)]\tPrecision: 0.984375\tLoss: 0.065531\tSurrogate Loss: 0.000000\tTotal Loss: 0.065531\n","---->[51200/60000 (85%)]\tPrecision: 0.921875\tLoss: 0.150485\tSurrogate Loss: 0.000000\tTotal Loss: 0.150485\n","----> Epoch 4:\n","---->[0/60000 (0%)]\tPrecision: 0.968750\tLoss: 0.149521\tSurrogate Loss: 0.000000\tTotal Loss: 0.149521\n","---->[12800/60000 (21%)]\tPrecision: 0.953125\tLoss: 0.160186\tSurrogate Loss: 0.000000\tTotal Loss: 0.160186\n","---->[25600/60000 (43%)]\tPrecision: 0.953125\tLoss: 0.131408\tSurrogate Loss: 0.000000\tTotal Loss: 0.131408\n","---->[38400/60000 (64%)]\tPrecision: 0.937500\tLoss: 0.427225\tSurrogate Loss: 0.000000\tTotal Loss: 0.427225\n","---->[51200/60000 (85%)]\tPrecision: 0.921875\tLoss: 0.398331\tSurrogate Loss: 0.000000\tTotal Loss: 0.398331\n","----> Epoch 5:\n","---->[0/60000 (0%)]\tPrecision: 0.984375\tLoss: 0.078604\tSurrogate Loss: 0.000000\tTotal Loss: 0.078604\n","---->[12800/60000 (21%)]\tPrecision: 0.968750\tLoss: 0.067274\tSurrogate Loss: 0.000000\tTotal Loss: 0.067274\n","---->[25600/60000 (43%)]\tPrecision: 0.953125\tLoss: 0.373542\tSurrogate Loss: 0.000000\tTotal Loss: 0.373542\n","---->[38400/60000 (64%)]\tPrecision: 0.968750\tLoss: 0.061361\tSurrogate Loss: 0.000000\tTotal Loss: 0.061361\n","---->[51200/60000 (85%)]\tPrecision: 0.953125\tLoss: 0.119078\tSurrogate Loss: 0.000000\tTotal Loss: 0.119078\n","[214.15231323242188, 0.15916131436824799, 0.27264752984046936, 0.31485608220100403, 0.299227237701416, 0.617380678653717, 0.46831589937210083, 0.09822779893875122, 0.2425614297389984, 0.1360028088092804, 0.2458462119102478, 0.07522450387477875, 0.5844087600708008, 0.03356785699725151, 0.4269251823425293, 0.12082965672016144, 0.10216205567121506, 0.3351016938686371, 0.06553108245134354, 0.15048541128635406, 0.14952139556407928, 0.1601855605840683, 0.13140791654586792, 0.427225261926651, 0.39833128452301025, 0.07860399037599564, 0.06727409362792969, 0.37354180216789246, 0.06136131286621094, 0.1190783679485321]\n","[214.15231323242188, 0.15916131436824799, 0.27264752984046936, 0.31485608220100403, 0.299227237701416, 0.617380678653717, 0.46831589937210083, 0.09822779893875122, 0.2425614297389984, 0.1360028088092804, 0.2458462119102478, 0.07522450387477875, 0.5844087600708008, 0.03356785699725151, 0.4269251823425293, 0.12082965672016144, 0.10216205567121506, 0.3351016938686371, 0.06553108245134354, 0.15048541128635406, 0.14952139556407928, 0.1601855605840683, 0.13140791654586792, 0.427225261926651, 0.39833128452301025, 0.07860399037599564, 0.06727409362792969, 0.37354180216789246, 0.06136131286621094, 0.1190783679485321]\n","--> Finished Training Task 0. Starting Test phase:\n","---->Test set 0: Average loss: 0.0002, Accuracy: 9588/10000 (96%)\n","---->Test set 1: Average loss: 0.0050, Accuracy: 1065/10000 (11%)\n","---->Test set 2: Average loss: 0.0034, Accuracy: 1386/10000 (14%)\n","---->Test set 3: Average loss: 0.0038, Accuracy: 1033/10000 (10%)\n","---->Test set 4: Average loss: 0.0045, Accuracy: 924/10000 (9%)\n","---->Test set 5: Average loss: 0.0037, Accuracy: 1022/10000 (10%)\n","---->Test set 6: Average loss: 0.0042, Accuracy: 1324/10000 (13%)\n","---->Test set 7: Average loss: 0.0041, Accuracy: 1194/10000 (12%)\n","---->Test set 8: Average loss: 0.0053, Accuracy: 941/10000 (9%)\n","---->Test set 9: Average loss: 0.0039, Accuracy: 1436/10000 (14%)\n","--> Training Task 1:\n","----> Epoch 0:\n","---->[0/60000 (0%)]\tPrecision: 0.109375\tLoss: 5.196594\tSurrogate Loss: 0.000000\tTotal Loss: 5.196594\n","---->[12800/60000 (21%)]\tPrecision: 0.140625\tLoss: 2.307477\tSurrogate Loss: 3.278907\tTotal Loss: 2.635368\n","---->[25600/60000 (43%)]\tPrecision: 0.093750\tLoss: 2.294568\tSurrogate Loss: 2.844077\tTotal Loss: 2.578976\n","---->[38400/60000 (64%)]\tPrecision: 0.187500\tLoss: 2.280234\tSurrogate Loss: 3.820707\tTotal Loss: 2.662305\n","---->[51200/60000 (85%)]\tPrecision: 0.031250\tLoss: 2.314557\tSurrogate Loss: 2.713371\tTotal Loss: 2.585894\n","----> Epoch 1:\n","---->[0/60000 (0%)]\tPrecision: 0.078125\tLoss: 2.291731\tSurrogate Loss: 1.618253\tTotal Loss: 2.453557\n","---->[12800/60000 (21%)]\tPrecision: 0.140625\tLoss: 2.296541\tSurrogate Loss: 2.773179\tTotal Loss: 2.573859\n","---->[25600/60000 (43%)]\tPrecision: 0.125000\tLoss: 2.309517\tSurrogate Loss: 1.596933\tTotal Loss: 2.469211\n","---->[38400/60000 (64%)]\tPrecision: 0.078125\tLoss: 2.310955\tSurrogate Loss: 1.223630\tTotal Loss: 2.433318\n","---->[51200/60000 (85%)]\tPrecision: 0.203125\tLoss: 2.127750\tSurrogate Loss: 1.725557\tTotal Loss: 2.300306\n","----> Epoch 2:\n","---->[0/60000 (0%)]\tPrecision: 0.046875\tLoss: 2.317691\tSurrogate Loss: 1.184786\tTotal Loss: 2.436170\n","---->[12800/60000 (21%)]\tPrecision: 0.125000\tLoss: 2.301223\tSurrogate Loss: 2.701000\tTotal Loss: 2.571323\n","---->[25600/60000 (43%)]\tPrecision: 0.187500\tLoss: 2.139192\tSurrogate Loss: 1.143899\tTotal Loss: 2.253582\n","---->[38400/60000 (64%)]\tPrecision: 0.171875\tLoss: 2.068355\tSurrogate Loss: 0.732320\tTotal Loss: 2.141587\n","---->[51200/60000 (85%)]\tPrecision: 0.125000\tLoss: 2.112304\tSurrogate Loss: 10.374812\tTotal Loss: 3.149785\n","----> Epoch 3:\n","---->[0/60000 (0%)]\tPrecision: 0.156250\tLoss: 2.045020\tSurrogate Loss: 2.037191\tTotal Loss: 2.248739\n","---->[12800/60000 (21%)]\tPrecision: 0.234375\tLoss: 1.978383\tSurrogate Loss: 1.434160\tTotal Loss: 2.121799\n","---->[25600/60000 (43%)]\tPrecision: 0.328125\tLoss: 1.732303\tSurrogate Loss: 0.907235\tTotal Loss: 1.823026\n","---->[38400/60000 (64%)]\tPrecision: 0.187500\tLoss: 1.959655\tSurrogate Loss: 0.657759\tTotal Loss: 2.025431\n","---->[51200/60000 (85%)]\tPrecision: 0.234375\tLoss: 2.028205\tSurrogate Loss: 1.327652\tTotal Loss: 2.160970\n","----> Epoch 4:\n","---->[0/60000 (0%)]\tPrecision: 0.125000\tLoss: 2.157828\tSurrogate Loss: 1.028313\tTotal Loss: 2.260660\n","---->[12800/60000 (21%)]\tPrecision: 0.171875\tLoss: 2.055991\tSurrogate Loss: 0.399491\tTotal Loss: 2.095940\n","---->[25600/60000 (43%)]\tPrecision: 0.125000\tLoss: 2.074558\tSurrogate Loss: 1.192363\tTotal Loss: 2.193794\n","---->[38400/60000 (64%)]\tPrecision: 0.109375\tLoss: 2.099152\tSurrogate Loss: 0.408627\tTotal Loss: 2.140014\n","---->[51200/60000 (85%)]\tPrecision: 0.187500\tLoss: 2.043943\tSurrogate Loss: 4.362536\tTotal Loss: 2.480196\n","----> Epoch 5:\n","---->[0/60000 (0%)]\tPrecision: 0.265625\tLoss: 1.929507\tSurrogate Loss: 4.247190\tTotal Loss: 2.354226\n","---->[12800/60000 (21%)]\tPrecision: 0.125000\tLoss: 2.048479\tSurrogate Loss: 2.701092\tTotal Loss: 2.318588\n","---->[25600/60000 (43%)]\tPrecision: 0.328125\tLoss: 1.751650\tSurrogate Loss: 7.078983\tTotal Loss: 2.459548\n","---->[38400/60000 (64%)]\tPrecision: 0.265625\tLoss: 2.419029\tSurrogate Loss: 1.967580\tTotal Loss: 2.615787\n","---->[51200/60000 (85%)]\tPrecision: 0.203125\tLoss: 2.063385\tSurrogate Loss: 1.746623\tTotal Loss: 2.238047\n","[5.196593761444092, 2.3074774742126465, 2.2945683002471924, 2.2802340984344482, 2.31455659866333, 2.291731357574463, 2.296541213989258, 2.3095173835754395, 2.310955047607422, 2.1277501583099365, 2.3176910877227783, 2.301222801208496, 2.1391916275024414, 2.068354606628418, 2.1123037338256836, 2.0450196266174316, 1.97838294506073, 1.7323026657104492, 1.9596548080444336, 2.028204917907715, 2.1578283309936523, 2.055990695953369, 2.0745582580566406, 2.099151611328125, 2.04394268989563, 1.9295073747634888, 2.048478603363037, 1.7516499757766724, 2.4190289974212646, 2.063384532928467]\n","[5.196593761444092, 2.6353681087493896, 2.5789759159088135, 2.6623048782348633, 2.5858936309814453, 2.453556537628174, 2.573859214782715, 2.469210624694824, 2.4333181381225586, 2.3003058433532715, 2.4361696243286133, 2.5713229179382324, 2.2535815238952637, 2.1415865421295166, 3.149785041809082, 2.2487387657165527, 2.1217989921569824, 1.823026180267334, 2.025430679321289, 2.1609702110290527, 2.260659694671631, 2.095939874649048, 2.1937944889068604, 2.140014410018921, 2.480196237564087, 2.3542263507843018, 2.3185877799987793, 2.459548234939575, 2.6157870292663574, 2.238046884536743]\n","--> Finished Training Task 1. Starting Test phase:\n","---->Test set 0: Average loss: 0.0002, Accuracy: 9356/10000 (94%)\n","---->Test set 1: Average loss: 0.0021, Accuracy: 2090/10000 (21%)\n","---->Test set 2: Average loss: 0.0024, Accuracy: 1008/10000 (10%)\n","---->Test set 3: Average loss: 0.0024, Accuracy: 1011/10000 (10%)\n","---->Test set 4: Average loss: 0.0024, Accuracy: 1018/10000 (10%)\n","---->Test set 5: Average loss: 0.0024, Accuracy: 1011/10000 (10%)\n","---->Test set 6: Average loss: 0.0024, Accuracy: 1008/10000 (10%)\n","---->Test set 7: Average loss: 0.0024, Accuracy: 1015/10000 (10%)\n","---->Test set 8: Average loss: 0.0024, Accuracy: 1008/10000 (10%)\n","---->Test set 9: Average loss: 0.0024, Accuracy: 1009/10000 (10%)\n","--> Training Task 2:\n","----> Epoch 0:\n","---->[0/60000 (0%)]\tPrecision: 0.140625\tLoss: 2.317603\tSurrogate Loss: 0.000000\tTotal Loss: 2.317603\n","---->[12800/60000 (21%)]\tPrecision: 0.062500\tLoss: 2.315138\tSurrogate Loss: 0.164512\tTotal Loss: 2.331589\n","---->[25600/60000 (43%)]\tPrecision: 0.093750\tLoss: 2.282756\tSurrogate Loss: 0.163079\tTotal Loss: 2.299063\n","---->[38400/60000 (64%)]\tPrecision: 0.109375\tLoss: 2.310017\tSurrogate Loss: 0.165631\tTotal Loss: 2.326580\n","---->[51200/60000 (85%)]\tPrecision: 0.109375\tLoss: 2.292619\tSurrogate Loss: 0.159541\tTotal Loss: 2.308573\n","----> Epoch 1:\n","---->[0/60000 (0%)]\tPrecision: 0.015625\tLoss: 2.314221\tSurrogate Loss: 0.166592\tTotal Loss: 2.330881\n","---->[12800/60000 (21%)]\tPrecision: 0.093750\tLoss: 2.355333\tSurrogate Loss: 0.159704\tTotal Loss: 2.371303\n","---->[25600/60000 (43%)]\tPrecision: 0.156250\tLoss: 2.289100\tSurrogate Loss: 0.194762\tTotal Loss: 2.308576\n","---->[38400/60000 (64%)]\tPrecision: 0.125000\tLoss: 2.301673\tSurrogate Loss: 0.183643\tTotal Loss: 2.320037\n","---->[51200/60000 (85%)]\tPrecision: 0.078125\tLoss: 2.335698\tSurrogate Loss: 0.161403\tTotal Loss: 2.351839\n","----> Epoch 2:\n","---->[0/60000 (0%)]\tPrecision: 0.156250\tLoss: 2.311661\tSurrogate Loss: 0.162160\tTotal Loss: 2.327877\n","---->[12800/60000 (21%)]\tPrecision: 0.046875\tLoss: 2.288046\tSurrogate Loss: 0.173608\tTotal Loss: 2.305407\n","---->[25600/60000 (43%)]\tPrecision: 0.062500\tLoss: 2.313264\tSurrogate Loss: 2.028984\tTotal Loss: 2.516162\n","---->[38400/60000 (64%)]\tPrecision: 0.062500\tLoss: 2.342845\tSurrogate Loss: 0.396488\tTotal Loss: 2.382494\n","---->[51200/60000 (85%)]\tPrecision: 0.078125\tLoss: 2.338026\tSurrogate Loss: 0.258143\tTotal Loss: 2.363840\n","----> Epoch 3:\n","---->[0/60000 (0%)]\tPrecision: 0.062500\tLoss: 2.311981\tSurrogate Loss: 0.223004\tTotal Loss: 2.334282\n","---->[12800/60000 (21%)]\tPrecision: 0.062500\tLoss: 2.334051\tSurrogate Loss: 0.195439\tTotal Loss: 2.353595\n","---->[25600/60000 (43%)]\tPrecision: 0.062500\tLoss: 2.308713\tSurrogate Loss: 0.192022\tTotal Loss: 2.327915\n","---->[38400/60000 (64%)]\tPrecision: 0.125000\tLoss: 2.305801\tSurrogate Loss: 0.188900\tTotal Loss: 2.324691\n","---->[51200/60000 (85%)]\tPrecision: 0.062500\tLoss: 2.346429\tSurrogate Loss: 0.173186\tTotal Loss: 2.363748\n","----> Epoch 4:\n","---->[0/60000 (0%)]\tPrecision: 0.093750\tLoss: 2.319900\tSurrogate Loss: 0.185318\tTotal Loss: 2.338431\n","---->[12800/60000 (21%)]\tPrecision: 0.156250\tLoss: 2.329296\tSurrogate Loss: 0.183829\tTotal Loss: 2.347678\n","---->[25600/60000 (43%)]\tPrecision: 0.140625\tLoss: 2.343752\tSurrogate Loss: 0.171695\tTotal Loss: 2.360922\n","---->[38400/60000 (64%)]\tPrecision: 0.031250\tLoss: 2.336101\tSurrogate Loss: 0.175208\tTotal Loss: 2.353621\n","---->[51200/60000 (85%)]\tPrecision: 0.078125\tLoss: 2.342925\tSurrogate Loss: 0.162637\tTotal Loss: 2.359189\n","----> Epoch 5:\n","---->[0/60000 (0%)]\tPrecision: 0.062500\tLoss: 2.314607\tSurrogate Loss: 0.183745\tTotal Loss: 2.332982\n","---->[12800/60000 (21%)]\tPrecision: 0.109375\tLoss: 2.300645\tSurrogate Loss: 0.182174\tTotal Loss: 2.318862\n","---->[25600/60000 (43%)]\tPrecision: 0.109375\tLoss: 2.292930\tSurrogate Loss: 0.185601\tTotal Loss: 2.311490\n","---->[38400/60000 (64%)]\tPrecision: 0.109375\tLoss: 2.294179\tSurrogate Loss: 0.159573\tTotal Loss: 2.310137\n","---->[51200/60000 (85%)]\tPrecision: 0.109375\tLoss: 2.294987\tSurrogate Loss: 0.165260\tTotal Loss: 2.311513\n","[2.31760311126709, 2.3151378631591797, 2.2827556133270264, 2.3100171089172363, 2.292618989944458, 2.3142213821411133, 2.355332612991333, 2.289099931716919, 2.30167293548584, 2.335698366165161, 2.3116605281829834, 2.288046360015869, 2.3132638931274414, 2.3428449630737305, 2.3380260467529297, 2.311981439590454, 2.3340506553649902, 2.308712959289551, 2.3058011531829834, 2.3464291095733643, 2.319899559020996, 2.3292956352233887, 2.343752145767212, 2.3361005783081055, 2.3429250717163086, 2.3146071434020996, 2.300644874572754, 2.2929301261901855, 2.2941792011260986, 2.294987440109253]\n","[2.31760311126709, 2.3315889835357666, 2.2990634441375732, 2.326580286026001, 2.308573007583618, 2.330880641937256, 2.371303081512451, 2.3085761070251465, 2.3200371265411377, 2.3518385887145996, 2.327876567840576, 2.3054070472717285, 2.516162395477295, 2.382493734359741, 2.363840341567993, 2.3342819213867188, 2.3535945415496826, 2.3279151916503906, 2.3246910572052, 2.3637475967407227, 2.3384313583374023, 2.3476784229278564, 2.360921621322632, 2.353621482849121, 2.3591887950897217, 2.332981586456299, 2.318862199783325, 2.311490297317505, 2.310136556625366, 2.3115134239196777]\n","--> Finished Training Task 2. Starting Test phase:\n","---->Test set 0: Average loss: 0.0002, Accuracy: 9356/10000 (94%)\n","---->Test set 1: Average loss: 0.0021, Accuracy: 2091/10000 (21%)\n","---->Test set 2: Average loss: 0.0023, Accuracy: 1009/10000 (10%)\n","---->Test set 3: Average loss: 0.0023, Accuracy: 1012/10000 (10%)\n","---->Test set 4: Average loss: 0.0023, Accuracy: 1013/10000 (10%)\n","---->Test set 5: Average loss: 0.0023, Accuracy: 1009/10000 (10%)\n","---->Test set 6: Average loss: 0.0023, Accuracy: 1008/10000 (10%)\n","---->Test set 7: Average loss: 0.0023, Accuracy: 1010/10000 (10%)\n","---->Test set 8: Average loss: 0.0023, Accuracy: 1009/10000 (10%)\n","---->Test set 9: Average loss: 0.0023, Accuracy: 1009/10000 (10%)\n","--> Training Task 3:\n","----> Epoch 0:\n","---->[0/60000 (0%)]\tPrecision: 0.093750\tLoss: 2.321967\tSurrogate Loss: 0.000000\tTotal Loss: 2.321967\n","---->[12800/60000 (21%)]\tPrecision: 0.109375\tLoss: 2.292994\tSurrogate Loss: 0.036015\tTotal Loss: 2.296596\n","---->[25600/60000 (43%)]\tPrecision: 0.078125\tLoss: 2.314223\tSurrogate Loss: 0.035236\tTotal Loss: 2.317746\n","---->[38400/60000 (64%)]\tPrecision: 0.109375\tLoss: 2.303109\tSurrogate Loss: 0.034209\tTotal Loss: 2.306530\n","---->[51200/60000 (85%)]\tPrecision: 0.234375\tLoss: 2.102719\tSurrogate Loss: 4.000826\tTotal Loss: 2.502802\n","----> Epoch 1:\n","---->[0/60000 (0%)]\tPrecision: 0.062500\tLoss: 2.327684\tSurrogate Loss: 1.116804\tTotal Loss: 2.439365\n","---->[12800/60000 (21%)]\tPrecision: 0.109375\tLoss: 2.250501\tSurrogate Loss: 2.442211\tTotal Loss: 2.494722\n","---->[25600/60000 (43%)]\tPrecision: 0.187500\tLoss: 2.175049\tSurrogate Loss: 0.600974\tTotal Loss: 2.235146\n","---->[38400/60000 (64%)]\tPrecision: 0.156250\tLoss: 2.055403\tSurrogate Loss: 1.760012\tTotal Loss: 2.231405\n","---->[51200/60000 (85%)]\tPrecision: 0.156250\tLoss: 2.054876\tSurrogate Loss: 3.333646\tTotal Loss: 2.388241\n","----> Epoch 2:\n","---->[0/60000 (0%)]\tPrecision: 0.265625\tLoss: 2.394494\tSurrogate Loss: 0.992345\tTotal Loss: 2.493728\n","---->[12800/60000 (21%)]\tPrecision: 0.156250\tLoss: 2.045956\tSurrogate Loss: 0.342877\tTotal Loss: 2.080244\n","---->[25600/60000 (43%)]\tPrecision: 0.109375\tLoss: 2.257803\tSurrogate Loss: 3.050126\tTotal Loss: 2.562815\n","---->[38400/60000 (64%)]\tPrecision: 0.375000\tLoss: 1.648747\tSurrogate Loss: 0.896912\tTotal Loss: 1.738438\n","---->[51200/60000 (85%)]\tPrecision: 0.156250\tLoss: 2.142606\tSurrogate Loss: 0.680029\tTotal Loss: 2.210609\n","----> Epoch 3:\n","---->[0/60000 (0%)]\tPrecision: 0.093750\tLoss: 2.163924\tSurrogate Loss: 1.382530\tTotal Loss: 2.302177\n","---->[12800/60000 (21%)]\tPrecision: 0.218750\tLoss: 1.969080\tSurrogate Loss: 0.366032\tTotal Loss: 2.005683\n","---->[25600/60000 (43%)]\tPrecision: 0.234375\tLoss: 1.988957\tSurrogate Loss: 1.034598\tTotal Loss: 2.092417\n","---->[38400/60000 (64%)]\tPrecision: 0.140625\tLoss: 2.078022\tSurrogate Loss: 0.550990\tTotal Loss: 2.133121\n","---->[51200/60000 (85%)]\tPrecision: 0.140625\tLoss: 3.586673\tSurrogate Loss: 56.579338\tTotal Loss: 9.244606\n","----> Epoch 4:\n","---->[0/60000 (0%)]\tPrecision: 0.265625\tLoss: 1.995618\tSurrogate Loss: 7.738498\tTotal Loss: 2.769468\n","---->[12800/60000 (21%)]\tPrecision: 0.218750\tLoss: 1.974697\tSurrogate Loss: 3.048111\tTotal Loss: 2.279508\n","---->[25600/60000 (43%)]\tPrecision: 0.234375\tLoss: 2.003244\tSurrogate Loss: 1.428016\tTotal Loss: 2.146045\n","---->[38400/60000 (64%)]\tPrecision: 0.265625\tLoss: 1.897659\tSurrogate Loss: 1.106002\tTotal Loss: 2.008259\n","---->[51200/60000 (85%)]\tPrecision: 0.265625\tLoss: 2.042759\tSurrogate Loss: 1.168360\tTotal Loss: 2.159595\n","----> Epoch 5:\n","---->[0/60000 (0%)]\tPrecision: 0.187500\tLoss: 1.985888\tSurrogate Loss: 0.917213\tTotal Loss: 2.077609\n","---->[12800/60000 (21%)]\tPrecision: 0.203125\tLoss: 2.133454\tSurrogate Loss: 0.633440\tTotal Loss: 2.196798\n","---->[25600/60000 (43%)]\tPrecision: 0.218750\tLoss: 2.018667\tSurrogate Loss: 1.055135\tTotal Loss: 2.124181\n","---->[38400/60000 (64%)]\tPrecision: 0.203125\tLoss: 2.006894\tSurrogate Loss: 0.632011\tTotal Loss: 2.070095\n","---->[51200/60000 (85%)]\tPrecision: 0.296875\tLoss: 2.035963\tSurrogate Loss: 0.496046\tTotal Loss: 2.085568\n","[2.3219666481018066, 2.292994260787964, 2.314222574234009, 2.3031089305877686, 2.1027190685272217, 2.3276844024658203, 2.2505009174346924, 2.175048589706421, 2.055403470993042, 2.0548763275146484, 2.394493818283081, 2.0459563732147217, 2.257802724838257, 1.648747205734253, 2.142606019973755, 2.163923740386963, 1.9690802097320557, 1.9889569282531738, 2.078021764755249, 3.58667254447937, 1.9956183433532715, 1.9746973514556885, 2.0032436847686768, 1.8976585865020752, 2.042759418487549, 1.9858875274658203, 2.1334543228149414, 2.018666982650757, 2.0068938732147217, 2.035963296890259]\n","[2.3219666481018066, 2.296595811843872, 2.317746162414551, 2.3065297603607178, 2.5028016567230225, 2.4393646717071533, 2.4947221279144287, 2.2351460456848145, 2.2314047813415527, 2.3882410526275635, 2.4937283992767334, 2.0802440643310547, 2.5628154277801514, 1.738438367843628, 2.210608959197998, 2.3021767139434814, 2.005683422088623, 2.092416763305664, 2.1331207752227783, 9.244606018066406, 2.769468307495117, 2.279508352279663, 2.146045207977295, 2.008258819580078, 2.159595489501953, 2.077608823776245, 2.196798324584961, 2.124180555343628, 2.0700950622558594, 2.0855679512023926]\n","--> Finished Training Task 3. Starting Test phase:\n","---->Test set 0: Average loss: 0.0005, Accuracy: 8497/10000 (85%)\n","---->Test set 1: Average loss: 0.0021, Accuracy: 2101/10000 (21%)\n","---->Test set 2: Average loss: 0.0023, Accuracy: 1009/10000 (10%)\n","---->Test set 3: Average loss: 0.0021, Accuracy: 2110/10000 (21%)\n","---->Test set 4: Average loss: 0.0023, Accuracy: 1012/10000 (10%)\n","---->Test set 5: Average loss: 0.0023, Accuracy: 1009/10000 (10%)\n","---->Test set 6: Average loss: 0.0023, Accuracy: 1024/10000 (10%)\n","---->Test set 7: Average loss: 0.0023, Accuracy: 1010/10000 (10%)\n","---->Test set 8: Average loss: 0.0023, Accuracy: 1009/10000 (10%)\n","---->Test set 9: Average loss: 0.0023, Accuracy: 1009/10000 (10%)\n","--> Training Task 4:\n","----> Epoch 0:\n","---->[0/60000 (0%)]\tPrecision: 0.078125\tLoss: 2.347188\tSurrogate Loss: 0.000000\tTotal Loss: 2.347188\n","---->[12800/60000 (21%)]\tPrecision: 0.140625\tLoss: 2.291547\tSurrogate Loss: 3.486811\tTotal Loss: 2.640228\n","---->[25600/60000 (43%)]\tPrecision: 0.125000\tLoss: 2.282918\tSurrogate Loss: 7.025881\tTotal Loss: 2.985506\n","---->[38400/60000 (64%)]\tPrecision: 0.140625\tLoss: 2.121299\tSurrogate Loss: 1.865428\tTotal Loss: 2.307842\n","---->[51200/60000 (85%)]\tPrecision: 0.218750\tLoss: 2.092445\tSurrogate Loss: 0.891721\tTotal Loss: 2.181617\n","----> Epoch 1:\n","---->[0/60000 (0%)]\tPrecision: 0.187500\tLoss: 2.113601\tSurrogate Loss: 0.661226\tTotal Loss: 2.179723\n","---->[12800/60000 (21%)]\tPrecision: 0.156250\tLoss: 2.124401\tSurrogate Loss: 0.465022\tTotal Loss: 2.170903\n","---->[25600/60000 (43%)]\tPrecision: 0.140625\tLoss: 2.166082\tSurrogate Loss: 51.926762\tTotal Loss: 7.358758\n","---->[38400/60000 (64%)]\tPrecision: 0.218750\tLoss: 2.101016\tSurrogate Loss: 2.344185\tTotal Loss: 2.335435\n","---->[51200/60000 (85%)]\tPrecision: 0.109375\tLoss: 2.264243\tSurrogate Loss: 1.771177\tTotal Loss: 2.441361\n","----> Epoch 2:\n","---->[0/60000 (0%)]\tPrecision: 0.328125\tLoss: 1.952525\tSurrogate Loss: 1.356598\tTotal Loss: 2.088185\n","---->[12800/60000 (21%)]\tPrecision: 0.359375\tLoss: 1.903311\tSurrogate Loss: 1.149160\tTotal Loss: 2.018226\n","---->[25600/60000 (43%)]\tPrecision: 0.203125\tLoss: 1.929313\tSurrogate Loss: 0.629885\tTotal Loss: 1.992301\n","---->[38400/60000 (64%)]\tPrecision: 0.187500\tLoss: 2.115028\tSurrogate Loss: 1.771299\tTotal Loss: 2.292158\n","---->[51200/60000 (85%)]\tPrecision: 0.234375\tLoss: 1.986010\tSurrogate Loss: 1.423628\tTotal Loss: 2.128372\n","----> Epoch 3:\n","---->[0/60000 (0%)]\tPrecision: 0.171875\tLoss: 2.022650\tSurrogate Loss: 0.439518\tTotal Loss: 2.066602\n","---->[12800/60000 (21%)]\tPrecision: 0.250000\tLoss: 1.940654\tSurrogate Loss: 0.796066\tTotal Loss: 2.020261\n","---->[25600/60000 (43%)]\tPrecision: 0.234375\tLoss: 1.954029\tSurrogate Loss: 0.758639\tTotal Loss: 2.029893\n","---->[38400/60000 (64%)]\tPrecision: 0.187500\tLoss: 2.113413\tSurrogate Loss: 0.814454\tTotal Loss: 2.194858\n","---->[51200/60000 (85%)]\tPrecision: 0.171875\tLoss: 2.101779\tSurrogate Loss: 0.850856\tTotal Loss: 2.186865\n","----> Epoch 4:\n","---->[0/60000 (0%)]\tPrecision: 0.109375\tLoss: 2.158590\tSurrogate Loss: 1.579563\tTotal Loss: 2.316547\n","---->[12800/60000 (21%)]\tPrecision: 0.218750\tLoss: 2.086221\tSurrogate Loss: 0.824396\tTotal Loss: 2.168660\n","---->[25600/60000 (43%)]\tPrecision: 0.203125\tLoss: 1.877265\tSurrogate Loss: 0.390939\tTotal Loss: 1.916359\n","---->[38400/60000 (64%)]\tPrecision: 0.203125\tLoss: 2.032288\tSurrogate Loss: 1.097034\tTotal Loss: 2.141992\n","---->[51200/60000 (85%)]\tPrecision: 0.140625\tLoss: 2.124321\tSurrogate Loss: 0.671945\tTotal Loss: 2.191516\n","----> Epoch 5:\n","---->[0/60000 (0%)]\tPrecision: 0.140625\tLoss: 2.076960\tSurrogate Loss: 1.699289\tTotal Loss: 2.246889\n","---->[12800/60000 (21%)]\tPrecision: 0.140625\tLoss: 2.106059\tSurrogate Loss: 0.271593\tTotal Loss: 2.133218\n","---->[25600/60000 (43%)]\tPrecision: 0.218750\tLoss: 2.147262\tSurrogate Loss: 1.403540\tTotal Loss: 2.287616\n","---->[38400/60000 (64%)]\tPrecision: 0.218750\tLoss: 1.997758\tSurrogate Loss: 0.246207\tTotal Loss: 2.022379\n","---->[51200/60000 (85%)]\tPrecision: 0.281250\tLoss: 1.906076\tSurrogate Loss: 0.450981\tTotal Loss: 1.951174\n","[2.3471877574920654, 2.2915470600128174, 2.2829182147979736, 2.1212992668151855, 2.092445135116577, 2.113600730895996, 2.1244008541107178, 2.1660819053649902, 2.101016044616699, 2.2642428874969482, 1.95252525806427, 1.9033105373382568, 1.9293127059936523, 2.115027666091919, 1.9860097169876099, 2.0226504802703857, 1.940653920173645, 1.9540292024612427, 2.113412857055664, 2.101778984069824, 2.158590316772461, 2.0862207412719727, 1.8772653341293335, 2.032288074493408, 2.124321222305298, 2.076960325241089, 2.1060590744018555, 2.1472620964050293, 1.9977582693099976, 1.9060759544372559]\n","[2.3471877574920654, 2.640228271484375, 2.985506296157837, 2.3078420162200928, 2.181617259979248, 2.1797232627868652, 2.170902967453003, 7.358757972717285, 2.335434675216675, 2.4413607120513916, 2.0881850719451904, 2.018226385116577, 1.9923012256622314, 2.2921576499938965, 2.1283724308013916, 2.0666022300720215, 2.0202605724334717, 2.029893159866333, 2.1948583126068115, 2.1868646144866943, 2.316546678543091, 2.1686604022979736, 1.9163591861724854, 2.14199161529541, 2.1915156841278076, 2.246889352798462, 2.133218288421631, 2.287616014480591, 2.022378921508789, 1.951174020767212]\n","--> Finished Training Task 4. Starting Test phase:\n","---->Test set 0: Average loss: 0.0005, Accuracy: 8471/10000 (85%)\n","---->Test set 1: Average loss: 0.0021, Accuracy: 2108/10000 (21%)\n","---->Test set 2: Average loss: 0.0024, Accuracy: 1009/10000 (10%)\n","---->Test set 3: Average loss: 0.0021, Accuracy: 2114/10000 (21%)\n","---->Test set 4: Average loss: 0.0020, Accuracy: 2086/10000 (21%)\n","---->Test set 5: Average loss: 0.0023, Accuracy: 1009/10000 (10%)\n","---->Test set 6: Average loss: 0.0023, Accuracy: 1055/10000 (11%)\n","---->Test set 7: Average loss: 0.0023, Accuracy: 1018/10000 (10%)\n","---->Test set 8: Average loss: 0.0023, Accuracy: 1009/10000 (10%)\n","---->Test set 9: Average loss: 0.0023, Accuracy: 1009/10000 (10%)\n","--> Training Task 5:\n","----> Epoch 0:\n","---->[0/60000 (0%)]\tPrecision: 0.093750\tLoss: 2.336768\tSurrogate Loss: 0.000000\tTotal Loss: 2.336768\n","---->[12800/60000 (21%)]\tPrecision: 0.156250\tLoss: 2.340367\tSurrogate Loss: 0.073227\tTotal Loss: 2.347690\n","---->[25600/60000 (43%)]\tPrecision: 0.140625\tLoss: 2.391906\tSurrogate Loss: 0.036825\tTotal Loss: 2.395588\n","---->[38400/60000 (64%)]\tPrecision: 0.109375\tLoss: 2.357150\tSurrogate Loss: 0.031322\tTotal Loss: 2.360282\n","---->[51200/60000 (85%)]\tPrecision: 0.078125\tLoss: 2.357089\tSurrogate Loss: 0.035073\tTotal Loss: 2.360596\n","----> Epoch 1:\n","---->[0/60000 (0%)]\tPrecision: 0.078125\tLoss: 2.318120\tSurrogate Loss: 0.052808\tTotal Loss: 2.323401\n","---->[12800/60000 (21%)]\tPrecision: 0.125000\tLoss: 2.318272\tSurrogate Loss: 0.052803\tTotal Loss: 2.323552\n","---->[25600/60000 (43%)]\tPrecision: 0.093750\tLoss: 2.382920\tSurrogate Loss: 0.727182\tTotal Loss: 2.455638\n","---->[38400/60000 (64%)]\tPrecision: 0.093750\tLoss: 2.301911\tSurrogate Loss: 0.033226\tTotal Loss: 2.305233\n","---->[51200/60000 (85%)]\tPrecision: 0.093750\tLoss: 2.359161\tSurrogate Loss: 0.035244\tTotal Loss: 2.362685\n","----> Epoch 2:\n","---->[0/60000 (0%)]\tPrecision: 0.125000\tLoss: 2.347287\tSurrogate Loss: 0.036514\tTotal Loss: 2.350939\n","---->[12800/60000 (21%)]\tPrecision: 0.078125\tLoss: 2.330648\tSurrogate Loss: 0.042260\tTotal Loss: 2.334874\n","---->[25600/60000 (43%)]\tPrecision: 0.046875\tLoss: 2.350039\tSurrogate Loss: 0.034474\tTotal Loss: 2.353486\n","---->[38400/60000 (64%)]\tPrecision: 0.078125\tLoss: 2.321075\tSurrogate Loss: 0.038676\tTotal Loss: 2.324942\n","---->[51200/60000 (85%)]\tPrecision: 0.078125\tLoss: 2.317063\tSurrogate Loss: 0.035331\tTotal Loss: 2.320596\n","----> Epoch 3:\n","---->[0/60000 (0%)]\tPrecision: 0.062500\tLoss: 2.319803\tSurrogate Loss: 0.036114\tTotal Loss: 2.323415\n","---->[12800/60000 (21%)]\tPrecision: 0.062500\tLoss: 2.358927\tSurrogate Loss: 0.035246\tTotal Loss: 2.362452\n","---->[25600/60000 (43%)]\tPrecision: 0.156250\tLoss: 2.328473\tSurrogate Loss: 0.050422\tTotal Loss: 2.333516\n","---->[38400/60000 (64%)]\tPrecision: 0.093750\tLoss: 2.367297\tSurrogate Loss: 0.027947\tTotal Loss: 2.370091\n","---->[51200/60000 (85%)]\tPrecision: 0.078125\tLoss: 2.322561\tSurrogate Loss: 0.036660\tTotal Loss: 2.326227\n","----> Epoch 4:\n","---->[0/60000 (0%)]\tPrecision: 0.078125\tLoss: 2.360641\tSurrogate Loss: 0.038887\tTotal Loss: 2.364530\n","---->[12800/60000 (21%)]\tPrecision: 0.062500\tLoss: 2.353211\tSurrogate Loss: 0.030772\tTotal Loss: 2.356288\n","---->[25600/60000 (43%)]\tPrecision: 0.140625\tLoss: 2.352124\tSurrogate Loss: 0.038604\tTotal Loss: 2.355985\n","---->[38400/60000 (64%)]\tPrecision: 0.062500\tLoss: 2.385207\tSurrogate Loss: 0.032572\tTotal Loss: 2.388465\n","---->[51200/60000 (85%)]\tPrecision: 0.109375\tLoss: 2.287317\tSurrogate Loss: 0.037682\tTotal Loss: 2.291085\n","----> Epoch 5:\n","---->[0/60000 (0%)]\tPrecision: 0.140625\tLoss: 2.339054\tSurrogate Loss: 0.035814\tTotal Loss: 2.342635\n","---->[12800/60000 (21%)]\tPrecision: 0.062500\tLoss: 2.358178\tSurrogate Loss: 0.030295\tTotal Loss: 2.361208\n","---->[25600/60000 (43%)]\tPrecision: 0.109375\tLoss: 2.376952\tSurrogate Loss: 0.037952\tTotal Loss: 2.380747\n","---->[38400/60000 (64%)]\tPrecision: 0.093750\tLoss: 2.383927\tSurrogate Loss: 0.044798\tTotal Loss: 2.388407\n","---->[51200/60000 (85%)]\tPrecision: 0.031250\tLoss: 2.361252\tSurrogate Loss: 0.040496\tTotal Loss: 2.365301\n","[2.33676815032959, 2.340367317199707, 2.3919055461883545, 2.357149839401245, 2.357088804244995, 2.318120241165161, 2.3182716369628906, 2.3829197883605957, 2.301910638809204, 2.359160900115967, 2.347287178039551, 2.3306477069854736, 2.350038766860962, 2.3210747241973877, 2.3170626163482666, 2.319803237915039, 2.3589272499084473, 2.3284733295440674, 2.3672966957092285, 2.322561025619507, 2.3606412410736084, 2.353210926055908, 2.3521244525909424, 2.385207414627075, 2.2873167991638184, 2.3390536308288574, 2.35817813873291, 2.3769519329071045, 2.3839268684387207, 2.3612515926361084]\n","[2.33676815032959, 2.3476901054382324, 2.3955881595611572, 2.3602819442749023, 2.360596179962158, 2.3234009742736816, 2.323551893234253, 2.4556379318237305, 2.3052332401275635, 2.362685203552246, 2.350938558578491, 2.334873676300049, 2.3534860610961914, 2.3249423503875732, 2.3205957412719727, 2.3234145641326904, 2.3624517917633057, 2.3335156440734863, 2.370091438293457, 2.3262269496917725, 2.364529848098755, 2.356288194656372, 2.355984926223755, 2.3884646892547607, 2.2910850048065186, 2.342635154724121, 2.3612077236175537, 2.380747079849243, 2.388406753540039, 2.3653011322021484]\n","--> Finished Training Task 5. Starting Test phase:\n","---->Test set 0: Average loss: 0.0005, Accuracy: 8464/10000 (85%)\n","---->Test set 1: Average loss: 0.0021, Accuracy: 2108/10000 (21%)\n","---->Test set 2: Average loss: 0.0023, Accuracy: 1009/10000 (10%)\n","---->Test set 3: Average loss: 0.0021, Accuracy: 2114/10000 (21%)\n","---->Test set 4: Average loss: 0.0020, Accuracy: 2086/10000 (21%)\n","---->Test set 5: Average loss: 0.0023, Accuracy: 1009/10000 (10%)\n","---->Test set 6: Average loss: 0.0023, Accuracy: 1054/10000 (11%)\n","---->Test set 7: Average loss: 0.0023, Accuracy: 1017/10000 (10%)\n","---->Test set 8: Average loss: 0.0023, Accuracy: 1009/10000 (10%)\n","---->Test set 9: Average loss: 0.0023, Accuracy: 1009/10000 (10%)\n","--> Training Task 6:\n","----> Epoch 0:\n","---->[0/60000 (0%)]\tPrecision: 0.109375\tLoss: 2.340831\tSurrogate Loss: 0.000000\tTotal Loss: 2.340831\n","---->[12800/60000 (21%)]\tPrecision: 0.218750\tLoss: 1.965994\tSurrogate Loss: 1.510212\tTotal Loss: 2.117015\n","---->[25600/60000 (43%)]\tPrecision: 0.156250\tLoss: 2.111864\tSurrogate Loss: 1.359011\tTotal Loss: 2.247765\n","---->[38400/60000 (64%)]\tPrecision: 0.203125\tLoss: 2.016253\tSurrogate Loss: 2.055344\tTotal Loss: 2.221788\n","---->[51200/60000 (85%)]\tPrecision: 0.125000\tLoss: 2.183558\tSurrogate Loss: 1.706025\tTotal Loss: 2.354161\n","----> Epoch 1:\n","---->[0/60000 (0%)]\tPrecision: 0.156250\tLoss: 2.057648\tSurrogate Loss: 1.385304\tTotal Loss: 2.196179\n","---->[12800/60000 (21%)]\tPrecision: 0.203125\tLoss: 2.005145\tSurrogate Loss: 0.623767\tTotal Loss: 2.067522\n","---->[25600/60000 (43%)]\tPrecision: 0.171875\tLoss: 2.084170\tSurrogate Loss: 1.309657\tTotal Loss: 2.215135\n","---->[38400/60000 (64%)]\tPrecision: 0.140625\tLoss: 2.115099\tSurrogate Loss: 0.949281\tTotal Loss: 2.210027\n","---->[51200/60000 (85%)]\tPrecision: 0.281250\tLoss: 1.992873\tSurrogate Loss: 1.550868\tTotal Loss: 2.147959\n","----> Epoch 2:\n","---->[0/60000 (0%)]\tPrecision: 0.171875\tLoss: 2.036419\tSurrogate Loss: 0.313614\tTotal Loss: 2.067780\n","---->[12800/60000 (21%)]\tPrecision: 0.187500\tLoss: 2.072388\tSurrogate Loss: 0.625355\tTotal Loss: 2.134924\n","---->[25600/60000 (43%)]\tPrecision: 0.156250\tLoss: 2.075649\tSurrogate Loss: 0.761676\tTotal Loss: 2.151817\n","---->[38400/60000 (64%)]\tPrecision: 0.265625\tLoss: 1.994807\tSurrogate Loss: 0.476632\tTotal Loss: 2.042470\n","---->[51200/60000 (85%)]\tPrecision: 0.218750\tLoss: 2.029236\tSurrogate Loss: 1.411368\tTotal Loss: 2.170373\n","----> Epoch 3:\n","---->[0/60000 (0%)]\tPrecision: 0.187500\tLoss: 2.038664\tSurrogate Loss: 0.474857\tTotal Loss: 2.086149\n","---->[12800/60000 (21%)]\tPrecision: 0.187500\tLoss: 2.071980\tSurrogate Loss: 1.393564\tTotal Loss: 2.211337\n","---->[25600/60000 (43%)]\tPrecision: 0.218750\tLoss: 2.004490\tSurrogate Loss: 0.288185\tTotal Loss: 2.033309\n","---->[38400/60000 (64%)]\tPrecision: 0.203125\tLoss: 1.928627\tSurrogate Loss: 0.100391\tTotal Loss: 1.938666\n","---->[51200/60000 (85%)]\tPrecision: 0.140625\tLoss: 2.034230\tSurrogate Loss: 1.060966\tTotal Loss: 2.140326\n","----> Epoch 4:\n","---->[0/60000 (0%)]\tPrecision: 0.187500\tLoss: 2.146578\tSurrogate Loss: 1.579746\tTotal Loss: 2.304552\n","---->[12800/60000 (21%)]\tPrecision: 0.203125\tLoss: 1.974941\tSurrogate Loss: 0.154810\tTotal Loss: 1.990422\n","---->[25600/60000 (43%)]\tPrecision: 0.156250\tLoss: 2.221049\tSurrogate Loss: 1.849512\tTotal Loss: 2.406000\n","---->[38400/60000 (64%)]\tPrecision: 0.156250\tLoss: 2.051356\tSurrogate Loss: 0.551860\tTotal Loss: 2.106542\n","---->[51200/60000 (85%)]\tPrecision: 0.250000\tLoss: 1.979710\tSurrogate Loss: 0.849118\tTotal Loss: 2.064622\n","----> Epoch 5:\n","---->[0/60000 (0%)]\tPrecision: 0.218750\tLoss: 2.058941\tSurrogate Loss: 0.769871\tTotal Loss: 2.135928\n","---->[12800/60000 (21%)]\tPrecision: 0.156250\tLoss: 2.029578\tSurrogate Loss: 0.450284\tTotal Loss: 2.074607\n","---->[25600/60000 (43%)]\tPrecision: 0.156250\tLoss: 2.037411\tSurrogate Loss: 1.361255\tTotal Loss: 2.173537\n","---->[38400/60000 (64%)]\tPrecision: 0.234375\tLoss: 1.868153\tSurrogate Loss: 0.300122\tTotal Loss: 1.898165\n","---->[51200/60000 (85%)]\tPrecision: 0.203125\tLoss: 2.185548\tSurrogate Loss: 0.628876\tTotal Loss: 2.248436\n","[2.3408305644989014, 1.9659940004348755, 2.111863613128662, 2.0162532329559326, 2.183558225631714, 2.0576484203338623, 2.0051448345184326, 2.084169626235962, 2.115098714828491, 1.992872714996338, 2.0364186763763428, 2.072388172149658, 2.0756490230560303, 1.9948067665100098, 2.029236078262329, 2.038663625717163, 2.0719802379608154, 2.0044898986816406, 1.9286268949508667, 2.0342295169830322, 2.146577835083008, 1.9749407768249512, 2.221048593521118, 2.051356315612793, 1.9797098636627197, 2.058941125869751, 2.02957820892334, 2.0374112129211426, 1.8681529760360718, 2.1855480670928955]\n","[2.3408305644989014, 2.1170151233673096, 2.2477645874023438, 2.221787691116333, 2.354160785675049, 2.196178913116455, 2.067521572113037, 2.215135335922241, 2.210026741027832, 2.1479594707489014, 2.067780017852783, 2.1349236965179443, 2.1518166065216064, 2.0424699783325195, 2.17037296295166, 2.086149215698242, 2.211336612701416, 2.033308506011963, 1.9386659860610962, 2.1403262615203857, 2.3045523166656494, 1.9904217720031738, 2.4059996604919434, 2.1065423488616943, 2.064621686935425, 2.135928153991699, 2.0746066570281982, 2.173536777496338, 1.8981651067733765, 2.2484357357025146]\n","--> Finished Training Task 6. Starting Test phase:\n","---->Test set 0: Average loss: 0.0005, Accuracy: 8463/10000 (85%)\n","---->Test set 1: Average loss: 0.0021, Accuracy: 2107/10000 (21%)\n","---->Test set 2: Average loss: 0.0023, Accuracy: 1009/10000 (10%)\n","---->Test set 3: Average loss: 0.0021, Accuracy: 2115/10000 (21%)\n","---->Test set 4: Average loss: 0.0020, Accuracy: 2089/10000 (21%)\n","---->Test set 5: Average loss: 0.0023, Accuracy: 1009/10000 (10%)\n","---->Test set 6: Average loss: 0.0020, Accuracy: 2081/10000 (21%)\n","---->Test set 7: Average loss: 0.0023, Accuracy: 1073/10000 (11%)\n","---->Test set 8: Average loss: 0.0023, Accuracy: 1009/10000 (10%)\n","---->Test set 9: Average loss: 0.0023, Accuracy: 1009/10000 (10%)\n","--> Training Task 7:\n","----> Epoch 0:\n","---->[0/60000 (0%)]\tPrecision: 0.046875\tLoss: 2.276574\tSurrogate Loss: 0.000000\tTotal Loss: 2.276574\n","---->[12800/60000 (21%)]\tPrecision: 0.125000\tLoss: 2.126016\tSurrogate Loss: 4.833845\tTotal Loss: 2.609400\n","---->[25600/60000 (43%)]\tPrecision: 0.218750\tLoss: 1.962858\tSurrogate Loss: 1.657981\tTotal Loss: 2.128656\n","---->[38400/60000 (64%)]\tPrecision: 0.187500\tLoss: 2.035619\tSurrogate Loss: 0.661268\tTotal Loss: 2.101746\n","---->[51200/60000 (85%)]\tPrecision: 0.171875\tLoss: 2.089891\tSurrogate Loss: 0.805363\tTotal Loss: 2.170427\n","----> Epoch 1:\n","---->[0/60000 (0%)]\tPrecision: 0.125000\tLoss: 2.181695\tSurrogate Loss: 1.912016\tTotal Loss: 2.372896\n","---->[12800/60000 (21%)]\tPrecision: 0.156250\tLoss: 2.003374\tSurrogate Loss: 1.684766\tTotal Loss: 2.171850\n","---->[25600/60000 (43%)]\tPrecision: 0.218750\tLoss: 2.193510\tSurrogate Loss: 1.897335\tTotal Loss: 2.383243\n","---->[38400/60000 (64%)]\tPrecision: 0.171875\tLoss: 2.087907\tSurrogate Loss: 0.748098\tTotal Loss: 2.162717\n","---->[51200/60000 (85%)]\tPrecision: 0.343750\tLoss: 1.895919\tSurrogate Loss: 0.629866\tTotal Loss: 1.958906\n","----> Epoch 2:\n","---->[0/60000 (0%)]\tPrecision: 0.156250\tLoss: 2.157351\tSurrogate Loss: 1.480293\tTotal Loss: 2.305380\n","---->[12800/60000 (21%)]\tPrecision: 0.171875\tLoss: 2.026661\tSurrogate Loss: 1.396995\tTotal Loss: 2.166361\n","---->[25600/60000 (43%)]\tPrecision: 0.171875\tLoss: 2.129691\tSurrogate Loss: 0.650417\tTotal Loss: 2.194733\n","---->[38400/60000 (64%)]\tPrecision: 0.187500\tLoss: 2.046131\tSurrogate Loss: 0.825986\tTotal Loss: 2.128730\n","---->[51200/60000 (85%)]\tPrecision: 0.250000\tLoss: 1.974184\tSurrogate Loss: 1.035797\tTotal Loss: 2.077763\n","----> Epoch 3:\n","---->[0/60000 (0%)]\tPrecision: 0.265625\tLoss: 1.989755\tSurrogate Loss: 0.454687\tTotal Loss: 2.035224\n","---->[12800/60000 (21%)]\tPrecision: 0.218750\tLoss: 2.044563\tSurrogate Loss: 0.412329\tTotal Loss: 2.085796\n","---->[25600/60000 (43%)]\tPrecision: 0.234375\tLoss: 2.067386\tSurrogate Loss: 1.101730\tTotal Loss: 2.177559\n","---->[38400/60000 (64%)]\tPrecision: 0.187500\tLoss: 2.114289\tSurrogate Loss: 0.412084\tTotal Loss: 2.155497\n","---->[51200/60000 (85%)]\tPrecision: 0.203125\tLoss: 1.924009\tSurrogate Loss: 0.159618\tTotal Loss: 1.939971\n","----> Epoch 4:\n","---->[0/60000 (0%)]\tPrecision: 0.203125\tLoss: 2.029901\tSurrogate Loss: 0.259034\tTotal Loss: 2.055804\n","---->[12800/60000 (21%)]\tPrecision: 0.156250\tLoss: 2.048958\tSurrogate Loss: 1.298747\tTotal Loss: 2.178832\n","---->[25600/60000 (43%)]\tPrecision: 0.140625\tLoss: 2.135223\tSurrogate Loss: 0.638938\tTotal Loss: 2.199117\n","---->[38400/60000 (64%)]\tPrecision: 0.171875\tLoss: 2.056748\tSurrogate Loss: 0.835543\tTotal Loss: 2.140302\n","---->[51200/60000 (85%)]\tPrecision: 0.203125\tLoss: 2.042195\tSurrogate Loss: 0.793346\tTotal Loss: 2.121529\n","----> Epoch 5:\n","---->[0/60000 (0%)]\tPrecision: 0.187500\tLoss: 1.974365\tSurrogate Loss: 0.226417\tTotal Loss: 1.997007\n","---->[12800/60000 (21%)]\tPrecision: 0.296875\tLoss: 1.815506\tSurrogate Loss: 0.242866\tTotal Loss: 1.839792\n","---->[25600/60000 (43%)]\tPrecision: 0.265625\tLoss: 1.891499\tSurrogate Loss: 0.131045\tTotal Loss: 1.904603\n","---->[38400/60000 (64%)]\tPrecision: 0.265625\tLoss: 1.966366\tSurrogate Loss: 1.372532\tTotal Loss: 2.103619\n","---->[51200/60000 (85%)]\tPrecision: 0.187500\tLoss: 2.074869\tSurrogate Loss: 0.791649\tTotal Loss: 2.154034\n","[2.2765743732452393, 2.1260156631469727, 1.962857961654663, 2.035619020462036, 2.089890956878662, 2.181694746017456, 2.003373622894287, 2.193509817123413, 2.087906837463379, 1.8959189653396606, 2.157351016998291, 2.0266611576080322, 2.1296908855438232, 2.046131134033203, 1.9741836786270142, 1.989755392074585, 2.044563055038452, 2.067385673522949, 2.114288568496704, 1.9240093231201172, 2.0299010276794434, 2.048957586288452, 2.135223388671875, 2.056748151779175, 2.0421946048736572, 1.9743648767471313, 1.815505862236023, 1.8914988040924072, 1.966365933418274, 2.074869155883789]\n","[2.2765743732452393, 2.6094000339508057, 2.1286561489105225, 2.101745843887329, 2.1704273223876953, 2.372896432876587, 2.1718502044677734, 2.3832433223724365, 2.1627166271209717, 1.958905577659607, 2.305380344390869, 2.16636061668396, 2.194732666015625, 2.128729820251465, 2.077763319015503, 2.035223960876465, 2.0857958793640137, 2.177558660507202, 2.155496835708618, 1.9399710893630981, 2.055804491043091, 2.1788322925567627, 2.1991171836853027, 2.1403024196624756, 2.1215291023254395, 1.9970065355300903, 1.8397924900054932, 1.904603362083435, 2.10361909866333, 2.15403413772583]\n","--> Finished Training Task 7. Starting Test phase:\n","---->Test set 0: Average loss: 0.0005, Accuracy: 8483/10000 (85%)\n","---->Test set 1: Average loss: 0.0021, Accuracy: 2100/10000 (21%)\n","---->Test set 2: Average loss: 0.0023, Accuracy: 1009/10000 (10%)\n","---->Test set 3: Average loss: 0.0021, Accuracy: 2110/10000 (21%)\n","---->Test set 4: Average loss: 0.0020, Accuracy: 2078/10000 (21%)\n","---->Test set 5: Average loss: 0.0023, Accuracy: 1009/10000 (10%)\n","---->Test set 6: Average loss: 0.0020, Accuracy: 2065/10000 (21%)\n","---->Test set 7: Average loss: 0.0020, Accuracy: 1986/10000 (20%)\n","---->Test set 8: Average loss: 0.0023, Accuracy: 1009/10000 (10%)\n","---->Test set 9: Average loss: 0.0023, Accuracy: 1009/10000 (10%)\n","--> Training Task 8:\n","----> Epoch 0:\n","---->[0/60000 (0%)]\tPrecision: 0.171875\tLoss: 2.323664\tSurrogate Loss: 0.000000\tTotal Loss: 2.323664\n","---->[12800/60000 (21%)]\tPrecision: 0.125000\tLoss: 2.335417\tSurrogate Loss: 0.015310\tTotal Loss: 2.336948\n","---->[25600/60000 (43%)]\tPrecision: 0.093750\tLoss: 2.322430\tSurrogate Loss: 0.013299\tTotal Loss: 2.323760\n","---->[38400/60000 (64%)]\tPrecision: 0.187500\tLoss: 2.379685\tSurrogate Loss: 0.012429\tTotal Loss: 2.380928\n","---->[51200/60000 (85%)]\tPrecision: 0.046875\tLoss: 2.287879\tSurrogate Loss: 0.009869\tTotal Loss: 2.288865\n","----> Epoch 1:\n","---->[0/60000 (0%)]\tPrecision: 0.046875\tLoss: 2.285784\tSurrogate Loss: 0.009047\tTotal Loss: 2.286688\n","---->[12800/60000 (21%)]\tPrecision: 0.078125\tLoss: 2.357178\tSurrogate Loss: 0.011819\tTotal Loss: 2.358360\n","---->[25600/60000 (43%)]\tPrecision: 0.125000\tLoss: 2.293510\tSurrogate Loss: 0.017268\tTotal Loss: 2.295237\n","---->[38400/60000 (64%)]\tPrecision: 0.046875\tLoss: 2.387942\tSurrogate Loss: 0.013271\tTotal Loss: 2.389269\n","---->[51200/60000 (85%)]\tPrecision: 0.109375\tLoss: 2.370899\tSurrogate Loss: 0.014303\tTotal Loss: 2.372330\n","----> Epoch 2:\n","---->[0/60000 (0%)]\tPrecision: 0.203125\tLoss: 2.314935\tSurrogate Loss: 0.014024\tTotal Loss: 2.316337\n","---->[12800/60000 (21%)]\tPrecision: 0.078125\tLoss: 2.315675\tSurrogate Loss: 0.009595\tTotal Loss: 2.316635\n","---->[25600/60000 (43%)]\tPrecision: 0.109375\tLoss: 2.348907\tSurrogate Loss: 0.015934\tTotal Loss: 2.350500\n","---->[38400/60000 (64%)]\tPrecision: 0.109375\tLoss: 2.318683\tSurrogate Loss: 0.012809\tTotal Loss: 2.319963\n","---->[51200/60000 (85%)]\tPrecision: 0.093750\tLoss: 2.324708\tSurrogate Loss: 0.012430\tTotal Loss: 2.325951\n","----> Epoch 3:\n","---->[0/60000 (0%)]\tPrecision: 0.156250\tLoss: 2.333048\tSurrogate Loss: 0.009640\tTotal Loss: 2.334012\n","---->[12800/60000 (21%)]\tPrecision: 0.062500\tLoss: 2.339555\tSurrogate Loss: 0.013123\tTotal Loss: 2.340867\n","---->[25600/60000 (43%)]\tPrecision: 0.046875\tLoss: 2.356282\tSurrogate Loss: 0.012908\tTotal Loss: 2.357572\n","---->[38400/60000 (64%)]\tPrecision: 0.093750\tLoss: 2.319290\tSurrogate Loss: 0.010061\tTotal Loss: 2.320296\n","---->[51200/60000 (85%)]\tPrecision: 0.125000\tLoss: 2.359373\tSurrogate Loss: 0.010289\tTotal Loss: 2.360402\n","----> Epoch 4:\n","---->[0/60000 (0%)]\tPrecision: 0.109375\tLoss: 2.300636\tSurrogate Loss: 0.012535\tTotal Loss: 2.301889\n","---->[12800/60000 (21%)]\tPrecision: 0.093750\tLoss: 2.343107\tSurrogate Loss: 0.022810\tTotal Loss: 2.345388\n","---->[25600/60000 (43%)]\tPrecision: 0.109375\tLoss: 2.361095\tSurrogate Loss: 0.016075\tTotal Loss: 2.362702\n","---->[38400/60000 (64%)]\tPrecision: 0.109375\tLoss: 2.357555\tSurrogate Loss: 0.011820\tTotal Loss: 2.358737\n","---->[51200/60000 (85%)]\tPrecision: 0.093750\tLoss: 2.338916\tSurrogate Loss: 0.009048\tTotal Loss: 2.339820\n","----> Epoch 5:\n","---->[0/60000 (0%)]\tPrecision: 0.171875\tLoss: 2.308449\tSurrogate Loss: 0.011895\tTotal Loss: 2.309639\n","---->[12800/60000 (21%)]\tPrecision: 0.187500\tLoss: 2.349058\tSurrogate Loss: 0.009485\tTotal Loss: 2.350007\n","---->[25600/60000 (43%)]\tPrecision: 0.109375\tLoss: 2.363788\tSurrogate Loss: 0.013647\tTotal Loss: 2.365153\n","---->[38400/60000 (64%)]\tPrecision: 0.078125\tLoss: 2.341227\tSurrogate Loss: 0.013304\tTotal Loss: 2.342558\n","---->[51200/60000 (85%)]\tPrecision: 0.093750\tLoss: 2.293986\tSurrogate Loss: 0.008273\tTotal Loss: 2.294813\n","[2.3236641883850098, 2.3354172706604004, 2.32243013381958, 2.3796849250793457, 2.2878785133361816, 2.285783529281616, 2.3571784496307373, 2.2935104370117188, 2.387941837310791, 2.3708994388580322, 2.314934730529785, 2.3156752586364746, 2.3489065170288086, 2.3186826705932617, 2.3247079849243164, 2.33304762840271, 2.33955454826355, 2.3562815189361572, 2.3192896842956543, 2.359373092651367, 2.300635814666748, 2.3431074619293213, 2.3610949516296387, 2.3575546741485596, 2.3389155864715576, 2.3084490299224854, 2.3490583896636963, 2.36378812789917, 2.3412272930145264, 2.2939858436584473]\n","[2.3236641883850098, 2.3369481563568115, 2.3237600326538086, 2.380927801132202, 2.288865327835083, 2.2866883277893066, 2.3583602905273438, 2.2952373027801514, 2.3892688751220703, 2.3723297119140625, 2.3163371086120605, 2.3166348934173584, 2.3504998683929443, 2.3199634552001953, 2.325950860977173, 2.3340115547180176, 2.340866804122925, 2.357572317123413, 2.320295810699463, 2.3604018688201904, 2.301889181137085, 2.345388412475586, 2.3627023696899414, 2.358736753463745, 2.339820384979248, 2.309638500213623, 2.3500068187713623, 2.3651528358459473, 2.342557668685913, 2.2948131561279297]\n","--> Finished Training Task 8. Starting Test phase:\n","---->Test set 0: Average loss: 0.0005, Accuracy: 8483/10000 (85%)\n","---->Test set 1: Average loss: 0.0021, Accuracy: 2100/10000 (21%)\n","---->Test set 2: Average loss: 0.0023, Accuracy: 1009/10000 (10%)\n","---->Test set 3: Average loss: 0.0021, Accuracy: 2110/10000 (21%)\n","---->Test set 4: Average loss: 0.0020, Accuracy: 2078/10000 (21%)\n","---->Test set 5: Average loss: 0.0023, Accuracy: 1009/10000 (10%)\n","---->Test set 6: Average loss: 0.0020, Accuracy: 2065/10000 (21%)\n","---->Test set 7: Average loss: 0.0020, Accuracy: 1986/10000 (20%)\n","---->Test set 8: Average loss: 0.0023, Accuracy: 1009/10000 (10%)\n","---->Test set 9: Average loss: 0.0023, Accuracy: 1009/10000 (10%)\n","--> Training Task 9:\n","----> Epoch 0:\n","---->[0/60000 (0%)]\tPrecision: 0.140625\tLoss: 2.295270\tSurrogate Loss: 0.000000\tTotal Loss: 2.295270\n","---->[12800/60000 (21%)]\tPrecision: 0.078125\tLoss: 2.338607\tSurrogate Loss: 0.009241\tTotal Loss: 2.339531\n","---->[25600/60000 (43%)]\tPrecision: 0.109375\tLoss: 2.352294\tSurrogate Loss: 0.005844\tTotal Loss: 2.352879\n","---->[38400/60000 (64%)]\tPrecision: 0.125000\tLoss: 2.367233\tSurrogate Loss: 0.009464\tTotal Loss: 2.368180\n","---->[51200/60000 (85%)]\tPrecision: 0.093750\tLoss: 2.354322\tSurrogate Loss: 0.006943\tTotal Loss: 2.355016\n","----> Epoch 1:\n","---->[0/60000 (0%)]\tPrecision: 0.187500\tLoss: 2.319632\tSurrogate Loss: 0.008408\tTotal Loss: 2.320472\n","---->[12800/60000 (21%)]\tPrecision: 0.093750\tLoss: 2.399601\tSurrogate Loss: 0.016464\tTotal Loss: 2.401248\n","---->[25600/60000 (43%)]\tPrecision: 0.140625\tLoss: 2.310947\tSurrogate Loss: 0.011738\tTotal Loss: 2.312120\n","---->[38400/60000 (64%)]\tPrecision: 0.093750\tLoss: 2.345868\tSurrogate Loss: 0.007657\tTotal Loss: 2.346634\n","---->[51200/60000 (85%)]\tPrecision: 0.078125\tLoss: 2.371976\tSurrogate Loss: 0.007709\tTotal Loss: 2.372747\n","----> Epoch 2:\n","---->[0/60000 (0%)]\tPrecision: 0.031250\tLoss: 2.425846\tSurrogate Loss: 0.007055\tTotal Loss: 2.426552\n","---->[12800/60000 (21%)]\tPrecision: 0.109375\tLoss: 2.350513\tSurrogate Loss: 0.007511\tTotal Loss: 2.351264\n","---->[25600/60000 (43%)]\tPrecision: 0.125000\tLoss: 2.395676\tSurrogate Loss: 0.005759\tTotal Loss: 2.396252\n","---->[38400/60000 (64%)]\tPrecision: 0.125000\tLoss: 2.317371\tSurrogate Loss: 0.005544\tTotal Loss: 2.317925\n","---->[51200/60000 (85%)]\tPrecision: 0.046875\tLoss: 2.376501\tSurrogate Loss: 0.010544\tTotal Loss: 2.377555\n","----> Epoch 3:\n","---->[0/60000 (0%)]\tPrecision: 0.046875\tLoss: 2.342957\tSurrogate Loss: 0.009260\tTotal Loss: 2.343883\n","---->[12800/60000 (21%)]\tPrecision: 0.062500\tLoss: 2.307829\tSurrogate Loss: 0.016608\tTotal Loss: 2.309489\n","---->[25600/60000 (43%)]\tPrecision: 0.062500\tLoss: 2.368747\tSurrogate Loss: 0.011223\tTotal Loss: 2.369869\n","---->[38400/60000 (64%)]\tPrecision: 0.078125\tLoss: 2.312014\tSurrogate Loss: 0.010156\tTotal Loss: 2.313030\n","---->[51200/60000 (85%)]\tPrecision: 0.093750\tLoss: 2.341853\tSurrogate Loss: 0.009357\tTotal Loss: 2.342789\n","----> Epoch 4:\n","---->[0/60000 (0%)]\tPrecision: 0.078125\tLoss: 2.315790\tSurrogate Loss: 0.014324\tTotal Loss: 2.317222\n","---->[12800/60000 (21%)]\tPrecision: 0.078125\tLoss: 2.320757\tSurrogate Loss: 0.012894\tTotal Loss: 2.322047\n","---->[25600/60000 (43%)]\tPrecision: 0.015625\tLoss: 2.433782\tSurrogate Loss: 0.013912\tTotal Loss: 2.435173\n","---->[38400/60000 (64%)]\tPrecision: 0.078125\tLoss: 2.332956\tSurrogate Loss: 0.007670\tTotal Loss: 2.333723\n","---->[51200/60000 (85%)]\tPrecision: 0.093750\tLoss: 2.312382\tSurrogate Loss: 0.007062\tTotal Loss: 2.313089\n","----> Epoch 5:\n","---->[0/60000 (0%)]\tPrecision: 0.109375\tLoss: 2.322038\tSurrogate Loss: 0.010510\tTotal Loss: 2.323089\n","---->[12800/60000 (21%)]\tPrecision: 0.062500\tLoss: 2.344457\tSurrogate Loss: 0.006717\tTotal Loss: 2.345129\n","---->[25600/60000 (43%)]\tPrecision: 0.093750\tLoss: 2.359098\tSurrogate Loss: 0.010344\tTotal Loss: 2.360132\n","---->[38400/60000 (64%)]\tPrecision: 0.078125\tLoss: 2.336789\tSurrogate Loss: 0.006983\tTotal Loss: 2.337487\n","---->[51200/60000 (85%)]\tPrecision: 0.140625\tLoss: 2.285937\tSurrogate Loss: 0.005060\tTotal Loss: 2.286443\n","[2.2952699661254883, 2.3386070728302, 2.3522942066192627, 2.3672330379486084, 2.3543217182159424, 2.319631576538086, 2.399601459503174, 2.3109467029571533, 2.3458681106567383, 2.371976375579834, 2.4258460998535156, 2.350512981414795, 2.3956758975982666, 2.317370891571045, 2.3765008449554443, 2.34295654296875, 2.307828664779663, 2.368746519088745, 2.312014102935791, 2.341853141784668, 2.3157899379730225, 2.3207573890686035, 2.433781862258911, 2.332955837249756, 2.312382459640503, 2.322038412094116, 2.344456911087036, 2.359097719192505, 2.3367888927459717, 2.2859368324279785]\n","[2.2952699661254883, 2.339531183242798, 2.3528785705566406, 2.3681795597076416, 2.355015993118286, 2.320472478866577, 2.401247978210449, 2.3121204376220703, 2.3466339111328125, 2.3727474212646484, 2.426551580429077, 2.351263999938965, 2.396251916885376, 2.317925453186035, 2.3775551319122314, 2.3438825607299805, 2.3094894886016846, 2.369868755340576, 2.3130297660827637, 2.3427889347076416, 2.3172223567962646, 2.3220467567443848, 2.4351730346679688, 2.3337228298187256, 2.3130886554718018, 2.323089361190796, 2.3451285362243652, 2.3601322174072266, 2.33748722076416, 2.286442756652832]\n","--> Finished Training Task 9. Starting Test phase:\n","---->Test set 0: Average loss: 0.0005, Accuracy: 8481/10000 (85%)\n","---->Test set 1: Average loss: 0.0021, Accuracy: 2100/10000 (21%)\n","---->Test set 2: Average loss: 0.0023, Accuracy: 1009/10000 (10%)\n","---->Test set 3: Average loss: 0.0021, Accuracy: 2110/10000 (21%)\n","---->Test set 4: Average loss: 0.0020, Accuracy: 2078/10000 (21%)\n","---->Test set 5: Average loss: 0.0023, Accuracy: 1009/10000 (10%)\n","---->Test set 6: Average loss: 0.0020, Accuracy: 2066/10000 (21%)\n","---->Test set 7: Average loss: 0.0020, Accuracy: 1986/10000 (20%)\n","---->Test set 8: Average loss: 0.0023, Accuracy: 1009/10000 (10%)\n","---->Test set 9: Average loss: 0.0023, Accuracy: 1009/10000 (10%)\n","Accuracy: 0.22857000000000002\n","Confusion matrix:\n","0.1208,0.1215,0.0953,0.1154,0.1035,0.1098,0.1053,0.0724,0.078,0.1012\n","0.9588,0.1065,0.1386,0.1033,0.0924,0.1022,0.1324,0.1194,0.0941,0.1436\n","0.9356,0.209,0.1008,0.1011,0.1018,0.1011,0.1008,0.1015,0.1008,0.1009\n","0.9356,0.2091,0.1009,0.1012,0.1013,0.1009,0.1008,0.101,0.1009,0.1009\n","0.8497,0.2101,0.1009,0.211,0.1012,0.1009,0.1024,0.101,0.1009,0.1009\n","0.8471,0.2108,0.1009,0.2114,0.2086,0.1009,0.1055,0.1018,0.1009,0.1009\n","0.8464,0.2108,0.1009,0.2114,0.2086,0.1009,0.1054,0.1017,0.1009,0.1009\n","0.8463,0.2107,0.1009,0.2115,0.2089,0.1009,0.2081,0.1073,0.1009,0.1009\n","0.8483,0.21,0.1009,0.211,0.2078,0.1009,0.2065,0.1986,0.1009,0.1009\n","0.8483,0.21,0.1009,0.211,0.2078,0.1009,0.2065,0.1986,0.1009,0.1009\n","0.8481,0.21,0.1009,0.211,0.2078,0.1009,0.2066,0.1986,0.1009,0.1009\n","--> Training:\n","---->Test set 0: Average loss: 0.2008, Accuracy: 882/10000 (9%)\n","---->Test set 1: Average loss: 0.1556, Accuracy: 1071/10000 (11%)\n","---->Test set 2: Average loss: 0.1924, Accuracy: 915/10000 (9%)\n","---->Test set 3: Average loss: 0.2027, Accuracy: 1180/10000 (12%)\n","---->Test set 4: Average loss: 0.1957, Accuracy: 1085/10000 (11%)\n","---->Test set 5: Average loss: 0.1817, Accuracy: 606/10000 (6%)\n","---->Test set 6: Average loss: 0.1752, Accuracy: 731/10000 (7%)\n","---->Test set 7: Average loss: 0.1725, Accuracy: 982/10000 (10%)\n","---->Test set 8: Average loss: 0.1655, Accuracy: 730/10000 (7%)\n","---->Test set 9: Average loss: 0.1385, Accuracy: 1115/10000 (11%)\n","--> Training Task 0:\n","----> Epoch 0:\n","---->[0/60000 (0%)]\tPrecision: 0.109375\tLoss: 193.346573\tSurrogate Loss: 0.000000\tTotal Loss: 193.346573\n","---->[12800/60000 (21%)]\tPrecision: 0.890625\tLoss: 0.299636\tSurrogate Loss: 0.000000\tTotal Loss: 0.299636\n","---->[25600/60000 (43%)]\tPrecision: 0.875000\tLoss: 0.242517\tSurrogate Loss: 0.000000\tTotal Loss: 0.242517\n","---->[38400/60000 (64%)]\tPrecision: 0.984375\tLoss: 0.100190\tSurrogate Loss: 0.000000\tTotal Loss: 0.100190\n","---->[51200/60000 (85%)]\tPrecision: 0.921875\tLoss: 0.483686\tSurrogate Loss: 0.000000\tTotal Loss: 0.483686\n","----> Epoch 1:\n","---->[0/60000 (0%)]\tPrecision: 0.984375\tLoss: 0.061490\tSurrogate Loss: 0.000000\tTotal Loss: 0.061490\n","---->[12800/60000 (21%)]\tPrecision: 0.968750\tLoss: 0.134789\tSurrogate Loss: 0.000000\tTotal Loss: 0.134789\n","---->[25600/60000 (43%)]\tPrecision: 0.968750\tLoss: 0.077600\tSurrogate Loss: 0.000000\tTotal Loss: 0.077600\n","---->[38400/60000 (64%)]\tPrecision: 0.921875\tLoss: 0.172475\tSurrogate Loss: 0.000000\tTotal Loss: 0.172475\n","---->[51200/60000 (85%)]\tPrecision: 0.953125\tLoss: 0.247613\tSurrogate Loss: 0.000000\tTotal Loss: 0.247613\n","----> Epoch 2:\n","---->[0/60000 (0%)]\tPrecision: 0.921875\tLoss: 0.275325\tSurrogate Loss: 0.000000\tTotal Loss: 0.275325\n","---->[12800/60000 (21%)]\tPrecision: 0.953125\tLoss: 0.133304\tSurrogate Loss: 0.000000\tTotal Loss: 0.133304\n","---->[25600/60000 (43%)]\tPrecision: 0.968750\tLoss: 0.067511\tSurrogate Loss: 0.000000\tTotal Loss: 0.067511\n","---->[38400/60000 (64%)]\tPrecision: 0.859375\tLoss: 0.618526\tSurrogate Loss: 0.000000\tTotal Loss: 0.618526\n","---->[51200/60000 (85%)]\tPrecision: 0.937500\tLoss: 0.146663\tSurrogate Loss: 0.000000\tTotal Loss: 0.146663\n","----> Epoch 3:\n","---->[0/60000 (0%)]\tPrecision: 0.984375\tLoss: 0.028864\tSurrogate Loss: 0.000000\tTotal Loss: 0.028864\n","---->[12800/60000 (21%)]\tPrecision: 0.937500\tLoss: 0.131231\tSurrogate Loss: 0.000000\tTotal Loss: 0.131231\n","---->[25600/60000 (43%)]\tPrecision: 0.968750\tLoss: 0.109538\tSurrogate Loss: 0.000000\tTotal Loss: 0.109538\n","---->[38400/60000 (64%)]\tPrecision: 0.953125\tLoss: 0.236338\tSurrogate Loss: 0.000000\tTotal Loss: 0.236338\n","---->[51200/60000 (85%)]\tPrecision: 0.984375\tLoss: 0.072642\tSurrogate Loss: 0.000000\tTotal Loss: 0.072642\n","----> Epoch 4:\n","---->[0/60000 (0%)]\tPrecision: 0.968750\tLoss: 0.099678\tSurrogate Loss: 0.000000\tTotal Loss: 0.099678\n","---->[12800/60000 (21%)]\tPrecision: 0.968750\tLoss: 0.194787\tSurrogate Loss: 0.000000\tTotal Loss: 0.194787\n","---->[25600/60000 (43%)]\tPrecision: 0.968750\tLoss: 0.250708\tSurrogate Loss: 0.000000\tTotal Loss: 0.250708\n","---->[38400/60000 (64%)]\tPrecision: 0.968750\tLoss: 0.131871\tSurrogate Loss: 0.000000\tTotal Loss: 0.131871\n","---->[51200/60000 (85%)]\tPrecision: 0.984375\tLoss: 0.059941\tSurrogate Loss: 0.000000\tTotal Loss: 0.059941\n","----> Epoch 5:\n","---->[0/60000 (0%)]\tPrecision: 0.937500\tLoss: 0.322825\tSurrogate Loss: 0.000000\tTotal Loss: 0.322825\n","---->[12800/60000 (21%)]\tPrecision: 0.984375\tLoss: 0.075832\tSurrogate Loss: 0.000000\tTotal Loss: 0.075832\n","---->[25600/60000 (43%)]\tPrecision: 0.906250\tLoss: 0.232845\tSurrogate Loss: 0.000000\tTotal Loss: 0.232845\n","---->[38400/60000 (64%)]\tPrecision: 0.937500\tLoss: 0.213708\tSurrogate Loss: 0.000000\tTotal Loss: 0.213708\n","---->[51200/60000 (85%)]\tPrecision: 0.906250\tLoss: 0.282189\tSurrogate Loss: 0.000000\tTotal Loss: 0.282189\n","----> Epoch 6:\n","---->[0/60000 (0%)]\tPrecision: 0.984375\tLoss: 0.071892\tSurrogate Loss: 0.000000\tTotal Loss: 0.071892\n","---->[12800/60000 (21%)]\tPrecision: 0.953125\tLoss: 0.456712\tSurrogate Loss: 0.000000\tTotal Loss: 0.456712\n","---->[25600/60000 (43%)]\tPrecision: 0.906250\tLoss: 0.243487\tSurrogate Loss: 0.000000\tTotal Loss: 0.243487\n","---->[38400/60000 (64%)]\tPrecision: 0.984375\tLoss: 0.070552\tSurrogate Loss: 0.000000\tTotal Loss: 0.070552\n","---->[51200/60000 (85%)]\tPrecision: 0.953125\tLoss: 0.091659\tSurrogate Loss: 0.000000\tTotal Loss: 0.091659\n","[193.34657287597656, 0.29963579773902893, 0.2425174117088318, 0.10019049793481827, 0.48368582129478455, 0.061489664018154144, 0.13478900492191315, 0.07760027050971985, 0.17247457802295685, 0.24761337041854858, 0.2753252387046814, 0.13330429792404175, 0.06751059740781784, 0.6185256838798523, 0.14666323363780975, 0.02886371500790119, 0.13123145699501038, 0.1095379889011383, 0.23633836209774017, 0.0726417601108551, 0.09967774152755737, 0.19478732347488403, 0.2507082521915436, 0.1318710744380951, 0.05994076654314995, 0.322824627161026, 0.0758315771818161, 0.2328447550535202, 0.21370846033096313, 0.28218936920166016, 0.07189194858074188, 0.4567124545574188, 0.2434874176979065, 0.07055232673883438, 0.09165868163108826]\n","[193.34657287597656, 0.29963579773902893, 0.2425174117088318, 0.10019049793481827, 0.48368582129478455, 0.061489664018154144, 0.13478900492191315, 0.07760027050971985, 0.17247457802295685, 0.24761337041854858, 0.2753252387046814, 0.13330429792404175, 0.06751059740781784, 0.6185256838798523, 0.14666323363780975, 0.02886371500790119, 0.13123145699501038, 0.1095379889011383, 0.23633836209774017, 0.0726417601108551, 0.09967774152755737, 0.19478732347488403, 0.2507082521915436, 0.1318710744380951, 0.05994076654314995, 0.322824627161026, 0.0758315771818161, 0.2328447550535202, 0.21370846033096313, 0.28218936920166016, 0.07189194858074188, 0.4567124545574188, 0.2434874176979065, 0.07055232673883438, 0.09165868163108826]\n","--> Finished Training Task 0. Starting Test phase:\n","---->Test set 0: Average loss: 0.0002, Accuracy: 9569/10000 (96%)\n","---->Test set 1: Average loss: 0.0071, Accuracy: 1415/10000 (14%)\n","---->Test set 2: Average loss: 0.0070, Accuracy: 1122/10000 (11%)\n","---->Test set 3: Average loss: 0.0122, Accuracy: 993/10000 (10%)\n","---->Test set 4: Average loss: 0.0051, Accuracy: 1068/10000 (11%)\n","---->Test set 5: Average loss: 0.0041, Accuracy: 1461/10000 (15%)\n","---->Test set 6: Average loss: 0.0068, Accuracy: 639/10000 (6%)\n","---->Test set 7: Average loss: 0.0072, Accuracy: 834/10000 (8%)\n","---->Test set 8: Average loss: 0.0110, Accuracy: 1037/10000 (10%)\n","---->Test set 9: Average loss: 0.0093, Accuracy: 927/10000 (9%)\n","--> Training Task 1:\n","----> Epoch 0:\n","---->[0/60000 (0%)]\tPrecision: 0.125000\tLoss: 8.286850\tSurrogate Loss: 0.000000\tTotal Loss: 8.286850\n","---->[12800/60000 (21%)]\tPrecision: 0.093750\tLoss: 2.305244\tSurrogate Loss: 2.005916\tTotal Loss: 2.505836\n","---->[25600/60000 (43%)]\tPrecision: 0.109375\tLoss: 2.307569\tSurrogate Loss: 0.908722\tTotal Loss: 2.398442\n","---->[38400/60000 (64%)]\tPrecision: 0.093750\tLoss: 2.311607\tSurrogate Loss: 0.556554\tTotal Loss: 2.367262\n","---->[51200/60000 (85%)]\tPrecision: 0.062500\tLoss: 2.307784\tSurrogate Loss: 5.280493\tTotal Loss: 2.835833\n","----> Epoch 1:\n","---->[0/60000 (0%)]\tPrecision: 0.156250\tLoss: 2.296202\tSurrogate Loss: 0.565028\tTotal Loss: 2.352705\n","---->[12800/60000 (21%)]\tPrecision: 0.093750\tLoss: 2.301128\tSurrogate Loss: 0.323219\tTotal Loss: 2.333450\n","---->[25600/60000 (43%)]\tPrecision: 0.125000\tLoss: 2.299820\tSurrogate Loss: 0.253804\tTotal Loss: 2.325200\n","---->[38400/60000 (64%)]\tPrecision: 0.156250\tLoss: 2.295074\tSurrogate Loss: 1.592805\tTotal Loss: 2.454354\n","---->[51200/60000 (85%)]\tPrecision: 0.109375\tLoss: 2.309411\tSurrogate Loss: 13.296952\tTotal Loss: 3.639106\n","----> Epoch 2:\n","---->[0/60000 (0%)]\tPrecision: 0.156250\tLoss: 2.302029\tSurrogate Loss: 8.338902\tTotal Loss: 3.135919\n","---->[12800/60000 (21%)]\tPrecision: 0.046875\tLoss: 2.313347\tSurrogate Loss: 5.620228\tTotal Loss: 2.875370\n","---->[25600/60000 (43%)]\tPrecision: 0.093750\tLoss: 2.308033\tSurrogate Loss: 4.207838\tTotal Loss: 2.728816\n","---->[38400/60000 (64%)]\tPrecision: 0.109375\tLoss: 2.310052\tSurrogate Loss: 3.292051\tTotal Loss: 2.639257\n","---->[51200/60000 (85%)]\tPrecision: 0.046875\tLoss: 2.311309\tSurrogate Loss: 2.649736\tTotal Loss: 2.576282\n","----> Epoch 3:\n","---->[0/60000 (0%)]\tPrecision: 0.109375\tLoss: 2.309035\tSurrogate Loss: 2.308713\tTotal Loss: 2.539907\n","---->[12800/60000 (21%)]\tPrecision: 0.015625\tLoss: 2.311259\tSurrogate Loss: 1.912245\tTotal Loss: 2.502483\n","---->[25600/60000 (43%)]\tPrecision: 0.093750\tLoss: 2.300071\tSurrogate Loss: 1.599210\tTotal Loss: 2.459992\n","---->[38400/60000 (64%)]\tPrecision: 0.078125\tLoss: 2.303436\tSurrogate Loss: 1.350285\tTotal Loss: 2.438464\n","---->[51200/60000 (85%)]\tPrecision: 0.156250\tLoss: 2.298449\tSurrogate Loss: 1.135886\tTotal Loss: 2.412038\n","----> Epoch 4:\n","---->[0/60000 (0%)]\tPrecision: 0.093750\tLoss: 2.309933\tSurrogate Loss: 1.018282\tTotal Loss: 2.411762\n","---->[12800/60000 (21%)]\tPrecision: 0.046875\tLoss: 2.313937\tSurrogate Loss: 0.866455\tTotal Loss: 2.400583\n","---->[25600/60000 (43%)]\tPrecision: 0.093750\tLoss: 2.301692\tSurrogate Loss: 0.745474\tTotal Loss: 2.376240\n","---->[38400/60000 (64%)]\tPrecision: 0.046875\tLoss: 2.309849\tSurrogate Loss: 0.639160\tTotal Loss: 2.373765\n","---->[51200/60000 (85%)]\tPrecision: 0.171875\tLoss: 2.310819\tSurrogate Loss: 0.558663\tTotal Loss: 2.366686\n","----> Epoch 5:\n","---->[0/60000 (0%)]\tPrecision: 0.187500\tLoss: 2.297570\tSurrogate Loss: 0.502452\tTotal Loss: 2.347816\n","---->[12800/60000 (21%)]\tPrecision: 0.062500\tLoss: 2.315462\tSurrogate Loss: 0.434285\tTotal Loss: 2.358891\n","---->[25600/60000 (43%)]\tPrecision: 0.109375\tLoss: 2.305237\tSurrogate Loss: 0.384694\tTotal Loss: 2.343706\n","---->[38400/60000 (64%)]\tPrecision: 0.078125\tLoss: 2.314127\tSurrogate Loss: 0.332762\tTotal Loss: 2.347403\n","---->[51200/60000 (85%)]\tPrecision: 0.109375\tLoss: 2.308412\tSurrogate Loss: 0.297136\tTotal Loss: 2.338126\n","----> Epoch 6:\n","---->[0/60000 (0%)]\tPrecision: 0.156250\tLoss: 2.296446\tSurrogate Loss: 0.276866\tTotal Loss: 2.324133\n","---->[12800/60000 (21%)]\tPrecision: 0.078125\tLoss: 2.307353\tSurrogate Loss: 0.239160\tTotal Loss: 2.331269\n","---->[25600/60000 (43%)]\tPrecision: 0.093750\tLoss: 2.302908\tSurrogate Loss: 0.225202\tTotal Loss: 2.325428\n","---->[38400/60000 (64%)]\tPrecision: 0.078125\tLoss: 2.298020\tSurrogate Loss: 0.199572\tTotal Loss: 2.317977\n","---->[51200/60000 (85%)]\tPrecision: 0.015625\tLoss: 2.313405\tSurrogate Loss: 0.193603\tTotal Loss: 2.332765\n","[8.286849975585938, 2.3052444458007812, 2.3075692653656006, 2.3116066455841064, 2.307783603668213, 2.2962019443511963, 2.3011276721954346, 2.2998199462890625, 2.2950737476348877, 2.309410810470581, 2.3020291328430176, 2.313347101211548, 2.308032512664795, 2.3100521564483643, 2.3113086223602295, 2.309035062789917, 2.311258554458618, 2.3000712394714355, 2.3034355640411377, 2.2984490394592285, 2.3099334239959717, 2.313936948776245, 2.301692485809326, 2.3098487854003906, 2.310819387435913, 2.2975704669952393, 2.315462112426758, 2.305237054824829, 2.31412672996521, 2.3084120750427246, 2.2964460849761963, 2.3073532581329346, 2.302908182144165, 2.298020124435425, 2.3134050369262695]\n","[8.286849975585938, 2.505836009979248, 2.3984415531158447, 2.367262125015259, 2.8358328342437744, 2.3527047634124756, 2.333449602127075, 2.325200319290161, 2.4543542861938477, 3.639106035232544, 3.1359193325042725, 2.8753700256347656, 2.728816270828247, 2.6392571926116943, 2.576282262802124, 2.5399065017700195, 2.5024831295013428, 2.4599921703338623, 2.4384639263153076, 2.4120376110076904, 2.411761522293091, 2.4005825519561768, 2.3762400150299072, 2.373764753341675, 2.366685628890991, 2.347815752029419, 2.3588905334472656, 2.3437063694000244, 2.347402811050415, 2.338125705718994, 2.3241326808929443, 2.3312692642211914, 2.3254284858703613, 2.3179774284362793, 2.3327653408050537]\n","--> Finished Training Task 1. Starting Test phase:\n","---->Test set 0: Average loss: 0.0006, Accuracy: 9074/10000 (91%)\n","---->Test set 1: Average loss: 0.0717, Accuracy: 958/10000 (10%)\n","---->Test set 2: Average loss: 0.0166, Accuracy: 956/10000 (10%)\n","---->Test set 3: Average loss: 0.0214, Accuracy: 980/10000 (10%)\n","---->Test set 4: Average loss: 0.0271, Accuracy: 954/10000 (10%)\n","---->Test set 5: Average loss: 0.0199, Accuracy: 1070/10000 (11%)\n","---->Test set 6: Average loss: 0.0147, Accuracy: 741/10000 (7%)\n","---->Test set 7: Average loss: 0.0192, Accuracy: 1129/10000 (11%)\n","---->Test set 8: Average loss: 0.0269, Accuracy: 1022/10000 (10%)\n","---->Test set 9: Average loss: 0.0192, Accuracy: 691/10000 (7%)\n","--> Training Task 2:\n","----> Epoch 0:\n","---->[0/60000 (0%)]\tPrecision: 0.171875\tLoss: 14.369658\tSurrogate Loss: 0.000000\tTotal Loss: 14.369658\n","---->[12800/60000 (21%)]\tPrecision: 0.062500\tLoss: 2.299421\tSurrogate Loss: 1.694479\tTotal Loss: 2.468869\n","---->[25600/60000 (43%)]\tPrecision: 0.062500\tLoss: 2.302511\tSurrogate Loss: 1.476762\tTotal Loss: 2.450187\n","---->[38400/60000 (64%)]\tPrecision: 0.125000\tLoss: 2.301478\tSurrogate Loss: 4.597251\tTotal Loss: 2.761203\n","---->[51200/60000 (85%)]\tPrecision: 0.109375\tLoss: 2.298959\tSurrogate Loss: 1.693457\tTotal Loss: 2.468304\n","----> Epoch 1:\n","---->[0/60000 (0%)]\tPrecision: 0.078125\tLoss: 2.299791\tSurrogate Loss: 1.165289\tTotal Loss: 2.416320\n","---->[12800/60000 (21%)]\tPrecision: 0.125000\tLoss: 2.302540\tSurrogate Loss: 0.775228\tTotal Loss: 2.380062\n","---->[25600/60000 (43%)]\tPrecision: 0.125000\tLoss: 2.302067\tSurrogate Loss: 0.555730\tTotal Loss: 2.357640\n","---->[38400/60000 (64%)]\tPrecision: 0.109375\tLoss: 2.297363\tSurrogate Loss: 0.417998\tTotal Loss: 2.339163\n","---->[51200/60000 (85%)]\tPrecision: 0.125000\tLoss: 2.300317\tSurrogate Loss: 0.323190\tTotal Loss: 2.332636\n","----> Epoch 2:\n","---->[0/60000 (0%)]\tPrecision: 0.093750\tLoss: 2.314507\tSurrogate Loss: 0.273859\tTotal Loss: 2.341893\n","---->[12800/60000 (21%)]\tPrecision: 0.078125\tLoss: 2.308854\tSurrogate Loss: 0.218537\tTotal Loss: 2.330708\n","---->[25600/60000 (43%)]\tPrecision: 0.156250\tLoss: 2.298492\tSurrogate Loss: 0.176770\tTotal Loss: 2.316170\n","---->[38400/60000 (64%)]\tPrecision: 0.062500\tLoss: 2.300378\tSurrogate Loss: 0.147329\tTotal Loss: 2.315111\n","---->[51200/60000 (85%)]\tPrecision: 0.140625\tLoss: 2.297317\tSurrogate Loss: 0.123303\tTotal Loss: 2.309648\n","----> Epoch 3:\n","---->[0/60000 (0%)]\tPrecision: 0.078125\tLoss: 2.303012\tSurrogate Loss: 0.107482\tTotal Loss: 2.313760\n","---->[12800/60000 (21%)]\tPrecision: 0.109375\tLoss: 2.309201\tSurrogate Loss: 0.086057\tTotal Loss: 2.317807\n","---->[25600/60000 (43%)]\tPrecision: 0.062500\tLoss: 2.298814\tSurrogate Loss: 0.072452\tTotal Loss: 2.306059\n","---->[38400/60000 (64%)]\tPrecision: 0.171875\tLoss: 2.297718\tSurrogate Loss: 0.062935\tTotal Loss: 2.304012\n","---->[51200/60000 (85%)]\tPrecision: 0.171875\tLoss: 2.305767\tSurrogate Loss: 0.052628\tTotal Loss: 2.311029\n","----> Epoch 4:\n","---->[0/60000 (0%)]\tPrecision: 0.218750\tLoss: 2.293710\tSurrogate Loss: 0.048050\tTotal Loss: 2.298514\n","---->[12800/60000 (21%)]\tPrecision: 0.109375\tLoss: 2.309832\tSurrogate Loss: 0.039234\tTotal Loss: 2.313755\n","---->[25600/60000 (43%)]\tPrecision: 0.156250\tLoss: 2.295716\tSurrogate Loss: 0.034823\tTotal Loss: 2.299198\n","---->[38400/60000 (64%)]\tPrecision: 0.093750\tLoss: 2.308602\tSurrogate Loss: 0.026693\tTotal Loss: 2.311271\n","---->[51200/60000 (85%)]\tPrecision: 0.125000\tLoss: 2.304918\tSurrogate Loss: 0.024598\tTotal Loss: 2.307377\n","----> Epoch 5:\n","---->[0/60000 (0%)]\tPrecision: 0.078125\tLoss: 2.307785\tSurrogate Loss: 0.024274\tTotal Loss: 2.310212\n","---->[12800/60000 (21%)]\tPrecision: 0.156250\tLoss: 2.294529\tSurrogate Loss: 0.020153\tTotal Loss: 2.296545\n","---->[25600/60000 (43%)]\tPrecision: 0.046875\tLoss: 2.308782\tSurrogate Loss: 0.021478\tTotal Loss: 2.310930\n","---->[38400/60000 (64%)]\tPrecision: 0.062500\tLoss: 2.298517\tSurrogate Loss: 0.017690\tTotal Loss: 2.300286\n","---->[51200/60000 (85%)]\tPrecision: 0.125000\tLoss: 2.298160\tSurrogate Loss: 0.016887\tTotal Loss: 2.299849\n","----> Epoch 6:\n","---->[0/60000 (0%)]\tPrecision: 0.093750\tLoss: 2.305461\tSurrogate Loss: 0.015532\tTotal Loss: 2.307014\n","---->[12800/60000 (21%)]\tPrecision: 0.046875\tLoss: 2.309256\tSurrogate Loss: 0.011162\tTotal Loss: 2.310372\n","---->[25600/60000 (43%)]\tPrecision: 0.078125\tLoss: 2.301515\tSurrogate Loss: 0.012688\tTotal Loss: 2.302783\n","---->[38400/60000 (64%)]\tPrecision: 0.062500\tLoss: 2.311475\tSurrogate Loss: 0.010108\tTotal Loss: 2.312486\n","---->[51200/60000 (85%)]\tPrecision: 0.218750\tLoss: 2.302052\tSurrogate Loss: 0.007947\tTotal Loss: 2.302846\n","[14.369658470153809, 2.2994208335876465, 2.3025107383728027, 2.301478147506714, 2.2989585399627686, 2.299790620803833, 2.302539587020874, 2.302067279815674, 2.29736328125, 2.3003170490264893, 2.314507484436035, 2.3088538646698, 2.298492431640625, 2.3003783226013184, 2.2973172664642334, 2.303011655807495, 2.30920147895813, 2.298813581466675, 2.297718048095703, 2.3057665824890137, 2.2937095165252686, 2.3098318576812744, 2.2957160472869873, 2.3086020946502686, 2.304917573928833, 2.3077850341796875, 2.2945291996002197, 2.3087823390960693, 2.2985165119171143, 2.2981603145599365, 2.3054606914520264, 2.309256076812744, 2.3015146255493164, 2.3114748001098633, 2.3020517826080322]\n","[14.369658470153809, 2.4688687324523926, 2.4501869678497314, 2.7612032890319824, 2.46830415725708, 2.4163196086883545, 2.3800623416900635, 2.357640266418457, 2.339163064956665, 2.3326361179351807, 2.341893434524536, 2.330707550048828, 2.316169500350952, 2.3151111602783203, 2.309647560119629, 2.3137598037719727, 2.317807197570801, 2.306058883666992, 2.304011583328247, 2.3110294342041016, 2.2985143661499023, 2.3137552738189697, 2.2991983890533447, 2.3112714290618896, 2.307377338409424, 2.3102123737335205, 2.2965445518493652, 2.310930013656616, 2.300285577774048, 2.299849033355713, 2.307013988494873, 2.3103723526000977, 2.302783489227295, 2.312485694885254, 2.3028464317321777]\n","--> Finished Training Task 2. Starting Test phase:\n","---->Test set 0: Average loss: 0.0005, Accuracy: 9082/10000 (91%)\n","---->Test set 1: Average loss: 0.0535, Accuracy: 958/10000 (10%)\n","---->Test set 2: Average loss: 0.0023, Accuracy: 1032/10000 (10%)\n","---->Test set 3: Average loss: 0.0028, Accuracy: 1098/10000 (11%)\n","---->Test set 4: Average loss: 0.0023, Accuracy: 1027/10000 (10%)\n","---->Test set 5: Average loss: 0.0023, Accuracy: 1032/10000 (10%)\n","---->Test set 6: Average loss: 0.0023, Accuracy: 1033/10000 (10%)\n","---->Test set 7: Average loss: 0.0023, Accuracy: 1040/10000 (10%)\n","---->Test set 8: Average loss: 0.0024, Accuracy: 1055/10000 (11%)\n","---->Test set 9: Average loss: 0.0024, Accuracy: 1020/10000 (10%)\n","--> Training Task 3:\n","----> Epoch 0:\n","---->[0/60000 (0%)]\tPrecision: 0.078125\tLoss: 2.895127\tSurrogate Loss: 0.000000\tTotal Loss: 2.895127\n","---->[12800/60000 (21%)]\tPrecision: 0.156250\tLoss: 2.300747\tSurrogate Loss: 0.027071\tTotal Loss: 2.303454\n","---->[25600/60000 (43%)]\tPrecision: 0.125000\tLoss: 2.292532\tSurrogate Loss: 0.036246\tTotal Loss: 2.296157\n","---->[38400/60000 (64%)]\tPrecision: 0.062500\tLoss: 2.313300\tSurrogate Loss: 0.006411\tTotal Loss: 2.313941\n","---->[51200/60000 (85%)]\tPrecision: 0.109375\tLoss: 2.310802\tSurrogate Loss: 0.006651\tTotal Loss: 2.311466\n","----> Epoch 1:\n","---->[0/60000 (0%)]\tPrecision: 0.078125\tLoss: 2.308016\tSurrogate Loss: 0.005740\tTotal Loss: 2.308590\n","---->[12800/60000 (21%)]\tPrecision: 0.140625\tLoss: 2.293383\tSurrogate Loss: 0.003472\tTotal Loss: 2.293730\n","---->[25600/60000 (43%)]\tPrecision: 0.109375\tLoss: 2.308090\tSurrogate Loss: 0.003195\tTotal Loss: 2.308409\n","---->[38400/60000 (64%)]\tPrecision: 0.031250\tLoss: 2.317988\tSurrogate Loss: 0.001914\tTotal Loss: 2.318179\n","---->[51200/60000 (85%)]\tPrecision: 0.046875\tLoss: 2.307558\tSurrogate Loss: 0.007069\tTotal Loss: 2.308265\n","----> Epoch 2:\n","---->[0/60000 (0%)]\tPrecision: 0.109375\tLoss: 2.302035\tSurrogate Loss: 0.001420\tTotal Loss: 2.302177\n","---->[12800/60000 (21%)]\tPrecision: 0.093750\tLoss: 2.302859\tSurrogate Loss: 0.001548\tTotal Loss: 2.303014\n","---->[25600/60000 (43%)]\tPrecision: 0.140625\tLoss: 2.298317\tSurrogate Loss: 0.002635\tTotal Loss: 2.298580\n","---->[38400/60000 (64%)]\tPrecision: 0.078125\tLoss: 2.303790\tSurrogate Loss: 0.001891\tTotal Loss: 2.303979\n","---->[51200/60000 (85%)]\tPrecision: 0.062500\tLoss: 2.304343\tSurrogate Loss: 0.003685\tTotal Loss: 2.304712\n","----> Epoch 3:\n","---->[0/60000 (0%)]\tPrecision: 0.109375\tLoss: 2.300819\tSurrogate Loss: 0.002646\tTotal Loss: 2.301083\n","---->[12800/60000 (21%)]\tPrecision: 0.093750\tLoss: 2.309398\tSurrogate Loss: 0.002821\tTotal Loss: 2.309680\n","---->[25600/60000 (43%)]\tPrecision: 0.093750\tLoss: 2.300035\tSurrogate Loss: 0.000712\tTotal Loss: 2.300106\n","---->[38400/60000 (64%)]\tPrecision: 0.109375\tLoss: 2.299058\tSurrogate Loss: 0.003648\tTotal Loss: 2.299423\n","---->[51200/60000 (85%)]\tPrecision: 0.093750\tLoss: 2.301840\tSurrogate Loss: 0.001528\tTotal Loss: 2.301992\n","----> Epoch 4:\n","---->[0/60000 (0%)]\tPrecision: 0.093750\tLoss: 2.308177\tSurrogate Loss: 0.004261\tTotal Loss: 2.308603\n","---->[12800/60000 (21%)]\tPrecision: 0.109375\tLoss: 2.311946\tSurrogate Loss: 0.003690\tTotal Loss: 2.312315\n","---->[25600/60000 (43%)]\tPrecision: 0.093750\tLoss: 2.308336\tSurrogate Loss: 0.003360\tTotal Loss: 2.308672\n","---->[38400/60000 (64%)]\tPrecision: 0.109375\tLoss: 2.307625\tSurrogate Loss: 0.003936\tTotal Loss: 2.308019\n","---->[51200/60000 (85%)]\tPrecision: 0.140625\tLoss: 2.305533\tSurrogate Loss: 0.002989\tTotal Loss: 2.305832\n","----> Epoch 5:\n","---->[0/60000 (0%)]\tPrecision: 0.171875\tLoss: 2.299324\tSurrogate Loss: 0.004299\tTotal Loss: 2.299754\n","---->[12800/60000 (21%)]\tPrecision: 0.125000\tLoss: 2.315082\tSurrogate Loss: 0.004036\tTotal Loss: 2.315485\n","---->[25600/60000 (43%)]\tPrecision: 0.140625\tLoss: 2.296729\tSurrogate Loss: 0.003294\tTotal Loss: 2.297058\n","---->[38400/60000 (64%)]\tPrecision: 0.078125\tLoss: 2.302546\tSurrogate Loss: 0.004876\tTotal Loss: 2.303034\n","---->[51200/60000 (85%)]\tPrecision: 0.046875\tLoss: 2.300837\tSurrogate Loss: 0.001679\tTotal Loss: 2.301004\n","----> Epoch 6:\n","---->[0/60000 (0%)]\tPrecision: 0.156250\tLoss: 2.303049\tSurrogate Loss: 0.002862\tTotal Loss: 2.303335\n","---->[12800/60000 (21%)]\tPrecision: 0.046875\tLoss: 2.308666\tSurrogate Loss: 0.004312\tTotal Loss: 2.309097\n","---->[25600/60000 (43%)]\tPrecision: 0.093750\tLoss: 2.294532\tSurrogate Loss: 0.002714\tTotal Loss: 2.294803\n","---->[38400/60000 (64%)]\tPrecision: 0.093750\tLoss: 2.315901\tSurrogate Loss: 0.001510\tTotal Loss: 2.316052\n","---->[51200/60000 (85%)]\tPrecision: 0.093750\tLoss: 2.290129\tSurrogate Loss: 0.003596\tTotal Loss: 2.290489\n","[2.895127296447754, 2.3007473945617676, 2.292532444000244, 2.3132996559143066, 2.3108015060424805, 2.308016300201416, 2.2933826446533203, 2.3080897331237793, 2.3179876804351807, 2.3075578212738037, 2.302034854888916, 2.302859306335449, 2.298316717147827, 2.3037900924682617, 2.3043434619903564, 2.300818681716919, 2.3093979358673096, 2.300034999847412, 2.299058437347412, 2.301839590072632, 2.3081765174865723, 2.311946392059326, 2.308335781097412, 2.3076252937316895, 2.3055334091186523, 2.299323797225952, 2.3150815963745117, 2.2967288494110107, 2.3025460243225098, 2.3008365631103516, 2.303048849105835, 2.308666229248047, 2.294532060623169, 2.315901279449463, 2.2901291847229004]\n","[2.895127296447754, 2.3034543991088867, 2.296157121658325, 2.3139407634735107, 2.3114664554595947, 2.3085901737213135, 2.293729782104492, 2.3084092140197754, 2.318179130554199, 2.30826473236084, 2.3021769523620605, 2.303014039993286, 2.2985801696777344, 2.3039791584014893, 2.3047120571136475, 2.3010833263397217, 2.3096799850463867, 2.3001062870025635, 2.2994232177734375, 2.301992416381836, 2.3086025714874268, 2.3123154640197754, 2.308671712875366, 2.308018922805786, 2.3058323860168457, 2.2997536659240723, 2.3154852390289307, 2.297058343887329, 2.3030335903167725, 2.301004409790039, 2.303334951400757, 2.3090972900390625, 2.2948033809661865, 2.316052198410034, 2.2904887199401855]\n","--> Finished Training Task 3. Starting Test phase:\n","---->Test set 0: Average loss: 0.0005, Accuracy: 9107/10000 (91%)\n","---->Test set 1: Average loss: 0.0433, Accuracy: 957/10000 (10%)\n","---->Test set 2: Average loss: 0.0023, Accuracy: 1032/10000 (10%)\n","---->Test set 3: Average loss: 0.0023, Accuracy: 1032/10000 (10%)\n","---->Test set 4: Average loss: 0.0023, Accuracy: 1031/10000 (10%)\n","---->Test set 5: Average loss: 0.0023, Accuracy: 1033/10000 (10%)\n","---->Test set 6: Average loss: 0.0023, Accuracy: 1032/10000 (10%)\n","---->Test set 7: Average loss: 0.0023, Accuracy: 1032/10000 (10%)\n","---->Test set 8: Average loss: 0.0023, Accuracy: 1032/10000 (10%)\n","---->Test set 9: Average loss: 0.0023, Accuracy: 1032/10000 (10%)\n","--> Training Task 4:\n","----> Epoch 0:\n","---->[0/60000 (0%)]\tPrecision: 0.078125\tLoss: 2.296905\tSurrogate Loss: 0.000000\tTotal Loss: 2.296905\n","---->[12800/60000 (21%)]\tPrecision: 0.078125\tLoss: 2.301100\tSurrogate Loss: 0.016559\tTotal Loss: 2.302756\n","---->[25600/60000 (43%)]\tPrecision: 0.156250\tLoss: 2.290085\tSurrogate Loss: 0.003396\tTotal Loss: 2.290424\n","---->[38400/60000 (64%)]\tPrecision: 0.046875\tLoss: 2.312764\tSurrogate Loss: 0.002501\tTotal Loss: 2.313014\n","---->[51200/60000 (85%)]\tPrecision: 0.140625\tLoss: 2.306010\tSurrogate Loss: 0.002868\tTotal Loss: 2.306297\n","----> Epoch 1:\n","---->[0/60000 (0%)]\tPrecision: 0.046875\tLoss: 2.307109\tSurrogate Loss: 0.000647\tTotal Loss: 2.307174\n","---->[12800/60000 (21%)]\tPrecision: 0.125000\tLoss: 2.300153\tSurrogate Loss: 0.001980\tTotal Loss: 2.300351\n","---->[25600/60000 (43%)]\tPrecision: 0.093750\tLoss: 2.293886\tSurrogate Loss: 0.001674\tTotal Loss: 2.294054\n","---->[38400/60000 (64%)]\tPrecision: 0.062500\tLoss: 2.306056\tSurrogate Loss: 0.001447\tTotal Loss: 2.306200\n","---->[51200/60000 (85%)]\tPrecision: 0.078125\tLoss: 2.310894\tSurrogate Loss: 0.002966\tTotal Loss: 2.311191\n","----> Epoch 2:\n","---->[0/60000 (0%)]\tPrecision: 0.093750\tLoss: 2.301198\tSurrogate Loss: 0.002503\tTotal Loss: 2.301448\n","---->[12800/60000 (21%)]\tPrecision: 0.125000\tLoss: 2.304065\tSurrogate Loss: 0.003133\tTotal Loss: 2.304378\n","---->[25600/60000 (43%)]\tPrecision: 0.109375\tLoss: 2.308194\tSurrogate Loss: 0.001862\tTotal Loss: 2.308380\n","---->[38400/60000 (64%)]\tPrecision: 0.140625\tLoss: 2.300843\tSurrogate Loss: 0.002154\tTotal Loss: 2.301059\n","---->[51200/60000 (85%)]\tPrecision: 0.093750\tLoss: 2.303513\tSurrogate Loss: 0.005628\tTotal Loss: 2.304076\n","----> Epoch 3:\n","---->[0/60000 (0%)]\tPrecision: 0.125000\tLoss: 2.308817\tSurrogate Loss: 0.003272\tTotal Loss: 2.309144\n","---->[12800/60000 (21%)]\tPrecision: 0.093750\tLoss: 2.306585\tSurrogate Loss: 0.002611\tTotal Loss: 2.306846\n","---->[25600/60000 (43%)]\tPrecision: 0.109375\tLoss: 2.300410\tSurrogate Loss: 0.001554\tTotal Loss: 2.300566\n","---->[38400/60000 (64%)]\tPrecision: 0.171875\tLoss: 2.290698\tSurrogate Loss: 0.002902\tTotal Loss: 2.290988\n","---->[51200/60000 (85%)]\tPrecision: 0.093750\tLoss: 2.306320\tSurrogate Loss: 0.003150\tTotal Loss: 2.306635\n","----> Epoch 4:\n","---->[0/60000 (0%)]\tPrecision: 0.093750\tLoss: 2.308637\tSurrogate Loss: 0.007210\tTotal Loss: 2.309358\n","---->[12800/60000 (21%)]\tPrecision: 0.062500\tLoss: 2.304475\tSurrogate Loss: 0.003756\tTotal Loss: 2.304850\n","---->[25600/60000 (43%)]\tPrecision: 0.093750\tLoss: 2.297978\tSurrogate Loss: 0.004977\tTotal Loss: 2.298475\n","---->[38400/60000 (64%)]\tPrecision: 0.109375\tLoss: 2.304208\tSurrogate Loss: 0.003422\tTotal Loss: 2.304550\n","---->[51200/60000 (85%)]\tPrecision: 0.046875\tLoss: 2.318600\tSurrogate Loss: 0.002596\tTotal Loss: 2.318860\n","----> Epoch 5:\n","---->[0/60000 (0%)]\tPrecision: 0.078125\tLoss: 2.301632\tSurrogate Loss: 0.002603\tTotal Loss: 2.301893\n","---->[12800/60000 (21%)]\tPrecision: 0.093750\tLoss: 2.298406\tSurrogate Loss: 0.005516\tTotal Loss: 2.298958\n","---->[25600/60000 (43%)]\tPrecision: 0.078125\tLoss: 2.302871\tSurrogate Loss: 0.002079\tTotal Loss: 2.303079\n","---->[38400/60000 (64%)]\tPrecision: 0.140625\tLoss: 2.306470\tSurrogate Loss: 0.002278\tTotal Loss: 2.306698\n","---->[51200/60000 (85%)]\tPrecision: 0.062500\tLoss: 2.309431\tSurrogate Loss: 0.003093\tTotal Loss: 2.309740\n","----> Epoch 6:\n","---->[0/60000 (0%)]\tPrecision: 0.203125\tLoss: 2.290175\tSurrogate Loss: 0.001319\tTotal Loss: 2.290307\n","---->[12800/60000 (21%)]\tPrecision: 0.093750\tLoss: 2.306700\tSurrogate Loss: 0.003805\tTotal Loss: 2.307081\n","---->[25600/60000 (43%)]\tPrecision: 0.156250\tLoss: 2.290921\tSurrogate Loss: 0.003078\tTotal Loss: 2.291229\n","---->[38400/60000 (64%)]\tPrecision: 0.046875\tLoss: 2.301491\tSurrogate Loss: 0.006015\tTotal Loss: 2.302092\n","---->[51200/60000 (85%)]\tPrecision: 0.140625\tLoss: 2.291414\tSurrogate Loss: 0.004457\tTotal Loss: 2.291860\n","[2.2969045639038086, 2.301100492477417, 2.2900846004486084, 2.3127639293670654, 2.3060104846954346, 2.3071091175079346, 2.3001527786254883, 2.293886184692383, 2.3060555458068848, 2.310894012451172, 2.3011975288391113, 2.304064989089966, 2.3081939220428467, 2.3008432388305664, 2.303513288497925, 2.30881667137146, 2.3065853118896484, 2.300410270690918, 2.2906978130340576, 2.3063201904296875, 2.3086369037628174, 2.3044748306274414, 2.2979776859283447, 2.3042078018188477, 2.3186001777648926, 2.3016321659088135, 2.2984063625335693, 2.3028714656829834, 2.3064699172973633, 2.3094305992126465, 2.290175437927246, 2.3067002296447754, 2.2909209728240967, 2.3014907836914062, 2.291414260864258]\n","[2.2969045639038086, 2.3027563095092773, 2.290424108505249, 2.313014030456543, 2.3062973022460938, 2.307173728942871, 2.3003506660461426, 2.294053554534912, 2.3062002658843994, 2.311190605163574, 2.301447868347168, 2.3043782711029053, 2.308380126953125, 2.301058530807495, 2.3040761947631836, 2.3091437816619873, 2.3068463802337646, 2.300565719604492, 2.290987968444824, 2.3066351413726807, 2.3093578815460205, 2.3048503398895264, 2.2984752655029297, 2.3045499324798584, 2.318859815597534, 2.3018925189971924, 2.2989580631256104, 2.30307936668396, 2.3066976070404053, 2.309739828109741, 2.2903072834014893, 2.3070807456970215, 2.291228771209717, 2.3020923137664795, 2.2918598651885986]\n","--> Finished Training Task 4. Starting Test phase:\n","---->Test set 0: Average loss: 0.0005, Accuracy: 9108/10000 (91%)\n","---->Test set 1: Average loss: 0.0411, Accuracy: 957/10000 (10%)\n","---->Test set 2: Average loss: 0.0023, Accuracy: 1032/10000 (10%)\n","---->Test set 3: Average loss: 0.0023, Accuracy: 1032/10000 (10%)\n","---->Test set 4: Average loss: 0.0023, Accuracy: 1032/10000 (10%)\n","---->Test set 5: Average loss: 0.0023, Accuracy: 1032/10000 (10%)\n","---->Test set 6: Average loss: 0.0023, Accuracy: 1032/10000 (10%)\n","---->Test set 7: Average loss: 0.0023, Accuracy: 1032/10000 (10%)\n","---->Test set 8: Average loss: 0.0023, Accuracy: 1032/10000 (10%)\n","---->Test set 9: Average loss: 0.0023, Accuracy: 1032/10000 (10%)\n","--> Training Task 5:\n","----> Epoch 0:\n","---->[0/60000 (0%)]\tPrecision: 0.125000\tLoss: 2.307660\tSurrogate Loss: 0.000000\tTotal Loss: 2.307660\n","---->[12800/60000 (21%)]\tPrecision: 0.109375\tLoss: 2.316742\tSurrogate Loss: 1.308202\tTotal Loss: 2.447562\n","---->[25600/60000 (43%)]\tPrecision: 0.093750\tLoss: 2.310268\tSurrogate Loss: 0.002844\tTotal Loss: 2.310552\n","---->[38400/60000 (64%)]\tPrecision: 0.093750\tLoss: 2.298494\tSurrogate Loss: 90.917946\tTotal Loss: 11.390289\n","---->[51200/60000 (85%)]\tPrecision: 0.093750\tLoss: 2.303474\tSurrogate Loss: 1.156004\tTotal Loss: 2.419074\n","----> Epoch 1:\n","---->[0/60000 (0%)]\tPrecision: 0.078125\tLoss: 2.303802\tSurrogate Loss: 0.510857\tTotal Loss: 2.354887\n","---->[12800/60000 (21%)]\tPrecision: 0.140625\tLoss: 2.294039\tSurrogate Loss: 0.230857\tTotal Loss: 2.317124\n","---->[25600/60000 (43%)]\tPrecision: 0.125000\tLoss: 2.296452\tSurrogate Loss: 0.135929\tTotal Loss: 2.310045\n","---->[38400/60000 (64%)]\tPrecision: 0.046875\tLoss: 2.311261\tSurrogate Loss: 0.091557\tTotal Loss: 2.320417\n","---->[51200/60000 (85%)]\tPrecision: 0.062500\tLoss: 2.313451\tSurrogate Loss: 0.068780\tTotal Loss: 2.320329\n","----> Epoch 2:\n","---->[0/60000 (0%)]\tPrecision: 0.078125\tLoss: 2.305540\tSurrogate Loss: 0.054547\tTotal Loss: 2.310995\n","---->[12800/60000 (21%)]\tPrecision: 0.078125\tLoss: 2.303660\tSurrogate Loss: 0.038497\tTotal Loss: 2.307510\n","---->[25600/60000 (43%)]\tPrecision: 0.078125\tLoss: 2.308808\tSurrogate Loss: 0.030417\tTotal Loss: 2.311849\n","---->[38400/60000 (64%)]\tPrecision: 0.093750\tLoss: 2.306932\tSurrogate Loss: 0.023573\tTotal Loss: 2.309289\n","---->[51200/60000 (85%)]\tPrecision: 0.093750\tLoss: 2.303971\tSurrogate Loss: 0.017500\tTotal Loss: 2.305721\n","----> Epoch 3:\n","---->[0/60000 (0%)]\tPrecision: 0.203125\tLoss: 2.285655\tSurrogate Loss: 0.015368\tTotal Loss: 2.287192\n","---->[12800/60000 (21%)]\tPrecision: 0.062500\tLoss: 2.307518\tSurrogate Loss: 0.011608\tTotal Loss: 2.308679\n","---->[25600/60000 (43%)]\tPrecision: 0.093750\tLoss: 2.306912\tSurrogate Loss: 0.012100\tTotal Loss: 2.308122\n","---->[38400/60000 (64%)]\tPrecision: 0.093750\tLoss: 2.292450\tSurrogate Loss: 0.008555\tTotal Loss: 2.293306\n","---->[51200/60000 (85%)]\tPrecision: 0.078125\tLoss: 2.305957\tSurrogate Loss: 0.006236\tTotal Loss: 2.306580\n","----> Epoch 4:\n","---->[0/60000 (0%)]\tPrecision: 0.109375\tLoss: 2.294986\tSurrogate Loss: 0.006091\tTotal Loss: 2.295595\n","---->[12800/60000 (21%)]\tPrecision: 0.171875\tLoss: 2.299386\tSurrogate Loss: 0.004688\tTotal Loss: 2.299855\n","---->[25600/60000 (43%)]\tPrecision: 0.078125\tLoss: 2.302959\tSurrogate Loss: 0.009226\tTotal Loss: 2.303882\n","---->[38400/60000 (64%)]\tPrecision: 0.125000\tLoss: 2.307819\tSurrogate Loss: 0.003852\tTotal Loss: 2.308204\n","---->[51200/60000 (85%)]\tPrecision: 0.109375\tLoss: 2.306619\tSurrogate Loss: 0.003466\tTotal Loss: 2.306966\n","----> Epoch 5:\n","---->[0/60000 (0%)]\tPrecision: 0.093750\tLoss: 2.304719\tSurrogate Loss: 0.005222\tTotal Loss: 2.305241\n","---->[12800/60000 (21%)]\tPrecision: 0.140625\tLoss: 2.302491\tSurrogate Loss: 0.006579\tTotal Loss: 2.303149\n","---->[25600/60000 (43%)]\tPrecision: 0.125000\tLoss: 2.292284\tSurrogate Loss: 0.003171\tTotal Loss: 2.292601\n","---->[38400/60000 (64%)]\tPrecision: 0.078125\tLoss: 2.309163\tSurrogate Loss: 0.004555\tTotal Loss: 2.309618\n","---->[51200/60000 (85%)]\tPrecision: 0.109375\tLoss: 2.299626\tSurrogate Loss: 0.005329\tTotal Loss: 2.300159\n","----> Epoch 6:\n","---->[0/60000 (0%)]\tPrecision: 0.109375\tLoss: 2.313062\tSurrogate Loss: 0.002883\tTotal Loss: 2.313350\n","---->[12800/60000 (21%)]\tPrecision: 0.078125\tLoss: 2.298940\tSurrogate Loss: 0.003311\tTotal Loss: 2.299271\n","---->[25600/60000 (43%)]\tPrecision: 0.140625\tLoss: 2.298571\tSurrogate Loss: 0.003532\tTotal Loss: 2.298924\n","---->[38400/60000 (64%)]\tPrecision: 0.156250\tLoss: 2.292200\tSurrogate Loss: 0.005492\tTotal Loss: 2.292749\n","---->[51200/60000 (85%)]\tPrecision: 0.062500\tLoss: 2.315214\tSurrogate Loss: 0.002034\tTotal Loss: 2.315418\n","[2.307659864425659, 2.316742181777954, 2.310267686843872, 2.298494338989258, 2.303473711013794, 2.3038017749786377, 2.294038772583008, 2.296452283859253, 2.3112614154815674, 2.313450813293457, 2.305540084838867, 2.3036599159240723, 2.308807611465454, 2.306932210922241, 2.303971290588379, 2.2856547832489014, 2.307518243789673, 2.306912422180176, 2.292450189590454, 2.3059566020965576, 2.294985771179199, 2.2993857860565186, 2.3029589653015137, 2.307819128036499, 2.30661940574646, 2.3047189712524414, 2.3024914264678955, 2.2922842502593994, 2.3091628551483154, 2.299625873565674, 2.3130621910095215, 2.2989401817321777, 2.2985713481903076, 2.2922000885009766, 2.3152143955230713]\n","[2.307659864425659, 2.4475624561309814, 2.3105521202087402, 11.390289306640625, 2.419074058532715, 2.3548874855041504, 2.317124366760254, 2.3100452423095703, 2.3204171657562256, 2.320328712463379, 2.310994863510132, 2.307509660720825, 2.311849355697632, 2.3092894554138184, 2.3057212829589844, 2.28719162940979, 2.3086791038513184, 2.308122396469116, 2.2933056354522705, 2.3065803050994873, 2.2955949306488037, 2.2998545169830322, 2.3038816452026367, 2.308204412460327, 2.3069660663604736, 2.305241107940674, 2.3031492233276367, 2.2926013469696045, 2.3096182346343994, 2.300158739089966, 2.3133504390716553, 2.29927134513855, 2.298924446105957, 2.2927494049072266, 2.315417766571045]\n","--> Finished Training Task 5. Starting Test phase:\n","---->Test set 0: Average loss: 0.0005, Accuracy: 9107/10000 (91%)\n","---->Test set 1: Average loss: 0.0367, Accuracy: 957/10000 (10%)\n","---->Test set 2: Average loss: 0.0023, Accuracy: 1032/10000 (10%)\n","---->Test set 3: Average loss: 0.0023, Accuracy: 1032/10000 (10%)\n","---->Test set 4: Average loss: 0.0023, Accuracy: 1032/10000 (10%)\n","---->Test set 5: Average loss: 0.0023, Accuracy: 1032/10000 (10%)\n","---->Test set 6: Average loss: 0.0023, Accuracy: 1032/10000 (10%)\n","---->Test set 7: Average loss: 0.0023, Accuracy: 1032/10000 (10%)\n","---->Test set 8: Average loss: 0.0023, Accuracy: 1032/10000 (10%)\n","---->Test set 9: Average loss: 0.0023, Accuracy: 1032/10000 (10%)\n","--> Training Task 6:\n","----> Epoch 0:\n","---->[0/60000 (0%)]\tPrecision: 0.031250\tLoss: 2.316942\tSurrogate Loss: 0.000000\tTotal Loss: 2.316942\n","---->[12800/60000 (21%)]\tPrecision: 0.093750\tLoss: 2.300702\tSurrogate Loss: 0.064893\tTotal Loss: 2.307191\n","---->[25600/60000 (43%)]\tPrecision: 0.093750\tLoss: 2.299769\tSurrogate Loss: 0.004065\tTotal Loss: 2.300176\n","---->[38400/60000 (64%)]\tPrecision: 0.109375\tLoss: 2.300509\tSurrogate Loss: 0.565240\tTotal Loss: 2.357033\n","---->[51200/60000 (85%)]\tPrecision: 0.062500\tLoss: 2.311990\tSurrogate Loss: 0.006040\tTotal Loss: 2.312594\n","----> Epoch 1:\n","---->[0/60000 (0%)]\tPrecision: 0.062500\tLoss: 2.306740\tSurrogate Loss: 0.004217\tTotal Loss: 2.307161\n","---->[12800/60000 (21%)]\tPrecision: 0.125000\tLoss: 2.305800\tSurrogate Loss: 0.006786\tTotal Loss: 2.306479\n","---->[25600/60000 (43%)]\tPrecision: 0.046875\tLoss: 2.306216\tSurrogate Loss: 0.003617\tTotal Loss: 2.306578\n","---->[38400/60000 (64%)]\tPrecision: 0.093750\tLoss: 2.298746\tSurrogate Loss: 0.006490\tTotal Loss: 2.299395\n","---->[51200/60000 (85%)]\tPrecision: 0.093750\tLoss: 2.305686\tSurrogate Loss: 0.002632\tTotal Loss: 2.305950\n","----> Epoch 2:\n","---->[0/60000 (0%)]\tPrecision: 0.093750\tLoss: 2.302035\tSurrogate Loss: 0.001796\tTotal Loss: 2.302214\n","---->[12800/60000 (21%)]\tPrecision: 0.078125\tLoss: 2.299030\tSurrogate Loss: 0.002618\tTotal Loss: 2.299291\n","---->[25600/60000 (43%)]\tPrecision: 0.109375\tLoss: 2.295904\tSurrogate Loss: 0.003500\tTotal Loss: 2.296254\n","---->[38400/60000 (64%)]\tPrecision: 0.046875\tLoss: 2.312057\tSurrogate Loss: 0.001635\tTotal Loss: 2.312221\n","---->[51200/60000 (85%)]\tPrecision: 0.125000\tLoss: 2.291012\tSurrogate Loss: 0.005474\tTotal Loss: 2.291559\n","----> Epoch 3:\n","---->[0/60000 (0%)]\tPrecision: 0.093750\tLoss: 2.307770\tSurrogate Loss: 0.002665\tTotal Loss: 2.308037\n","---->[12800/60000 (21%)]\tPrecision: 0.093750\tLoss: 2.302190\tSurrogate Loss: 0.003953\tTotal Loss: 2.302585\n","---->[25600/60000 (43%)]\tPrecision: 0.093750\tLoss: 2.285919\tSurrogate Loss: 0.003644\tTotal Loss: 2.286284\n","---->[38400/60000 (64%)]\tPrecision: 0.156250\tLoss: 2.294168\tSurrogate Loss: 0.001537\tTotal Loss: 2.294321\n","---->[51200/60000 (85%)]\tPrecision: 0.093750\tLoss: 2.308867\tSurrogate Loss: 0.002651\tTotal Loss: 2.309132\n","----> Epoch 4:\n","---->[0/60000 (0%)]\tPrecision: 0.046875\tLoss: 2.307499\tSurrogate Loss: 0.002128\tTotal Loss: 2.307712\n","---->[12800/60000 (21%)]\tPrecision: 0.078125\tLoss: 2.311833\tSurrogate Loss: 0.002837\tTotal Loss: 2.312117\n","---->[25600/60000 (43%)]\tPrecision: 0.046875\tLoss: 2.299227\tSurrogate Loss: 0.003675\tTotal Loss: 2.299595\n","---->[38400/60000 (64%)]\tPrecision: 0.000000\tLoss: 2.306390\tSurrogate Loss: 0.005630\tTotal Loss: 2.306953\n","---->[51200/60000 (85%)]\tPrecision: 0.156250\tLoss: 2.301884\tSurrogate Loss: 0.002886\tTotal Loss: 2.302172\n","----> Epoch 5:\n","---->[0/60000 (0%)]\tPrecision: 0.109375\tLoss: 2.301156\tSurrogate Loss: 0.003117\tTotal Loss: 2.301468\n","---->[12800/60000 (21%)]\tPrecision: 0.046875\tLoss: 2.312327\tSurrogate Loss: 0.004210\tTotal Loss: 2.312748\n","---->[25600/60000 (43%)]\tPrecision: 0.093750\tLoss: 2.295421\tSurrogate Loss: 0.003111\tTotal Loss: 2.295732\n","---->[38400/60000 (64%)]\tPrecision: 0.125000\tLoss: 2.303257\tSurrogate Loss: 0.003771\tTotal Loss: 2.303634\n","---->[51200/60000 (85%)]\tPrecision: 0.125000\tLoss: 2.299333\tSurrogate Loss: 0.003341\tTotal Loss: 2.299667\n","----> Epoch 6:\n","---->[0/60000 (0%)]\tPrecision: 0.109375\tLoss: 2.299645\tSurrogate Loss: 0.003880\tTotal Loss: 2.300033\n","---->[12800/60000 (21%)]\tPrecision: 0.140625\tLoss: 2.296243\tSurrogate Loss: 0.001591\tTotal Loss: 2.296402\n","---->[25600/60000 (43%)]\tPrecision: 0.125000\tLoss: 2.304981\tSurrogate Loss: 0.003944\tTotal Loss: 2.305375\n","---->[38400/60000 (64%)]\tPrecision: 0.062500\tLoss: 2.308197\tSurrogate Loss: 0.002769\tTotal Loss: 2.308474\n","---->[51200/60000 (85%)]\tPrecision: 0.093750\tLoss: 2.290996\tSurrogate Loss: 0.002664\tTotal Loss: 2.291262\n","[2.3169422149658203, 2.3007020950317383, 2.299769401550293, 2.300509214401245, 2.3119897842407227, 2.306739568710327, 2.305799961090088, 2.3062164783477783, 2.298746109008789, 2.3056864738464355, 2.302034616470337, 2.299029588699341, 2.2959041595458984, 2.3120572566986084, 2.2910115718841553, 2.3077704906463623, 2.302189826965332, 2.285919189453125, 2.2941675186157227, 2.3088672161102295, 2.3074991703033447, 2.311833143234253, 2.299227237701416, 2.306389808654785, 2.3018836975097656, 2.3011562824249268, 2.3123269081115723, 2.2954208850860596, 2.3032572269439697, 2.299332857131958, 2.299644708633423, 2.2962429523468018, 2.304980754852295, 2.308196783065796, 2.2909955978393555]\n","[2.3169422149658203, 2.3071913719177246, 2.300175905227661, 2.3570332527160645, 2.312593698501587, 2.307161331176758, 2.306478500366211, 2.3065781593322754, 2.2993950843811035, 2.3059496879577637, 2.3022141456604004, 2.2992913722991943, 2.2962541580200195, 2.312220811843872, 2.2915589809417725, 2.308037042617798, 2.3025851249694824, 2.2862837314605713, 2.294321298599243, 2.3091323375701904, 2.3077118396759033, 2.312116861343384, 2.2995946407318115, 2.306952714920044, 2.3021724224090576, 2.3014678955078125, 2.3127479553222656, 2.295732021331787, 2.3036344051361084, 2.2996668815612793, 2.300032615661621, 2.2964019775390625, 2.305375099182129, 2.308473825454712, 2.291261911392212]\n","--> Finished Training Task 6. Starting Test phase:\n","---->Test set 0: Average loss: 0.0005, Accuracy: 9113/10000 (91%)\n","---->Test set 1: Average loss: 0.0360, Accuracy: 962/10000 (10%)\n","---->Test set 2: Average loss: 0.0023, Accuracy: 1032/10000 (10%)\n","---->Test set 3: Average loss: 0.0023, Accuracy: 1032/10000 (10%)\n","---->Test set 4: Average loss: 0.0023, Accuracy: 1032/10000 (10%)\n","---->Test set 5: Average loss: 0.0023, Accuracy: 1032/10000 (10%)\n","---->Test set 6: Average loss: 0.0023, Accuracy: 1032/10000 (10%)\n","---->Test set 7: Average loss: 0.0023, Accuracy: 1032/10000 (10%)\n","---->Test set 8: Average loss: 0.0023, Accuracy: 1032/10000 (10%)\n","---->Test set 9: Average loss: 0.0023, Accuracy: 1032/10000 (10%)\n","--> Training Task 7:\n","----> Epoch 0:\n","---->[0/60000 (0%)]\tPrecision: 0.093750\tLoss: 2.304021\tSurrogate Loss: 0.000000\tTotal Loss: 2.304021\n","---->[12800/60000 (21%)]\tPrecision: 0.109375\tLoss: 2.297345\tSurrogate Loss: 0.005831\tTotal Loss: 2.297928\n","---->[25600/60000 (43%)]\tPrecision: 0.093750\tLoss: 2.304265\tSurrogate Loss: 0.005123\tTotal Loss: 2.304778\n","---->[38400/60000 (64%)]\tPrecision: 0.171875\tLoss: 2.299713\tSurrogate Loss: 0.006859\tTotal Loss: 2.300399\n","---->[51200/60000 (85%)]\tPrecision: 0.078125\tLoss: 2.306712\tSurrogate Loss: 0.005649\tTotal Loss: 2.307277\n","----> Epoch 1:\n","---->[0/60000 (0%)]\tPrecision: 0.171875\tLoss: 2.294420\tSurrogate Loss: 0.001840\tTotal Loss: 2.294604\n","---->[12800/60000 (21%)]\tPrecision: 0.125000\tLoss: 2.299610\tSurrogate Loss: 0.003814\tTotal Loss: 2.299991\n","---->[25600/60000 (43%)]\tPrecision: 0.109375\tLoss: 2.309044\tSurrogate Loss: 0.001451\tTotal Loss: 2.309189\n","---->[38400/60000 (64%)]\tPrecision: 0.093750\tLoss: 2.295837\tSurrogate Loss: 0.005278\tTotal Loss: 2.296365\n","---->[51200/60000 (85%)]\tPrecision: 0.125000\tLoss: 2.297844\tSurrogate Loss: 0.002158\tTotal Loss: 2.298060\n","----> Epoch 2:\n","---->[0/60000 (0%)]\tPrecision: 0.093750\tLoss: 2.301073\tSurrogate Loss: 0.001293\tTotal Loss: 2.301202\n","---->[12800/60000 (21%)]\tPrecision: 0.109375\tLoss: 2.303121\tSurrogate Loss: 0.003225\tTotal Loss: 2.303444\n","---->[25600/60000 (43%)]\tPrecision: 0.156250\tLoss: 2.299700\tSurrogate Loss: 0.002777\tTotal Loss: 2.299978\n","---->[38400/60000 (64%)]\tPrecision: 0.156250\tLoss: 2.297480\tSurrogate Loss: 0.002151\tTotal Loss: 2.297695\n","---->[51200/60000 (85%)]\tPrecision: 0.140625\tLoss: 2.294333\tSurrogate Loss: 0.003454\tTotal Loss: 2.294678\n","----> Epoch 3:\n","---->[0/60000 (0%)]\tPrecision: 0.093750\tLoss: 2.306878\tSurrogate Loss: 0.001486\tTotal Loss: 2.307026\n","---->[12800/60000 (21%)]\tPrecision: 0.203125\tLoss: 2.306729\tSurrogate Loss: 0.004784\tTotal Loss: 2.307207\n","---->[25600/60000 (43%)]\tPrecision: 0.078125\tLoss: 2.310494\tSurrogate Loss: 0.001038\tTotal Loss: 2.310598\n","---->[38400/60000 (64%)]\tPrecision: 0.140625\tLoss: 2.305956\tSurrogate Loss: 0.002356\tTotal Loss: 2.306191\n","---->[51200/60000 (85%)]\tPrecision: 0.046875\tLoss: 2.303978\tSurrogate Loss: 0.001004\tTotal Loss: 2.304079\n","----> Epoch 4:\n","---->[0/60000 (0%)]\tPrecision: 0.156250\tLoss: 2.292209\tSurrogate Loss: 0.002221\tTotal Loss: 2.292431\n","---->[12800/60000 (21%)]\tPrecision: 0.031250\tLoss: 2.311109\tSurrogate Loss: 0.003121\tTotal Loss: 2.311421\n","---->[25600/60000 (43%)]\tPrecision: 0.093750\tLoss: 2.314001\tSurrogate Loss: 0.002580\tTotal Loss: 2.314259\n","---->[38400/60000 (64%)]\tPrecision: 0.125000\tLoss: 2.298452\tSurrogate Loss: 0.002088\tTotal Loss: 2.298661\n","---->[51200/60000 (85%)]\tPrecision: 0.078125\tLoss: 2.297286\tSurrogate Loss: 0.003087\tTotal Loss: 2.297595\n","----> Epoch 5:\n","---->[0/60000 (0%)]\tPrecision: 0.125000\tLoss: 2.296764\tSurrogate Loss: 0.002552\tTotal Loss: 2.297019\n","---->[12800/60000 (21%)]\tPrecision: 0.093750\tLoss: 2.310976\tSurrogate Loss: 0.002760\tTotal Loss: 2.311252\n","---->[25600/60000 (43%)]\tPrecision: 0.125000\tLoss: 2.292903\tSurrogate Loss: 0.002287\tTotal Loss: 2.293132\n","---->[38400/60000 (64%)]\tPrecision: 0.078125\tLoss: 2.306769\tSurrogate Loss: 0.002483\tTotal Loss: 2.307017\n","---->[51200/60000 (85%)]\tPrecision: 0.140625\tLoss: 2.302879\tSurrogate Loss: 0.004666\tTotal Loss: 2.303345\n","----> Epoch 6:\n","---->[0/60000 (0%)]\tPrecision: 0.062500\tLoss: 2.311563\tSurrogate Loss: 0.003616\tTotal Loss: 2.311925\n","---->[12800/60000 (21%)]\tPrecision: 0.078125\tLoss: 2.303879\tSurrogate Loss: 0.001994\tTotal Loss: 2.304078\n","---->[25600/60000 (43%)]\tPrecision: 0.078125\tLoss: 2.304182\tSurrogate Loss: 0.001209\tTotal Loss: 2.304303\n","---->[38400/60000 (64%)]\tPrecision: 0.093750\tLoss: 2.301893\tSurrogate Loss: 0.003727\tTotal Loss: 2.302266\n","---->[51200/60000 (85%)]\tPrecision: 0.093750\tLoss: 2.304882\tSurrogate Loss: 0.002143\tTotal Loss: 2.305096\n","[2.304020881652832, 2.2973451614379883, 2.304265260696411, 2.299713373184204, 2.3067123889923096, 2.2944202423095703, 2.299609661102295, 2.3090438842773438, 2.29583740234375, 2.297844409942627, 2.301072597503662, 2.3031210899353027, 2.2997000217437744, 2.2974801063537598, 2.29433274269104, 2.306877851486206, 2.3067288398742676, 2.3104944229125977, 2.305955648422241, 2.303978204727173, 2.2922089099884033, 2.3111088275909424, 2.3140008449554443, 2.298452138900757, 2.29728627204895, 2.2967641353607178, 2.310976028442383, 2.292903184890747, 2.3067686557769775, 2.3028786182403564, 2.31156325340271, 2.3038790225982666, 2.304182291030884, 2.3018932342529297, 2.3048815727233887]\n","[2.304020881652832, 2.2979283332824707, 2.3047776222229004, 2.3003993034362793, 2.307277202606201, 2.2946043014526367, 2.2999911308288574, 2.3091888427734375, 2.296365261077881, 2.298060178756714, 2.301201820373535, 2.303443670272827, 2.2999777793884277, 2.2976951599121094, 2.294678211212158, 2.3070263862609863, 2.3072073459625244, 2.310598134994507, 2.3061912059783936, 2.3040785789489746, 2.292430877685547, 2.3114209175109863, 2.3142588138580322, 2.29866099357605, 2.2975950241088867, 2.2970192432403564, 2.3112521171569824, 2.2931318283081055, 2.3070168495178223, 2.303345203399658, 2.311924695968628, 2.3040783405303955, 2.3043031692504883, 2.3022658824920654, 2.305095911026001]\n","--> Finished Training Task 7. Starting Test phase:\n","---->Test set 0: Average loss: 0.0005, Accuracy: 9112/10000 (91%)\n","---->Test set 1: Average loss: 0.0352, Accuracy: 959/10000 (10%)\n","---->Test set 2: Average loss: 0.0023, Accuracy: 1032/10000 (10%)\n","---->Test set 3: Average loss: 0.0023, Accuracy: 1032/10000 (10%)\n","---->Test set 4: Average loss: 0.0023, Accuracy: 1032/10000 (10%)\n","---->Test set 5: Average loss: 0.0023, Accuracy: 1032/10000 (10%)\n","---->Test set 6: Average loss: 0.0023, Accuracy: 1032/10000 (10%)\n","---->Test set 7: Average loss: 0.0023, Accuracy: 1032/10000 (10%)\n","---->Test set 8: Average loss: 0.0023, Accuracy: 1032/10000 (10%)\n","---->Test set 9: Average loss: 0.0023, Accuracy: 1032/10000 (10%)\n","--> Training Task 8:\n","----> Epoch 0:\n","---->[0/60000 (0%)]\tPrecision: 0.078125\tLoss: 2.308605\tSurrogate Loss: 0.000000\tTotal Loss: 2.308605\n","---->[12800/60000 (21%)]\tPrecision: 0.062500\tLoss: 2.299257\tSurrogate Loss: 0.017447\tTotal Loss: 2.301002\n","---->[25600/60000 (43%)]\tPrecision: 0.125000\tLoss: 2.306422\tSurrogate Loss: 0.005750\tTotal Loss: 2.306997\n","---->[38400/60000 (64%)]\tPrecision: 0.093750\tLoss: 2.303774\tSurrogate Loss: 0.002448\tTotal Loss: 2.304019\n","---->[51200/60000 (85%)]\tPrecision: 0.093750\tLoss: 2.300046\tSurrogate Loss: 0.001853\tTotal Loss: 2.300232\n","----> Epoch 1:\n","---->[0/60000 (0%)]\tPrecision: 0.093750\tLoss: 2.306962\tSurrogate Loss: 0.004415\tTotal Loss: 2.307404\n","---->[12800/60000 (21%)]\tPrecision: 0.062500\tLoss: 2.304980\tSurrogate Loss: 0.002600\tTotal Loss: 2.305240\n","---->[25600/60000 (43%)]\tPrecision: 0.031250\tLoss: 2.316244\tSurrogate Loss: 0.002985\tTotal Loss: 2.316542\n","---->[38400/60000 (64%)]\tPrecision: 0.125000\tLoss: 2.296287\tSurrogate Loss: 0.003393\tTotal Loss: 2.296626\n","---->[51200/60000 (85%)]\tPrecision: 0.078125\tLoss: 2.301797\tSurrogate Loss: 0.002070\tTotal Loss: 2.302004\n","----> Epoch 2:\n","---->[0/60000 (0%)]\tPrecision: 0.093750\tLoss: 2.315950\tSurrogate Loss: 0.003638\tTotal Loss: 2.316314\n","---->[12800/60000 (21%)]\tPrecision: 0.093750\tLoss: 2.306082\tSurrogate Loss: 0.001976\tTotal Loss: 2.306280\n","---->[25600/60000 (43%)]\tPrecision: 0.062500\tLoss: 2.306499\tSurrogate Loss: 0.003117\tTotal Loss: 2.306810\n","---->[38400/60000 (64%)]\tPrecision: 0.109375\tLoss: 2.299497\tSurrogate Loss: 0.001621\tTotal Loss: 2.299659\n","---->[51200/60000 (85%)]\tPrecision: 0.187500\tLoss: 2.294292\tSurrogate Loss: 0.004468\tTotal Loss: 2.294739\n","----> Epoch 3:\n","---->[0/60000 (0%)]\tPrecision: 0.062500\tLoss: 2.304609\tSurrogate Loss: 0.004946\tTotal Loss: 2.305103\n","---->[12800/60000 (21%)]\tPrecision: 0.109375\tLoss: 2.305327\tSurrogate Loss: 0.002989\tTotal Loss: 2.305626\n","---->[25600/60000 (43%)]\tPrecision: 0.187500\tLoss: 2.292726\tSurrogate Loss: 0.003566\tTotal Loss: 2.293083\n","---->[38400/60000 (64%)]\tPrecision: 0.140625\tLoss: 2.308906\tSurrogate Loss: 0.001668\tTotal Loss: 2.309073\n","---->[51200/60000 (85%)]\tPrecision: 0.125000\tLoss: 2.304153\tSurrogate Loss: 0.001812\tTotal Loss: 2.304335\n","----> Epoch 4:\n","---->[0/60000 (0%)]\tPrecision: 0.078125\tLoss: 2.292885\tSurrogate Loss: 0.003687\tTotal Loss: 2.293253\n","---->[12800/60000 (21%)]\tPrecision: 0.125000\tLoss: 2.299827\tSurrogate Loss: 0.003082\tTotal Loss: 2.300135\n","---->[25600/60000 (43%)]\tPrecision: 0.062500\tLoss: 2.310284\tSurrogate Loss: 0.002444\tTotal Loss: 2.310528\n","---->[38400/60000 (64%)]\tPrecision: 0.156250\tLoss: 2.300314\tSurrogate Loss: 0.003829\tTotal Loss: 2.300697\n","---->[51200/60000 (85%)]\tPrecision: 0.046875\tLoss: 2.311958\tSurrogate Loss: 0.003591\tTotal Loss: 2.312317\n","----> Epoch 5:\n","---->[0/60000 (0%)]\tPrecision: 0.078125\tLoss: 2.304076\tSurrogate Loss: 0.003017\tTotal Loss: 2.304377\n","---->[12800/60000 (21%)]\tPrecision: 0.187500\tLoss: 2.293460\tSurrogate Loss: 0.003374\tTotal Loss: 2.293797\n","---->[25600/60000 (43%)]\tPrecision: 0.078125\tLoss: 2.305252\tSurrogate Loss: 0.001041\tTotal Loss: 2.305357\n","---->[38400/60000 (64%)]\tPrecision: 0.062500\tLoss: 2.313504\tSurrogate Loss: 0.002185\tTotal Loss: 2.313722\n","---->[51200/60000 (85%)]\tPrecision: 0.171875\tLoss: 2.288649\tSurrogate Loss: 0.002768\tTotal Loss: 2.288926\n","----> Epoch 6:\n","---->[0/60000 (0%)]\tPrecision: 0.078125\tLoss: 2.311354\tSurrogate Loss: 0.002977\tTotal Loss: 2.311652\n","---->[12800/60000 (21%)]\tPrecision: 0.140625\tLoss: 2.301081\tSurrogate Loss: 0.002334\tTotal Loss: 2.301314\n","---->[25600/60000 (43%)]\tPrecision: 0.125000\tLoss: 2.308567\tSurrogate Loss: 0.001639\tTotal Loss: 2.308730\n","---->[38400/60000 (64%)]\tPrecision: 0.046875\tLoss: 2.301967\tSurrogate Loss: 0.003169\tTotal Loss: 2.302284\n","---->[51200/60000 (85%)]\tPrecision: 0.046875\tLoss: 2.301227\tSurrogate Loss: 0.003125\tTotal Loss: 2.301539\n","[2.3086047172546387, 2.2992568016052246, 2.3064215183258057, 2.303774118423462, 2.300046443939209, 2.306962013244629, 2.3049802780151367, 2.316243886947632, 2.2962870597839355, 2.3017971515655518, 2.315950393676758, 2.306082248687744, 2.3064987659454346, 2.299496650695801, 2.2942917346954346, 2.3046085834503174, 2.3053269386291504, 2.2927262783050537, 2.308906078338623, 2.3041534423828125, 2.2928848266601562, 2.2998273372650146, 2.310283899307251, 2.30031418800354, 2.311957836151123, 2.3040757179260254, 2.2934601306915283, 2.3052523136138916, 2.3135035037994385, 2.288648843765259, 2.311354398727417, 2.3010807037353516, 2.3085665702819824, 2.3019673824310303, 2.301226854324341]\n","[2.3086047172546387, 2.30100154876709, 2.3069965839385986, 2.304018974304199, 2.300231695175171, 2.307403564453125, 2.3052401542663574, 2.316542387008667, 2.296626329421997, 2.302004098892212, 2.316314220428467, 2.3062798976898193, 2.3068103790283203, 2.29965877532959, 2.294738531112671, 2.305103302001953, 2.3056259155273438, 2.2930829524993896, 2.309072971343994, 2.3043346405029297, 2.2932534217834473, 2.300135374069214, 2.31052827835083, 2.300697088241577, 2.31231689453125, 2.304377317428589, 2.293797492980957, 2.305356502532959, 2.3137218952178955, 2.2889256477355957, 2.3116519451141357, 2.301314115524292, 2.308730363845825, 2.3022842407226562, 2.301539421081543]\n","--> Finished Training Task 8. Starting Test phase:\n","---->Test set 0: Average loss: 0.0005, Accuracy: 9114/10000 (91%)\n","---->Test set 1: Average loss: 0.0323, Accuracy: 957/10000 (10%)\n","---->Test set 2: Average loss: 0.0023, Accuracy: 1032/10000 (10%)\n","---->Test set 3: Average loss: 0.0023, Accuracy: 1032/10000 (10%)\n","---->Test set 4: Average loss: 0.0023, Accuracy: 1032/10000 (10%)\n","---->Test set 5: Average loss: 0.0023, Accuracy: 1032/10000 (10%)\n","---->Test set 6: Average loss: 0.0023, Accuracy: 1032/10000 (10%)\n","---->Test set 7: Average loss: 0.0023, Accuracy: 1032/10000 (10%)\n","---->Test set 8: Average loss: 0.0023, Accuracy: 1032/10000 (10%)\n","---->Test set 9: Average loss: 0.0023, Accuracy: 1032/10000 (10%)\n","--> Training Task 9:\n","----> Epoch 0:\n","---->[0/60000 (0%)]\tPrecision: 0.078125\tLoss: 2.297983\tSurrogate Loss: 0.000000\tTotal Loss: 2.297983\n","---->[12800/60000 (21%)]\tPrecision: 0.109375\tLoss: 2.300435\tSurrogate Loss: 22.559223\tTotal Loss: 4.556357\n","---->[25600/60000 (43%)]\tPrecision: 0.093750\tLoss: 2.301636\tSurrogate Loss: 3.492242\tTotal Loss: 2.650860\n","---->[38400/60000 (64%)]\tPrecision: 0.109375\tLoss: 2.309524\tSurrogate Loss: 1.709260\tTotal Loss: 2.480450\n","---->[51200/60000 (85%)]\tPrecision: 0.125000\tLoss: 2.298092\tSurrogate Loss: 1.051970\tTotal Loss: 2.403289\n","----> Epoch 1:\n","---->[0/60000 (0%)]\tPrecision: 0.125000\tLoss: 2.295430\tSurrogate Loss: 0.803535\tTotal Loss: 2.375783\n","---->[12800/60000 (21%)]\tPrecision: 0.093750\tLoss: 2.298324\tSurrogate Loss: 0.570242\tTotal Loss: 2.355348\n","---->[25600/60000 (43%)]\tPrecision: 0.062500\tLoss: 2.307503\tSurrogate Loss: 0.423260\tTotal Loss: 2.349829\n","---->[38400/60000 (64%)]\tPrecision: 0.093750\tLoss: 2.313918\tSurrogate Loss: 0.322034\tTotal Loss: 2.346121\n","---->[51200/60000 (85%)]\tPrecision: 0.093750\tLoss: 2.297111\tSurrogate Loss: 0.250668\tTotal Loss: 2.322178\n","----> Epoch 2:\n","---->[0/60000 (0%)]\tPrecision: 0.156250\tLoss: 2.300240\tSurrogate Loss: 0.212838\tTotal Loss: 2.321524\n","---->[12800/60000 (21%)]\tPrecision: 0.031250\tLoss: 2.318602\tSurrogate Loss: 0.170236\tTotal Loss: 2.335625\n","---->[25600/60000 (43%)]\tPrecision: 0.046875\tLoss: 2.309662\tSurrogate Loss: 0.136649\tTotal Loss: 2.323327\n","---->[38400/60000 (64%)]\tPrecision: 0.031250\tLoss: 2.320870\tSurrogate Loss: 0.110937\tTotal Loss: 2.331964\n","---->[51200/60000 (85%)]\tPrecision: 0.187500\tLoss: 2.298169\tSurrogate Loss: 0.090487\tTotal Loss: 2.307218\n","----> Epoch 3:\n","---->[0/60000 (0%)]\tPrecision: 0.156250\tLoss: 2.301139\tSurrogate Loss: 0.078160\tTotal Loss: 2.308954\n","---->[12800/60000 (21%)]\tPrecision: 0.093750\tLoss: 2.311903\tSurrogate Loss: 0.064944\tTotal Loss: 2.318398\n","---->[25600/60000 (43%)]\tPrecision: 0.078125\tLoss: 2.308399\tSurrogate Loss: 0.054845\tTotal Loss: 2.313883\n","---->[38400/60000 (64%)]\tPrecision: 0.078125\tLoss: 2.298118\tSurrogate Loss: 0.043570\tTotal Loss: 2.302475\n","---->[51200/60000 (85%)]\tPrecision: 0.109375\tLoss: 2.299177\tSurrogate Loss: 0.035619\tTotal Loss: 2.302739\n","----> Epoch 4:\n","---->[0/60000 (0%)]\tPrecision: 0.187500\tLoss: 2.294178\tSurrogate Loss: 0.032647\tTotal Loss: 2.297443\n","---->[12800/60000 (21%)]\tPrecision: 0.062500\tLoss: 2.302490\tSurrogate Loss: 0.026774\tTotal Loss: 2.305168\n","---->[25600/60000 (43%)]\tPrecision: 0.156250\tLoss: 2.302752\tSurrogate Loss: 0.022725\tTotal Loss: 2.305025\n","---->[38400/60000 (64%)]\tPrecision: 0.109375\tLoss: 2.300927\tSurrogate Loss: 0.020123\tTotal Loss: 2.302940\n","---->[51200/60000 (85%)]\tPrecision: 0.078125\tLoss: 2.299862\tSurrogate Loss: 0.016069\tTotal Loss: 2.301469\n","----> Epoch 5:\n","---->[0/60000 (0%)]\tPrecision: 0.109375\tLoss: 2.302320\tSurrogate Loss: 0.014273\tTotal Loss: 2.303747\n","---->[12800/60000 (21%)]\tPrecision: 0.125000\tLoss: 2.300467\tSurrogate Loss: 0.012877\tTotal Loss: 2.301755\n","---->[25600/60000 (43%)]\tPrecision: 0.031250\tLoss: 2.307969\tSurrogate Loss: 0.010438\tTotal Loss: 2.309013\n","---->[38400/60000 (64%)]\tPrecision: 0.125000\tLoss: 2.298921\tSurrogate Loss: 0.008552\tTotal Loss: 2.299776\n","---->[51200/60000 (85%)]\tPrecision: 0.140625\tLoss: 2.301636\tSurrogate Loss: 0.009721\tTotal Loss: 2.302608\n","----> Epoch 6:\n","---->[0/60000 (0%)]\tPrecision: 0.062500\tLoss: 2.302828\tSurrogate Loss: 0.007409\tTotal Loss: 2.303569\n","---->[12800/60000 (21%)]\tPrecision: 0.093750\tLoss: 2.303070\tSurrogate Loss: 0.005092\tTotal Loss: 2.303579\n","---->[25600/60000 (43%)]\tPrecision: 0.093750\tLoss: 2.301223\tSurrogate Loss: 0.007599\tTotal Loss: 2.301983\n","---->[38400/60000 (64%)]\tPrecision: 0.125000\tLoss: 2.301181\tSurrogate Loss: 0.006827\tTotal Loss: 2.301863\n","---->[51200/60000 (85%)]\tPrecision: 0.109375\tLoss: 2.308789\tSurrogate Loss: 0.004265\tTotal Loss: 2.309216\n","[2.297982931137085, 2.3004353046417236, 2.3016357421875, 2.309523582458496, 2.2980921268463135, 2.2954297065734863, 2.2983241081237793, 2.3075034618377686, 2.313917875289917, 2.2971112728118896, 2.3002398014068604, 2.3186018466949463, 2.309662103652954, 2.3208699226379395, 2.2981691360473633, 2.3011386394500732, 2.311903238296509, 2.308398723602295, 2.2981178760528564, 2.299177408218384, 2.294178009033203, 2.302490472793579, 2.3027524948120117, 2.3009274005889893, 2.2998621463775635, 2.3023202419281006, 2.300467014312744, 2.307969093322754, 2.2989211082458496, 2.301635503768921, 2.302827835083008, 2.303069829940796, 2.3012232780456543, 2.301180601119995, 2.3087892532348633]\n","[2.297982931137085, 4.556357383728027, 2.650859832763672, 2.480449676513672, 2.4032890796661377, 2.3757832050323486, 2.3553483486175537, 2.3498294353485107, 2.346121311187744, 2.3221781253814697, 2.321523666381836, 2.335625410079956, 2.32332706451416, 2.331963539123535, 2.307217836380005, 2.3089544773101807, 2.3183975219726562, 2.3138833045959473, 2.3024749755859375, 2.302739381790161, 2.297442674636841, 2.3051679134368896, 2.305025100708008, 2.3029396533966064, 2.301469087600708, 2.3037474155426025, 2.3017547130584717, 2.3090128898620605, 2.299776315689087, 2.302607536315918, 2.3035688400268555, 2.303579092025757, 2.301983118057251, 2.301863431930542, 2.309215784072876]\n","--> Finished Training Task 9. Starting Test phase:\n","---->Test set 0: Average loss: 0.0005, Accuracy: 9114/10000 (91%)\n","---->Test set 1: Average loss: 0.0312, Accuracy: 955/10000 (10%)\n","---->Test set 2: Average loss: 0.0023, Accuracy: 1032/10000 (10%)\n","---->Test set 3: Average loss: 0.0023, Accuracy: 1032/10000 (10%)\n","---->Test set 4: Average loss: 0.0023, Accuracy: 1032/10000 (10%)\n","---->Test set 5: Average loss: 0.0023, Accuracy: 1032/10000 (10%)\n","---->Test set 6: Average loss: 0.0023, Accuracy: 1032/10000 (10%)\n","---->Test set 7: Average loss: 0.0023, Accuracy: 1032/10000 (10%)\n","---->Test set 8: Average loss: 0.0023, Accuracy: 1032/10000 (10%)\n","---->Test set 9: Average loss: 0.0023, Accuracy: 1032/10000 (10%)\n","Accuracy: 0.18324999999999997\n","Confusion matrix:\n","0.0882,0.1071,0.0915,0.118,0.1085,0.0606,0.0731,0.0982,0.073,0.1115\n","0.9569,0.1415,0.1122,0.0993,0.1068,0.1461,0.0639,0.0834,0.1037,0.0927\n","0.9074,0.0958,0.0956,0.098,0.0954,0.107,0.0741,0.1129,0.1022,0.0691\n","0.9082,0.0958,0.1032,0.1098,0.1027,0.1032,0.1033,0.104,0.1055,0.102\n","0.9107,0.0957,0.1032,0.1032,0.1031,0.1033,0.1032,0.1032,0.1032,0.1032\n","0.9108,0.0957,0.1032,0.1032,0.1032,0.1032,0.1032,0.1032,0.1032,0.1032\n","0.9107,0.0957,0.1032,0.1032,0.1032,0.1032,0.1032,0.1032,0.1032,0.1032\n","0.9113,0.0962,0.1032,0.1032,0.1032,0.1032,0.1032,0.1032,0.1032,0.1032\n","0.9112,0.0959,0.1032,0.1032,0.1032,0.1032,0.1032,0.1032,0.1032,0.1032\n","0.9114,0.0957,0.1032,0.1032,0.1032,0.1032,0.1032,0.1032,0.1032,0.1032\n","0.9114,0.0955,0.1032,0.1032,0.1032,0.1032,0.1032,0.1032,0.1032,0.1032\n","--> Training:\n","---->Test set 0: Average loss: 0.2131, Accuracy: 997/10000 (10%)\n","---->Test set 1: Average loss: 0.2094, Accuracy: 1013/10000 (10%)\n","---->Test set 2: Average loss: 0.2582, Accuracy: 754/10000 (8%)\n","---->Test set 3: Average loss: 0.1983, Accuracy: 893/10000 (9%)\n","---->Test set 4: Average loss: 0.2016, Accuracy: 975/10000 (10%)\n","---->Test set 5: Average loss: 0.1852, Accuracy: 820/10000 (8%)\n","---->Test set 6: Average loss: 0.2127, Accuracy: 802/10000 (8%)\n","---->Test set 7: Average loss: 0.1827, Accuracy: 610/10000 (6%)\n","---->Test set 8: Average loss: 0.2139, Accuracy: 904/10000 (9%)\n","---->Test set 9: Average loss: 0.2103, Accuracy: 804/10000 (8%)\n","--> Training Task 0:\n","----> Epoch 0:\n","---->[0/60000 (0%)]\tPrecision: 0.187500\tLoss: 194.915512\tSurrogate Loss: 0.000000\tTotal Loss: 194.915512\n","---->[12800/60000 (21%)]\tPrecision: 0.859375\tLoss: 0.309189\tSurrogate Loss: 0.000000\tTotal Loss: 0.309189\n","---->[25600/60000 (43%)]\tPrecision: 0.859375\tLoss: 0.447376\tSurrogate Loss: 0.000000\tTotal Loss: 0.447376\n","---->[38400/60000 (64%)]\tPrecision: 0.921875\tLoss: 0.389971\tSurrogate Loss: 0.000000\tTotal Loss: 0.389971\n","---->[51200/60000 (85%)]\tPrecision: 0.984375\tLoss: 0.125912\tSurrogate Loss: 0.000000\tTotal Loss: 0.125912\n","----> Epoch 1:\n","---->[0/60000 (0%)]\tPrecision: 0.937500\tLoss: 0.157947\tSurrogate Loss: 0.000000\tTotal Loss: 0.157947\n","---->[12800/60000 (21%)]\tPrecision: 0.890625\tLoss: 0.337446\tSurrogate Loss: 0.000000\tTotal Loss: 0.337446\n","---->[25600/60000 (43%)]\tPrecision: 0.921875\tLoss: 0.175476\tSurrogate Loss: 0.000000\tTotal Loss: 0.175476\n","---->[38400/60000 (64%)]\tPrecision: 0.921875\tLoss: 0.209942\tSurrogate Loss: 0.000000\tTotal Loss: 0.209942\n","---->[51200/60000 (85%)]\tPrecision: 0.968750\tLoss: 0.097496\tSurrogate Loss: 0.000000\tTotal Loss: 0.097496\n","----> Epoch 2:\n","---->[0/60000 (0%)]\tPrecision: 0.968750\tLoss: 0.157209\tSurrogate Loss: 0.000000\tTotal Loss: 0.157209\n","---->[12800/60000 (21%)]\tPrecision: 0.968750\tLoss: 0.071552\tSurrogate Loss: 0.000000\tTotal Loss: 0.071552\n","---->[25600/60000 (43%)]\tPrecision: 0.921875\tLoss: 0.257327\tSurrogate Loss: 0.000000\tTotal Loss: 0.257327\n","---->[38400/60000 (64%)]\tPrecision: 0.984375\tLoss: 0.046775\tSurrogate Loss: 0.000000\tTotal Loss: 0.046775\n","---->[51200/60000 (85%)]\tPrecision: 0.984375\tLoss: 0.115521\tSurrogate Loss: 0.000000\tTotal Loss: 0.115521\n","----> Epoch 3:\n","---->[0/60000 (0%)]\tPrecision: 0.968750\tLoss: 0.087793\tSurrogate Loss: 0.000000\tTotal Loss: 0.087793\n","---->[12800/60000 (21%)]\tPrecision: 0.984375\tLoss: 0.048669\tSurrogate Loss: 0.000000\tTotal Loss: 0.048669\n","---->[25600/60000 (43%)]\tPrecision: 1.000000\tLoss: 0.009242\tSurrogate Loss: 0.000000\tTotal Loss: 0.009242\n","---->[38400/60000 (64%)]\tPrecision: 0.968750\tLoss: 0.105994\tSurrogate Loss: 0.000000\tTotal Loss: 0.105994\n","---->[51200/60000 (85%)]\tPrecision: 1.000000\tLoss: 0.046122\tSurrogate Loss: 0.000000\tTotal Loss: 0.046122\n","----> Epoch 4:\n","---->[0/60000 (0%)]\tPrecision: 0.984375\tLoss: 0.069105\tSurrogate Loss: 0.000000\tTotal Loss: 0.069105\n","---->[12800/60000 (21%)]\tPrecision: 0.921875\tLoss: 0.227899\tSurrogate Loss: 0.000000\tTotal Loss: 0.227899\n","---->[25600/60000 (43%)]\tPrecision: 0.968750\tLoss: 0.112769\tSurrogate Loss: 0.000000\tTotal Loss: 0.112769\n","---->[38400/60000 (64%)]\tPrecision: 0.921875\tLoss: 0.236466\tSurrogate Loss: 0.000000\tTotal Loss: 0.236466\n","---->[51200/60000 (85%)]\tPrecision: 1.000000\tLoss: 0.032625\tSurrogate Loss: 0.000000\tTotal Loss: 0.032625\n","----> Epoch 5:\n","---->[0/60000 (0%)]\tPrecision: 0.937500\tLoss: 0.192270\tSurrogate Loss: 0.000000\tTotal Loss: 0.192270\n","---->[12800/60000 (21%)]\tPrecision: 0.953125\tLoss: 0.186195\tSurrogate Loss: 0.000000\tTotal Loss: 0.186195\n","---->[25600/60000 (43%)]\tPrecision: 0.968750\tLoss: 0.151460\tSurrogate Loss: 0.000000\tTotal Loss: 0.151460\n","---->[38400/60000 (64%)]\tPrecision: 0.953125\tLoss: 0.165227\tSurrogate Loss: 0.000000\tTotal Loss: 0.165227\n","---->[51200/60000 (85%)]\tPrecision: 1.000000\tLoss: 0.015487\tSurrogate Loss: 0.000000\tTotal Loss: 0.015487\n","----> Epoch 6:\n","---->[0/60000 (0%)]\tPrecision: 0.968750\tLoss: 0.066593\tSurrogate Loss: 0.000000\tTotal Loss: 0.066593\n","---->[12800/60000 (21%)]\tPrecision: 0.984375\tLoss: 0.073859\tSurrogate Loss: 0.000000\tTotal Loss: 0.073859\n","---->[25600/60000 (43%)]\tPrecision: 0.968750\tLoss: 0.101353\tSurrogate Loss: 0.000000\tTotal Loss: 0.101353\n","---->[38400/60000 (64%)]\tPrecision: 0.937500\tLoss: 0.313283\tSurrogate Loss: 0.000000\tTotal Loss: 0.313283\n","---->[51200/60000 (85%)]\tPrecision: 0.953125\tLoss: 0.306384\tSurrogate Loss: 0.000000\tTotal Loss: 0.306384\n","----> Epoch 7:\n","---->[0/60000 (0%)]\tPrecision: 0.984375\tLoss: 0.226293\tSurrogate Loss: 0.000000\tTotal Loss: 0.226293\n","---->[12800/60000 (21%)]\tPrecision: 0.968750\tLoss: 0.187854\tSurrogate Loss: 0.000000\tTotal Loss: 0.187854\n","---->[25600/60000 (43%)]\tPrecision: 0.906250\tLoss: 0.349123\tSurrogate Loss: 0.000000\tTotal Loss: 0.349123\n","---->[38400/60000 (64%)]\tPrecision: 0.953125\tLoss: 0.127233\tSurrogate Loss: 0.000000\tTotal Loss: 0.127233\n","---->[51200/60000 (85%)]\tPrecision: 0.937500\tLoss: 0.429706\tSurrogate Loss: 0.000000\tTotal Loss: 0.429706\n","[194.91551208496094, 0.30918920040130615, 0.4473762512207031, 0.3899705111980438, 0.1259116381406784, 0.1579471081495285, 0.337446004152298, 0.17547613382339478, 0.20994195342063904, 0.09749573469161987, 0.15720929205417633, 0.07155219465494156, 0.2573269009590149, 0.04677537456154823, 0.11552124470472336, 0.08779284358024597, 0.04866937920451164, 0.009241531603038311, 0.1059943363070488, 0.046121612191200256, 0.06910526752471924, 0.2278985232114792, 0.11276949942111969, 0.23646602034568787, 0.03262530267238617, 0.1922701895236969, 0.1861950308084488, 0.1514597237110138, 0.16522710025310516, 0.015486611053347588, 0.06659264117479324, 0.07385917752981186, 0.1013527438044548, 0.3132833242416382, 0.306384414434433, 0.22629310190677643, 0.18785442411899567, 0.349123477935791, 0.127232626080513, 0.42970576882362366]\n","[194.91551208496094, 0.30918920040130615, 0.4473762512207031, 0.3899705111980438, 0.1259116381406784, 0.1579471081495285, 0.337446004152298, 0.17547613382339478, 0.20994195342063904, 0.09749573469161987, 0.15720929205417633, 0.07155219465494156, 0.2573269009590149, 0.04677537456154823, 0.11552124470472336, 0.08779284358024597, 0.04866937920451164, 0.009241531603038311, 0.1059943363070488, 0.046121612191200256, 0.06910526752471924, 0.2278985232114792, 0.11276949942111969, 0.23646602034568787, 0.03262530267238617, 0.1922701895236969, 0.1861950308084488, 0.1514597237110138, 0.16522710025310516, 0.015486611053347588, 0.06659264117479324, 0.07385917752981186, 0.1013527438044548, 0.3132833242416382, 0.306384414434433, 0.22629310190677643, 0.18785442411899567, 0.349123477935791, 0.127232626080513, 0.42970576882362366]\n","--> Finished Training Task 0. Starting Test phase:\n","---->Test set 0: Average loss: 0.0002, Accuracy: 9435/10000 (94%)\n","---->Test set 1: Average loss: 0.0075, Accuracy: 937/10000 (9%)\n","---->Test set 2: Average loss: 0.0064, Accuracy: 955/10000 (10%)\n","---->Test set 3: Average loss: 0.0072, Accuracy: 1142/10000 (11%)\n","---->Test set 4: Average loss: 0.0058, Accuracy: 1244/10000 (12%)\n","---->Test set 5: Average loss: 0.0045, Accuracy: 956/10000 (10%)\n","---->Test set 6: Average loss: 0.0061, Accuracy: 963/10000 (10%)\n","---->Test set 7: Average loss: 0.0085, Accuracy: 1137/10000 (11%)\n","---->Test set 8: Average loss: 0.0089, Accuracy: 874/10000 (9%)\n","---->Test set 9: Average loss: 0.0041, Accuracy: 952/10000 (10%)\n","--> Training Task 1:\n","----> Epoch 0:\n","---->[0/60000 (0%)]\tPrecision: 0.062500\tLoss: 7.271019\tSurrogate Loss: 0.000000\tTotal Loss: 7.271019\n","---->[12800/60000 (21%)]\tPrecision: 0.125000\tLoss: 2.308159\tSurrogate Loss: 0.301423\tTotal Loss: 2.338302\n","---->[25600/60000 (43%)]\tPrecision: 0.093750\tLoss: 2.311824\tSurrogate Loss: 0.162828\tTotal Loss: 2.328107\n","---->[38400/60000 (64%)]\tPrecision: 0.078125\tLoss: 2.316888\tSurrogate Loss: 0.114735\tTotal Loss: 2.328361\n","---->[51200/60000 (85%)]\tPrecision: 0.093750\tLoss: 2.311786\tSurrogate Loss: 0.105828\tTotal Loss: 2.322369\n","----> Epoch 1:\n","---->[0/60000 (0%)]\tPrecision: 0.140625\tLoss: 2.304794\tSurrogate Loss: 0.101628\tTotal Loss: 2.314956\n","---->[12800/60000 (21%)]\tPrecision: 0.062500\tLoss: 2.321306\tSurrogate Loss: 0.337327\tTotal Loss: 2.355038\n","---->[25600/60000 (43%)]\tPrecision: 0.140625\tLoss: 2.309156\tSurrogate Loss: 0.154112\tTotal Loss: 2.324567\n","---->[38400/60000 (64%)]\tPrecision: 0.062500\tLoss: 2.302444\tSurrogate Loss: 0.117295\tTotal Loss: 2.314173\n","---->[51200/60000 (85%)]\tPrecision: 0.125000\tLoss: 2.321333\tSurrogate Loss: 0.095785\tTotal Loss: 2.330912\n","----> Epoch 2:\n","---->[0/60000 (0%)]\tPrecision: 0.109375\tLoss: 2.299579\tSurrogate Loss: 23.288700\tTotal Loss: 4.628449\n","---->[12800/60000 (21%)]\tPrecision: 0.140625\tLoss: 2.297647\tSurrogate Loss: 4.651518\tTotal Loss: 2.762799\n","---->[25600/60000 (43%)]\tPrecision: 0.093750\tLoss: 2.303972\tSurrogate Loss: 2.302789\tTotal Loss: 2.534251\n","---->[38400/60000 (64%)]\tPrecision: 0.062500\tLoss: 2.299528\tSurrogate Loss: 1.531563\tTotal Loss: 2.452684\n","---->[51200/60000 (85%)]\tPrecision: 0.093750\tLoss: 2.303745\tSurrogate Loss: 1.145984\tTotal Loss: 2.418344\n","----> Epoch 3:\n","---->[0/60000 (0%)]\tPrecision: 0.046875\tLoss: 2.313748\tSurrogate Loss: 0.985704\tTotal Loss: 2.412318\n","---->[12800/60000 (21%)]\tPrecision: 0.109375\tLoss: 2.312159\tSurrogate Loss: 0.799931\tTotal Loss: 2.392152\n","---->[25600/60000 (43%)]\tPrecision: 0.093750\tLoss: 2.299965\tSurrogate Loss: 0.692804\tTotal Loss: 2.369246\n","---->[38400/60000 (64%)]\tPrecision: 0.093750\tLoss: 2.309214\tSurrogate Loss: 0.588546\tTotal Loss: 2.368068\n","---->[51200/60000 (85%)]\tPrecision: 0.093750\tLoss: 2.312455\tSurrogate Loss: 0.517797\tTotal Loss: 2.364235\n","----> Epoch 4:\n","---->[0/60000 (0%)]\tPrecision: 0.125000\tLoss: 2.297715\tSurrogate Loss: 0.487794\tTotal Loss: 2.346494\n","---->[12800/60000 (21%)]\tPrecision: 0.078125\tLoss: 2.292001\tSurrogate Loss: 0.425660\tTotal Loss: 2.334567\n","---->[25600/60000 (43%)]\tPrecision: 0.109375\tLoss: 2.310389\tSurrogate Loss: 0.377639\tTotal Loss: 2.348153\n","---->[38400/60000 (64%)]\tPrecision: 0.140625\tLoss: 2.297022\tSurrogate Loss: 0.334287\tTotal Loss: 2.330450\n","---->[51200/60000 (85%)]\tPrecision: 0.046875\tLoss: 2.310163\tSurrogate Loss: 0.310666\tTotal Loss: 2.341230\n","----> Epoch 5:\n","---->[0/60000 (0%)]\tPrecision: 0.046875\tLoss: 2.298934\tSurrogate Loss: 0.283180\tTotal Loss: 2.327252\n","---->[12800/60000 (21%)]\tPrecision: 0.140625\tLoss: 2.297147\tSurrogate Loss: 0.251611\tTotal Loss: 2.322308\n","---->[25600/60000 (43%)]\tPrecision: 0.078125\tLoss: 2.323736\tSurrogate Loss: 0.235725\tTotal Loss: 2.347308\n","---->[38400/60000 (64%)]\tPrecision: 0.109375\tLoss: 2.307612\tSurrogate Loss: 0.204728\tTotal Loss: 2.328085\n","---->[51200/60000 (85%)]\tPrecision: 0.109375\tLoss: 2.305437\tSurrogate Loss: 0.199182\tTotal Loss: 2.325355\n","----> Epoch 6:\n","---->[0/60000 (0%)]\tPrecision: 0.031250\tLoss: 2.299843\tSurrogate Loss: 0.181383\tTotal Loss: 2.317981\n","---->[12800/60000 (21%)]\tPrecision: 0.187500\tLoss: 2.296148\tSurrogate Loss: 0.172946\tTotal Loss: 2.313443\n","---->[25600/60000 (43%)]\tPrecision: 0.062500\tLoss: 2.309571\tSurrogate Loss: 0.160823\tTotal Loss: 2.325654\n","---->[38400/60000 (64%)]\tPrecision: 0.062500\tLoss: 2.299785\tSurrogate Loss: 0.149377\tTotal Loss: 2.314723\n","---->[51200/60000 (85%)]\tPrecision: 0.109375\tLoss: 2.308476\tSurrogate Loss: 0.136435\tTotal Loss: 2.322119\n","----> Epoch 7:\n","---->[0/60000 (0%)]\tPrecision: 0.031250\tLoss: 2.307072\tSurrogate Loss: 0.136214\tTotal Loss: 2.320694\n","---->[12800/60000 (21%)]\tPrecision: 0.109375\tLoss: 2.297497\tSurrogate Loss: 0.122561\tTotal Loss: 2.309753\n","---->[25600/60000 (43%)]\tPrecision: 0.078125\tLoss: 2.297912\tSurrogate Loss: 0.126521\tTotal Loss: 2.310564\n","---->[38400/60000 (64%)]\tPrecision: 0.140625\tLoss: 2.305980\tSurrogate Loss: 0.120416\tTotal Loss: 2.318021\n","---->[51200/60000 (85%)]\tPrecision: 0.046875\tLoss: 2.301308\tSurrogate Loss: 0.104715\tTotal Loss: 2.311779\n","[7.271018981933594, 2.308159351348877, 2.311824321746826, 2.316887617111206, 2.311785936355591, 2.3047935962677, 2.321305751800537, 2.3091559410095215, 2.302443504333496, 2.321333408355713, 2.29957914352417, 2.297646999359131, 2.303971767425537, 2.299528121948242, 2.3037450313568115, 2.3137476444244385, 2.3121588230133057, 2.2999653816223145, 2.309213638305664, 2.31245493888855, 2.2977147102355957, 2.292001247406006, 2.310389280319214, 2.2970216274261475, 2.3101630210876465, 2.2989344596862793, 2.297146797180176, 2.3237357139587402, 2.307612180709839, 2.3054370880126953, 2.2998428344726562, 2.2961480617523193, 2.3095712661743164, 2.2997848987579346, 2.3084757328033447, 2.307072401046753, 2.297497272491455, 2.2979116439819336, 2.3059797286987305, 2.3013076782226562]\n","[7.271018981933594, 2.338301658630371, 2.3281071186065674, 2.3283610343933105, 2.322368621826172, 2.3149564266204834, 2.3550384044647217, 2.3245670795440674, 2.3141729831695557, 2.330911874771118, 4.628449440002441, 2.76279878616333, 2.5342507362365723, 2.4526844024658203, 2.4183435440063477, 2.412317991256714, 2.3921518325805664, 2.369245767593384, 2.368068218231201, 2.364234685897827, 2.346494197845459, 2.3345673084259033, 2.3481531143188477, 2.3304502964019775, 2.3412296772003174, 2.3272523880004883, 2.322307825088501, 2.3473081588745117, 2.328084945678711, 2.325355291366577, 2.317981243133545, 2.3134427070617676, 2.325653553009033, 2.3147225379943848, 2.3221192359924316, 2.3206937313079834, 2.30975341796875, 2.310563802719116, 2.318021297454834, 2.311779260635376]\n","--> Finished Training Task 1. Starting Test phase:\n","---->Test set 0: Average loss: 0.0003, Accuracy: 9382/10000 (94%)\n","---->Test set 1: Average loss: 0.0023, Accuracy: 1009/10000 (10%)\n","---->Test set 2: Average loss: 0.0023, Accuracy: 1009/10000 (10%)\n","---->Test set 3: Average loss: 0.0023, Accuracy: 1009/10000 (10%)\n","---->Test set 4: Average loss: 0.0023, Accuracy: 1009/10000 (10%)\n","---->Test set 5: Average loss: 0.0023, Accuracy: 1009/10000 (10%)\n","---->Test set 6: Average loss: 0.0023, Accuracy: 1009/10000 (10%)\n","---->Test set 7: Average loss: 0.0023, Accuracy: 1010/10000 (10%)\n","---->Test set 8: Average loss: 0.0023, Accuracy: 1009/10000 (10%)\n","---->Test set 9: Average loss: 0.0023, Accuracy: 1009/10000 (10%)\n","--> Training Task 2:\n","----> Epoch 0:\n","---->[0/60000 (0%)]\tPrecision: 0.031250\tLoss: 2.298854\tSurrogate Loss: 0.000000\tTotal Loss: 2.298854\n","---->[12800/60000 (21%)]\tPrecision: 0.093750\tLoss: 2.308282\tSurrogate Loss: 0.004533\tTotal Loss: 2.308735\n","---->[25600/60000 (43%)]\tPrecision: 0.125000\tLoss: 2.291729\tSurrogate Loss: 0.003784\tTotal Loss: 2.292108\n","---->[38400/60000 (64%)]\tPrecision: 0.125000\tLoss: 2.310112\tSurrogate Loss: 0.007397\tTotal Loss: 2.310851\n","---->[51200/60000 (85%)]\tPrecision: 0.062500\tLoss: 2.310332\tSurrogate Loss: 0.009199\tTotal Loss: 2.311252\n","----> Epoch 1:\n","---->[0/60000 (0%)]\tPrecision: 0.093750\tLoss: 2.317915\tSurrogate Loss: 0.005736\tTotal Loss: 2.318489\n","---->[12800/60000 (21%)]\tPrecision: 0.109375\tLoss: 2.299100\tSurrogate Loss: 0.002776\tTotal Loss: 2.299377\n","---->[25600/60000 (43%)]\tPrecision: 0.109375\tLoss: 2.308050\tSurrogate Loss: 0.006148\tTotal Loss: 2.308665\n","---->[38400/60000 (64%)]\tPrecision: 0.046875\tLoss: 2.303921\tSurrogate Loss: 0.008107\tTotal Loss: 2.304731\n","---->[51200/60000 (85%)]\tPrecision: 0.156250\tLoss: 2.295684\tSurrogate Loss: 0.007433\tTotal Loss: 2.296427\n","----> Epoch 2:\n","---->[0/60000 (0%)]\tPrecision: 0.078125\tLoss: 2.292982\tSurrogate Loss: 0.003459\tTotal Loss: 2.293328\n","---->[12800/60000 (21%)]\tPrecision: 0.156250\tLoss: 2.299512\tSurrogate Loss: 0.004326\tTotal Loss: 2.299944\n","---->[25600/60000 (43%)]\tPrecision: 0.078125\tLoss: 2.307634\tSurrogate Loss: 0.007635\tTotal Loss: 2.308398\n","---->[38400/60000 (64%)]\tPrecision: 0.093750\tLoss: 2.309908\tSurrogate Loss: 0.003829\tTotal Loss: 2.310291\n","---->[51200/60000 (85%)]\tPrecision: 0.062500\tLoss: 2.310613\tSurrogate Loss: 0.006019\tTotal Loss: 2.311215\n","----> Epoch 3:\n","---->[0/60000 (0%)]\tPrecision: 0.078125\tLoss: 2.304139\tSurrogate Loss: 0.008613\tTotal Loss: 2.305001\n","---->[12800/60000 (21%)]\tPrecision: 0.156250\tLoss: 2.296963\tSurrogate Loss: 0.003380\tTotal Loss: 2.297302\n","---->[25600/60000 (43%)]\tPrecision: 0.109375\tLoss: 2.302098\tSurrogate Loss: 0.005759\tTotal Loss: 2.302674\n","---->[38400/60000 (64%)]\tPrecision: 0.046875\tLoss: 2.324108\tSurrogate Loss: 0.005009\tTotal Loss: 2.324609\n","---->[51200/60000 (85%)]\tPrecision: 0.156250\tLoss: 2.302877\tSurrogate Loss: 0.004586\tTotal Loss: 2.303336\n","----> Epoch 4:\n","---->[0/60000 (0%)]\tPrecision: 0.140625\tLoss: 2.300220\tSurrogate Loss: 0.006569\tTotal Loss: 2.300876\n","---->[12800/60000 (21%)]\tPrecision: 0.140625\tLoss: 2.290039\tSurrogate Loss: 0.004373\tTotal Loss: 2.290476\n","---->[25600/60000 (43%)]\tPrecision: 0.078125\tLoss: 2.299007\tSurrogate Loss: 0.005102\tTotal Loss: 2.299517\n","---->[38400/60000 (64%)]\tPrecision: 0.109375\tLoss: 2.305321\tSurrogate Loss: 0.005898\tTotal Loss: 2.305911\n","---->[51200/60000 (85%)]\tPrecision: 0.078125\tLoss: 2.299902\tSurrogate Loss: 0.007069\tTotal Loss: 2.300609\n","----> Epoch 5:\n","---->[0/60000 (0%)]\tPrecision: 0.093750\tLoss: 2.310089\tSurrogate Loss: 0.005036\tTotal Loss: 2.310592\n","---->[12800/60000 (21%)]\tPrecision: 0.093750\tLoss: 2.314750\tSurrogate Loss: 0.005320\tTotal Loss: 2.315282\n"],"name":"stdout"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-44-1f9039dcb51d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mn_epochs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mn_epochs_range\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m   \u001b[0mtotal_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_test_losses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_epochs\u001b[0m\u001b[0;34m)\u001b[0m    \u001b[0;31m# acc of 10 tasks after training each task\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m   \u001b[0;31m# Get the accuracy metric as defined by Facebook paper: sum(R_Ti)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-33-1c2bbc38ca9c>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(config, n_epochs)\u001b[0m\n\u001b[1;32m     50\u001b[0m                                       sampler = data_utils.SubsetRandomSampler(perm), drop_last = True)\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m         \u001b[0mtrain_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_train_losses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_task\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0;31m# Reset the optimizer (if using adam)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-32-254b608a00a7>\u001b[0m in \u001b[0;36mtrain_task\u001b[0;34m(model, device, train_loader, optimizer, batch_log, n_epochs)\u001b[0m\n\u001b[1;32m     18\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"----> Epoch {}:\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m             \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0;31m# Get the prediction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"o_Cms18rDykC","executionInfo":{"status":"ok","timestamp":1609323003648,"user_tz":-420,"elapsed":633192,"user":{"displayName":"Lâm Trần","photoUrl":"","userId":"03210360344194418537"}},"outputId":"8a6c4fd8-d1c4-491e-981f-21a2305acdec"},"source":["result_path = '/content/drive/MyDrive/Colab_Notebooks/LAB/Some Models/SI/result/over_parameter'\n","\n","n_epochs_range = [6] #,7,8,9,10]\n","\n","haha = None\n","\n","for n_epochs in n_epochs_range:\n","  total_acc, total_test_losses = main(config, n_epochs)    # acc of 10 tasks after training each task \n","\n","  # Get the accuracy metric as defined by Facebook paper: sum(R_Ti) \n","  # where T is the test set of the last Task and i is the current trained task\n","  average_acc = np.mean(total_acc[n_tasks])\n","  print(\"Accuracy:\", average_acc)\n","  print(\"Confusion matrix:\")\n","  print('\\n'.join([','.join([str(item) for item in row]) for row in total_acc]))\n","\n","  result_name = '0.1_e' + str(n_epochs) + '_big_si_permuted_10'\n","  with open(result_path + \"/\" + result_name + \".txt\", \"w\") as f:\n","    for t in range(1, n_tasks+1):\n","      f.write(\"{} : \".format(str(t)))\n","      for acc in total_acc[t]:\n","        f.write(\"{:.4f} \".format(acc))\n","      # f.write(\" \".join(str(total_acc[t])))\n","      f.write(\"\\n\")\n","\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["--> Training:\n","---->Test set 0: Average loss: 0.1767, Accuracy: 1225/10000 (12%)\n","---->Test set 1: Average loss: 0.2056, Accuracy: 1172/10000 (12%)\n","---->Test set 2: Average loss: 0.1637, Accuracy: 1097/10000 (11%)\n","---->Test set 3: Average loss: 0.1905, Accuracy: 1092/10000 (11%)\n","---->Test set 4: Average loss: 0.1548, Accuracy: 893/10000 (9%)\n","---->Test set 5: Average loss: 0.1720, Accuracy: 704/10000 (7%)\n","---->Test set 6: Average loss: 0.2250, Accuracy: 844/10000 (8%)\n","---->Test set 7: Average loss: 0.1697, Accuracy: 1013/10000 (10%)\n","---->Test set 8: Average loss: 0.1630, Accuracy: 832/10000 (8%)\n","---->Test set 9: Average loss: 0.2288, Accuracy: 959/10000 (10%)\n","--> Training Task 0:\n","----> Epoch 0:\n","---->[0/60000 (0%)]\tPrecision: 0.140625\tLoss: 164.804474\tSurrogate Loss: 0.000000\tTotal Loss: 164.804474\n","---->[12800/60000 (21%)]\tPrecision: 0.843750\tLoss: 0.792060\tSurrogate Loss: 0.000000\tTotal Loss: 0.792060\n","---->[25600/60000 (43%)]\tPrecision: 0.906250\tLoss: 0.282400\tSurrogate Loss: 0.000000\tTotal Loss: 0.282400\n","---->[38400/60000 (64%)]\tPrecision: 0.859375\tLoss: 0.540470\tSurrogate Loss: 0.000000\tTotal Loss: 0.540470\n","---->[51200/60000 (85%)]\tPrecision: 0.953125\tLoss: 0.127749\tSurrogate Loss: 0.000000\tTotal Loss: 0.127749\n","----> Epoch 1:\n","---->[0/60000 (0%)]\tPrecision: 0.953125\tLoss: 0.174264\tSurrogate Loss: 0.000000\tTotal Loss: 0.174264\n","---->[12800/60000 (21%)]\tPrecision: 0.953125\tLoss: 0.322600\tSurrogate Loss: 0.000000\tTotal Loss: 0.322600\n","---->[25600/60000 (43%)]\tPrecision: 0.968750\tLoss: 0.072004\tSurrogate Loss: 0.000000\tTotal Loss: 0.072004\n","---->[38400/60000 (64%)]\tPrecision: 0.875000\tLoss: 0.279208\tSurrogate Loss: 0.000000\tTotal Loss: 0.279208\n","---->[51200/60000 (85%)]\tPrecision: 1.000000\tLoss: 0.035940\tSurrogate Loss: 0.000000\tTotal Loss: 0.035940\n","----> Epoch 2:\n","---->[0/60000 (0%)]\tPrecision: 0.968750\tLoss: 0.066547\tSurrogate Loss: 0.000000\tTotal Loss: 0.066547\n","---->[12800/60000 (21%)]\tPrecision: 0.984375\tLoss: 0.035362\tSurrogate Loss: 0.000000\tTotal Loss: 0.035362\n","---->[25600/60000 (43%)]\tPrecision: 0.984375\tLoss: 0.070329\tSurrogate Loss: 0.000000\tTotal Loss: 0.070329\n","---->[38400/60000 (64%)]\tPrecision: 0.921875\tLoss: 0.361861\tSurrogate Loss: 0.000000\tTotal Loss: 0.361861\n","---->[51200/60000 (85%)]\tPrecision: 1.000000\tLoss: 0.024937\tSurrogate Loss: 0.000000\tTotal Loss: 0.024937\n","----> Epoch 3:\n","---->[0/60000 (0%)]\tPrecision: 0.937500\tLoss: 0.088375\tSurrogate Loss: 0.000000\tTotal Loss: 0.088375\n","---->[12800/60000 (21%)]\tPrecision: 0.968750\tLoss: 0.101110\tSurrogate Loss: 0.000000\tTotal Loss: 0.101110\n","---->[25600/60000 (43%)]\tPrecision: 0.968750\tLoss: 0.059999\tSurrogate Loss: 0.000000\tTotal Loss: 0.059999\n","---->[38400/60000 (64%)]\tPrecision: 0.968750\tLoss: 0.088968\tSurrogate Loss: 0.000000\tTotal Loss: 0.088968\n","---->[51200/60000 (85%)]\tPrecision: 0.953125\tLoss: 0.266745\tSurrogate Loss: 0.000000\tTotal Loss: 0.266745\n","----> Epoch 4:\n","---->[0/60000 (0%)]\tPrecision: 0.937500\tLoss: 0.232706\tSurrogate Loss: 0.000000\tTotal Loss: 0.232706\n","---->[12800/60000 (21%)]\tPrecision: 0.984375\tLoss: 0.081481\tSurrogate Loss: 0.000000\tTotal Loss: 0.081481\n","---->[25600/60000 (43%)]\tPrecision: 0.953125\tLoss: 0.155940\tSurrogate Loss: 0.000000\tTotal Loss: 0.155940\n","---->[38400/60000 (64%)]\tPrecision: 0.953125\tLoss: 0.127221\tSurrogate Loss: 0.000000\tTotal Loss: 0.127221\n","---->[51200/60000 (85%)]\tPrecision: 0.953125\tLoss: 0.216935\tSurrogate Loss: 0.000000\tTotal Loss: 0.216935\n","----> Epoch 5:\n","---->[0/60000 (0%)]\tPrecision: 0.921875\tLoss: 0.179910\tSurrogate Loss: 0.000000\tTotal Loss: 0.179910\n","---->[12800/60000 (21%)]\tPrecision: 0.968750\tLoss: 0.170323\tSurrogate Loss: 0.000000\tTotal Loss: 0.170323\n","---->[25600/60000 (43%)]\tPrecision: 0.968750\tLoss: 0.156075\tSurrogate Loss: 0.000000\tTotal Loss: 0.156075\n","---->[38400/60000 (64%)]\tPrecision: 0.937500\tLoss: 0.213919\tSurrogate Loss: 0.000000\tTotal Loss: 0.213919\n","---->[51200/60000 (85%)]\tPrecision: 0.953125\tLoss: 0.302221\tSurrogate Loss: 0.000000\tTotal Loss: 0.302221\n","[164.80447387695312, 0.7920596599578857, 0.2824002206325531, 0.5404701828956604, 0.12774871289730072, 0.17426396906375885, 0.3226000964641571, 0.07200394570827484, 0.2792082130908966, 0.03593990579247475, 0.06654712557792664, 0.035361506044864655, 0.07032870501279831, 0.36186057329177856, 0.024937398731708527, 0.0883752629160881, 0.10110961645841599, 0.05999857559800148, 0.08896849304437637, 0.266745388507843, 0.23270639777183533, 0.08148109167814255, 0.15593977272510529, 0.1272209882736206, 0.21693456172943115, 0.17991048097610474, 0.1703232377767563, 0.15607477724552155, 0.2139185667037964, 0.302221417427063]\n","[164.80447387695312, 0.7920596599578857, 0.2824002206325531, 0.5404701828956604, 0.12774871289730072, 0.17426396906375885, 0.3226000964641571, 0.07200394570827484, 0.2792082130908966, 0.03593990579247475, 0.06654712557792664, 0.035361506044864655, 0.07032870501279831, 0.36186057329177856, 0.024937398731708527, 0.0883752629160881, 0.10110961645841599, 0.05999857559800148, 0.08896849304437637, 0.266745388507843, 0.23270639777183533, 0.08148109167814255, 0.15593977272510529, 0.1272209882736206, 0.21693456172943115, 0.17991048097610474, 0.1703232377767563, 0.15607477724552155, 0.2139185667037964, 0.302221417427063]\n","--> Finished Training Task 0. Starting Test phase:\n","---->Test set 0: Average loss: 0.0002, Accuracy: 9533/10000 (95%)\n","---->Test set 1: Average loss: 0.0053, Accuracy: 699/10000 (7%)\n","---->Test set 2: Average loss: 0.0063, Accuracy: 1161/10000 (12%)\n","---->Test set 3: Average loss: 0.0072, Accuracy: 1294/10000 (13%)\n","---->Test set 4: Average loss: 0.0043, Accuracy: 1335/10000 (13%)\n","---->Test set 5: Average loss: 0.0036, Accuracy: 1171/10000 (12%)\n","---->Test set 6: Average loss: 0.0043, Accuracy: 826/10000 (8%)\n","---->Test set 7: Average loss: 0.0038, Accuracy: 1456/10000 (15%)\n","---->Test set 8: Average loss: 0.0035, Accuracy: 1180/10000 (12%)\n","---->Test set 9: Average loss: 0.0051, Accuracy: 1201/10000 (12%)\n","--> Training Task 1:\n","----> Epoch 0:\n","---->[0/60000 (0%)]\tPrecision: 0.046875\tLoss: 5.355693\tSurrogate Loss: 0.000000\tTotal Loss: 5.355693\n","---->[12800/60000 (21%)]\tPrecision: 0.093750\tLoss: 2.314999\tSurrogate Loss: 0.629875\tTotal Loss: 2.377986\n","---->[25600/60000 (43%)]\tPrecision: 0.062500\tLoss: 2.300142\tSurrogate Loss: 22.177208\tTotal Loss: 4.517863\n","---->[38400/60000 (64%)]\tPrecision: 0.140625\tLoss: 2.291150\tSurrogate Loss: 3.524192\tTotal Loss: 2.643569\n","---->[51200/60000 (85%)]\tPrecision: 0.203125\tLoss: 2.077480\tSurrogate Loss: 2.532806\tTotal Loss: 2.330760\n","----> Epoch 1:\n","---->[0/60000 (0%)]\tPrecision: 0.203125\tLoss: 2.983341\tSurrogate Loss: 1.690946\tTotal Loss: 3.152435\n","---->[12800/60000 (21%)]\tPrecision: 0.218750\tLoss: 2.020537\tSurrogate Loss: 1.267167\tTotal Loss: 2.147254\n","---->[25600/60000 (43%)]\tPrecision: 0.296875\tLoss: 1.949969\tSurrogate Loss: 0.951900\tTotal Loss: 2.045159\n","---->[38400/60000 (64%)]\tPrecision: 0.171875\tLoss: 2.082858\tSurrogate Loss: 0.940166\tTotal Loss: 2.176875\n","---->[51200/60000 (85%)]\tPrecision: 0.203125\tLoss: 2.011266\tSurrogate Loss: 1.364156\tTotal Loss: 2.147681\n","----> Epoch 2:\n","---->[0/60000 (0%)]\tPrecision: 0.218750\tLoss: 2.012168\tSurrogate Loss: 1.814631\tTotal Loss: 2.193632\n","---->[12800/60000 (21%)]\tPrecision: 0.312500\tLoss: 2.185484\tSurrogate Loss: 2.244474\tTotal Loss: 2.409932\n","---->[25600/60000 (43%)]\tPrecision: 0.203125\tLoss: 2.010869\tSurrogate Loss: 0.916978\tTotal Loss: 2.102567\n","---->[38400/60000 (64%)]\tPrecision: 0.140625\tLoss: 2.044495\tSurrogate Loss: 0.806694\tTotal Loss: 2.125165\n","---->[51200/60000 (85%)]\tPrecision: 0.218750\tLoss: 2.008629\tSurrogate Loss: 0.534897\tTotal Loss: 2.062119\n","----> Epoch 3:\n","---->[0/60000 (0%)]\tPrecision: 0.265625\tLoss: 1.779435\tSurrogate Loss: 0.499087\tTotal Loss: 1.829344\n","---->[12800/60000 (21%)]\tPrecision: 0.234375\tLoss: 1.992548\tSurrogate Loss: 0.608333\tTotal Loss: 2.053382\n","---->[25600/60000 (43%)]\tPrecision: 0.140625\tLoss: 2.116099\tSurrogate Loss: 0.486106\tTotal Loss: 2.164710\n","---->[38400/60000 (64%)]\tPrecision: 0.218750\tLoss: 2.164600\tSurrogate Loss: 0.577405\tTotal Loss: 2.222341\n","---->[51200/60000 (85%)]\tPrecision: 0.156250\tLoss: 2.112334\tSurrogate Loss: 0.981661\tTotal Loss: 2.210500\n","----> Epoch 4:\n","---->[0/60000 (0%)]\tPrecision: 0.234375\tLoss: 1.870179\tSurrogate Loss: 1.991206\tTotal Loss: 2.069300\n","---->[12800/60000 (21%)]\tPrecision: 0.218750\tLoss: 1.970065\tSurrogate Loss: 3.085205\tTotal Loss: 2.278586\n","---->[25600/60000 (43%)]\tPrecision: 0.218750\tLoss: 1.973780\tSurrogate Loss: 6.109923\tTotal Loss: 2.584772\n","---->[38400/60000 (64%)]\tPrecision: 0.359375\tLoss: 1.820948\tSurrogate Loss: 1.898396\tTotal Loss: 2.010788\n","---->[51200/60000 (85%)]\tPrecision: 0.328125\tLoss: 1.705019\tSurrogate Loss: 1.710985\tTotal Loss: 1.876117\n","----> Epoch 5:\n","---->[0/60000 (0%)]\tPrecision: 0.484375\tLoss: 1.516946\tSurrogate Loss: 2.100814\tTotal Loss: 1.727027\n","---->[12800/60000 (21%)]\tPrecision: 0.359375\tLoss: 1.565780\tSurrogate Loss: 1.528637\tTotal Loss: 1.718644\n","---->[25600/60000 (43%)]\tPrecision: 0.296875\tLoss: 1.680512\tSurrogate Loss: 5.722291\tTotal Loss: 2.252741\n","---->[38400/60000 (64%)]\tPrecision: 0.484375\tLoss: 1.117803\tSurrogate Loss: 3.728165\tTotal Loss: 1.490620\n","---->[51200/60000 (85%)]\tPrecision: 0.703125\tLoss: 1.052712\tSurrogate Loss: 6.922562\tTotal Loss: 1.744968\n","[5.3556928634643555, 2.3149988651275635, 2.300142288208008, 2.2911503314971924, 2.077479839324951, 2.9833405017852783, 2.0205368995666504, 1.9499688148498535, 2.082858085632324, 2.011265516281128, 2.0121684074401855, 2.1854844093322754, 2.010869264602661, 2.044495105743408, 2.0086288452148438, 1.779435157775879, 1.9925484657287598, 2.1160988807678223, 2.164600372314453, 2.1123337745666504, 1.8701791763305664, 1.9700651168823242, 1.9737799167633057, 1.8209484815597534, 1.7050187587738037, 1.516945719718933, 1.5657801628112793, 1.6805120706558228, 1.1178033351898193, 1.0527117252349854]\n","[5.3556928634643555, 2.377986431121826, 4.5178632736206055, 2.6435694694519043, 2.3307604789733887, 3.152435064315796, 2.1472535133361816, 2.045158863067627, 2.1768746376037598, 2.1476809978485107, 2.193631649017334, 2.4099318981170654, 2.102567195892334, 2.12516450881958, 2.0621185302734375, 1.8293437957763672, 2.0533816814422607, 2.1647095680236816, 2.2223408222198486, 2.2104997634887695, 2.0692996978759766, 2.27858567237854, 2.5847721099853516, 2.0107879638671875, 1.87611722946167, 1.7270270586013794, 1.7186439037322998, 2.2527413368225098, 1.4906198978424072, 1.7449679374694824]\n","--> Finished Training Task 1. Starting Test phase:\n","---->Test set 0: Average loss: 0.0003, Accuracy: 9369/10000 (94%)\n","---->Test set 1: Average loss: 0.0009, Accuracy: 7196/10000 (72%)\n","---->Test set 2: Average loss: 0.0059, Accuracy: 1031/10000 (10%)\n","---->Test set 3: Average loss: 0.0043, Accuracy: 1126/10000 (11%)\n","---->Test set 4: Average loss: 0.0042, Accuracy: 986/10000 (10%)\n","---->Test set 5: Average loss: 0.0047, Accuracy: 1070/10000 (11%)\n","---->Test set 6: Average loss: 0.0039, Accuracy: 1788/10000 (18%)\n","---->Test set 7: Average loss: 0.0046, Accuracy: 928/10000 (9%)\n","---->Test set 8: Average loss: 0.0066, Accuracy: 1123/10000 (11%)\n","---->Test set 9: Average loss: 0.0051, Accuracy: 978/10000 (10%)\n","--> Training Task 2:\n","----> Epoch 0:\n","---->[0/60000 (0%)]\tPrecision: 0.046875\tLoss: 6.761897\tSurrogate Loss: 0.000000\tTotal Loss: 6.761897\n","---->[12800/60000 (21%)]\tPrecision: 0.125000\tLoss: 2.345394\tSurrogate Loss: 5.441038\tTotal Loss: 2.889498\n","---->[25600/60000 (43%)]\tPrecision: 0.515625\tLoss: 1.213904\tSurrogate Loss: 4.847732\tTotal Loss: 1.698677\n","---->[38400/60000 (64%)]\tPrecision: 0.875000\tLoss: 0.666017\tSurrogate Loss: 4.117116\tTotal Loss: 1.077729\n","---->[51200/60000 (85%)]\tPrecision: 0.921875\tLoss: 0.272383\tSurrogate Loss: 5.289801\tTotal Loss: 0.801363\n","----> Epoch 1:\n","---->[0/60000 (0%)]\tPrecision: 0.906250\tLoss: 0.291190\tSurrogate Loss: 3.901074\tTotal Loss: 0.681298\n","---->[12800/60000 (21%)]\tPrecision: 0.906250\tLoss: 0.493663\tSurrogate Loss: 3.491714\tTotal Loss: 0.842834\n","---->[25600/60000 (43%)]\tPrecision: 0.921875\tLoss: 0.234253\tSurrogate Loss: 3.159130\tTotal Loss: 0.550166\n","---->[38400/60000 (64%)]\tPrecision: 0.796875\tLoss: 0.650957\tSurrogate Loss: 3.686681\tTotal Loss: 1.019626\n","---->[51200/60000 (85%)]\tPrecision: 0.890625\tLoss: 0.420366\tSurrogate Loss: 4.714026\tTotal Loss: 0.891769\n","----> Epoch 2:\n","---->[0/60000 (0%)]\tPrecision: 0.937500\tLoss: 0.180258\tSurrogate Loss: 2.955385\tTotal Loss: 0.475797\n","---->[12800/60000 (21%)]\tPrecision: 0.968750\tLoss: 0.093002\tSurrogate Loss: 3.638465\tTotal Loss: 0.456849\n","---->[25600/60000 (43%)]\tPrecision: 0.937500\tLoss: 0.147040\tSurrogate Loss: 2.497004\tTotal Loss: 0.396741\n","---->[38400/60000 (64%)]\tPrecision: 0.937500\tLoss: 0.201624\tSurrogate Loss: 2.629722\tTotal Loss: 0.464596\n","---->[51200/60000 (85%)]\tPrecision: 0.921875\tLoss: 0.429530\tSurrogate Loss: 2.608366\tTotal Loss: 0.690367\n","----> Epoch 3:\n","---->[0/60000 (0%)]\tPrecision: 0.906250\tLoss: 0.244991\tSurrogate Loss: 3.085419\tTotal Loss: 0.553533\n","---->[12800/60000 (21%)]\tPrecision: 0.937500\tLoss: 0.183200\tSurrogate Loss: 3.064024\tTotal Loss: 0.489602\n","---->[25600/60000 (43%)]\tPrecision: 0.984375\tLoss: 0.158832\tSurrogate Loss: 2.990008\tTotal Loss: 0.457833\n","---->[38400/60000 (64%)]\tPrecision: 0.937500\tLoss: 0.145083\tSurrogate Loss: 2.365094\tTotal Loss: 0.381593\n","---->[51200/60000 (85%)]\tPrecision: 0.968750\tLoss: 0.085794\tSurrogate Loss: 4.722088\tTotal Loss: 0.558002\n","----> Epoch 4:\n","---->[0/60000 (0%)]\tPrecision: 1.000000\tLoss: 0.065330\tSurrogate Loss: 2.313440\tTotal Loss: 0.296674\n","---->[12800/60000 (21%)]\tPrecision: 0.968750\tLoss: 0.079408\tSurrogate Loss: 2.286964\tTotal Loss: 0.308105\n","---->[25600/60000 (43%)]\tPrecision: 0.937500\tLoss: 0.262282\tSurrogate Loss: 2.639449\tTotal Loss: 0.526227\n","---->[38400/60000 (64%)]\tPrecision: 0.937500\tLoss: 0.200079\tSurrogate Loss: 3.340606\tTotal Loss: 0.534140\n","---->[51200/60000 (85%)]\tPrecision: 0.937500\tLoss: 0.183203\tSurrogate Loss: 2.634966\tTotal Loss: 0.446700\n","----> Epoch 5:\n","---->[0/60000 (0%)]\tPrecision: 0.984375\tLoss: 0.070992\tSurrogate Loss: 2.441861\tTotal Loss: 0.315179\n","---->[12800/60000 (21%)]\tPrecision: 0.937500\tLoss: 0.169408\tSurrogate Loss: 2.397504\tTotal Loss: 0.409158\n","---->[25600/60000 (43%)]\tPrecision: 1.000000\tLoss: 0.051351\tSurrogate Loss: 3.496641\tTotal Loss: 0.401015\n","---->[38400/60000 (64%)]\tPrecision: 0.984375\tLoss: 0.062858\tSurrogate Loss: 3.277822\tTotal Loss: 0.390640\n","---->[51200/60000 (85%)]\tPrecision: 0.968750\tLoss: 0.143920\tSurrogate Loss: 2.770963\tTotal Loss: 0.421016\n","[6.76189661026001, 2.3453943729400635, 1.2139042615890503, 0.6660172343254089, 0.2723833918571472, 0.29119014739990234, 0.49366289377212524, 0.23425260186195374, 0.650957465171814, 0.42036640644073486, 0.18025833368301392, 0.09300202876329422, 0.14704035222530365, 0.20162354409694672, 0.42953017354011536, 0.24499072134494781, 0.1831999123096466, 0.15883241593837738, 0.14508330821990967, 0.0857936441898346, 0.06533010303974152, 0.07940828055143356, 0.26228204369544983, 0.20007888972759247, 0.1832031011581421, 0.07099241763353348, 0.16940803825855255, 0.051350608468055725, 0.06285756826400757, 0.14391973614692688]\n","[6.76189661026001, 2.889498233795166, 1.69867742061615, 1.0777288675308228, 0.801363468170166, 0.6812976002693176, 0.8428342342376709, 0.5501656532287598, 1.0196256637573242, 0.8917690515518188, 0.4757968783378601, 0.45684853196144104, 0.39674079418182373, 0.4645957350730896, 0.690366804599762, 0.5535326600074768, 0.48960235714912415, 0.4578332304954529, 0.38159269094467163, 0.5580024719238281, 0.29667407274246216, 0.30810466408729553, 0.5262269973754883, 0.5341395139694214, 0.44669970870018005, 0.3151785433292389, 0.4091584086418152, 0.40101468563079834, 0.3906397521495819, 0.42101606726646423]\n","--> Finished Training Task 2. Starting Test phase:\n","---->Test set 0: Average loss: 0.0003, Accuracy: 9391/10000 (94%)\n","---->Test set 1: Average loss: 0.0017, Accuracy: 6665/10000 (67%)\n","---->Test set 2: Average loss: 0.0002, Accuracy: 9602/10000 (96%)\n","---->Test set 3: Average loss: 0.0065, Accuracy: 838/10000 (8%)\n","---->Test set 4: Average loss: 0.0035, Accuracy: 1662/10000 (17%)\n","---->Test set 5: Average loss: 0.0037, Accuracy: 928/10000 (9%)\n","---->Test set 6: Average loss: 0.0049, Accuracy: 1226/10000 (12%)\n","---->Test set 7: Average loss: 0.0036, Accuracy: 1077/10000 (11%)\n","---->Test set 8: Average loss: 0.0047, Accuracy: 967/10000 (10%)\n","---->Test set 9: Average loss: 0.0041, Accuracy: 1083/10000 (11%)\n","--> Training Task 3:\n","----> Epoch 0:\n","---->[0/60000 (0%)]\tPrecision: 0.109375\tLoss: 6.704465\tSurrogate Loss: 0.000000\tTotal Loss: 6.704465\n","---->[12800/60000 (21%)]\tPrecision: 0.875000\tLoss: 0.429810\tSurrogate Loss: 9.035841\tTotal Loss: 1.333395\n","---->[25600/60000 (43%)]\tPrecision: 0.937500\tLoss: 0.249241\tSurrogate Loss: 5.932498\tTotal Loss: 0.842491\n","---->[38400/60000 (64%)]\tPrecision: 0.937500\tLoss: 0.381512\tSurrogate Loss: 5.348438\tTotal Loss: 0.916356\n","---->[51200/60000 (85%)]\tPrecision: 0.937500\tLoss: 0.188740\tSurrogate Loss: 5.563480\tTotal Loss: 0.745088\n","----> Epoch 1:\n","---->[0/60000 (0%)]\tPrecision: 0.937500\tLoss: 0.147952\tSurrogate Loss: 4.258679\tTotal Loss: 0.573820\n","---->[12800/60000 (21%)]\tPrecision: 0.968750\tLoss: 0.144892\tSurrogate Loss: 5.658310\tTotal Loss: 0.710723\n","---->[25600/60000 (43%)]\tPrecision: 0.906250\tLoss: 0.206757\tSurrogate Loss: 4.228326\tTotal Loss: 0.629589\n","---->[38400/60000 (64%)]\tPrecision: 0.921875\tLoss: 0.214370\tSurrogate Loss: 3.477195\tTotal Loss: 0.562090\n","---->[51200/60000 (85%)]\tPrecision: 0.937500\tLoss: 0.206323\tSurrogate Loss: 4.019807\tTotal Loss: 0.608304\n","----> Epoch 2:\n","---->[0/60000 (0%)]\tPrecision: 0.921875\tLoss: 0.278493\tSurrogate Loss: 3.683543\tTotal Loss: 0.646847\n","---->[12800/60000 (21%)]\tPrecision: 1.000000\tLoss: 0.028479\tSurrogate Loss: 3.462221\tTotal Loss: 0.374701\n","---->[25600/60000 (43%)]\tPrecision: 0.953125\tLoss: 0.234206\tSurrogate Loss: 5.860742\tTotal Loss: 0.820280\n","---->[38400/60000 (64%)]\tPrecision: 0.968750\tLoss: 0.125109\tSurrogate Loss: 3.976197\tTotal Loss: 0.522729\n","---->[51200/60000 (85%)]\tPrecision: 0.906250\tLoss: 0.253236\tSurrogate Loss: 3.554801\tTotal Loss: 0.608716\n","----> Epoch 3:\n","---->[0/60000 (0%)]\tPrecision: 0.906250\tLoss: 0.178689\tSurrogate Loss: 4.162702\tTotal Loss: 0.594959\n","---->[12800/60000 (21%)]\tPrecision: 0.953125\tLoss: 0.231643\tSurrogate Loss: 3.038171\tTotal Loss: 0.535460\n","---->[25600/60000 (43%)]\tPrecision: 0.937500\tLoss: 0.213430\tSurrogate Loss: 4.536273\tTotal Loss: 0.667057\n","---->[38400/60000 (64%)]\tPrecision: 0.953125\tLoss: 0.207476\tSurrogate Loss: 3.099706\tTotal Loss: 0.517447\n","---->[51200/60000 (85%)]\tPrecision: 0.906250\tLoss: 0.265169\tSurrogate Loss: 2.688622\tTotal Loss: 0.534031\n","----> Epoch 4:\n","---->[0/60000 (0%)]\tPrecision: 0.921875\tLoss: 0.360254\tSurrogate Loss: 3.299773\tTotal Loss: 0.690231\n","---->[12800/60000 (21%)]\tPrecision: 0.937500\tLoss: 0.180803\tSurrogate Loss: 2.970654\tTotal Loss: 0.477868\n","---->[25600/60000 (43%)]\tPrecision: 0.953125\tLoss: 0.096858\tSurrogate Loss: 5.021581\tTotal Loss: 0.599016\n","---->[38400/60000 (64%)]\tPrecision: 0.937500\tLoss: 0.250718\tSurrogate Loss: 3.123768\tTotal Loss: 0.563095\n","---->[51200/60000 (85%)]\tPrecision: 0.968750\tLoss: 0.116935\tSurrogate Loss: 5.864923\tTotal Loss: 0.703428\n","----> Epoch 5:\n","---->[0/60000 (0%)]\tPrecision: 0.984375\tLoss: 0.069295\tSurrogate Loss: 3.106593\tTotal Loss: 0.379954\n","---->[12800/60000 (21%)]\tPrecision: 0.968750\tLoss: 0.113806\tSurrogate Loss: 2.634518\tTotal Loss: 0.377257\n","---->[25600/60000 (43%)]\tPrecision: 0.906250\tLoss: 0.341932\tSurrogate Loss: 5.278111\tTotal Loss: 0.869744\n","---->[38400/60000 (64%)]\tPrecision: 0.968750\tLoss: 0.100528\tSurrogate Loss: 4.337115\tTotal Loss: 0.534239\n","---->[51200/60000 (85%)]\tPrecision: 0.890625\tLoss: 0.295772\tSurrogate Loss: 2.855963\tTotal Loss: 0.581369\n","[6.704465389251709, 0.42981040477752686, 0.249241441488266, 0.381512314081192, 0.18874011933803558, 0.14795218408107758, 0.1448916792869568, 0.20675674080848694, 0.21437028050422668, 0.20632319152355194, 0.27849259972572327, 0.028479108586907387, 0.23420555889606476, 0.12510913610458374, 0.25323623418807983, 0.17868857085704803, 0.23164263367652893, 0.2134297490119934, 0.2074759602546692, 0.2651687264442444, 0.3602539300918579, 0.18080250918865204, 0.09685780853033066, 0.2507183849811554, 0.11693549156188965, 0.06929466873407364, 0.11380557715892792, 0.3419324457645416, 0.10052750259637833, 0.29577237367630005]\n","[6.704465389251709, 1.3333945274353027, 0.8424912691116333, 0.9163562059402466, 0.7450881600379944, 0.5738200545310974, 0.7107226848602295, 0.629589319229126, 0.562089741230011, 0.6083039045333862, 0.6468468904495239, 0.37470120191574097, 0.8202797174453735, 0.5227288603782654, 0.6087163686752319, 0.5949587225914001, 0.5354597568511963, 0.6670570373535156, 0.5174466371536255, 0.5340309143066406, 0.690231204032898, 0.47786790132522583, 0.5990158915519714, 0.5630951523780823, 0.703427791595459, 0.3799539804458618, 0.37725740671157837, 0.8697435855865479, 0.5342390537261963, 0.5813686847686768]\n","--> Finished Training Task 3. Starting Test phase:\n","---->Test set 0: Average loss: 0.0002, Accuracy: 9439/10000 (94%)\n","---->Test set 1: Average loss: 0.0021, Accuracy: 6039/10000 (60%)\n","---->Test set 2: Average loss: 0.0002, Accuracy: 9493/10000 (95%)\n","---->Test set 3: Average loss: 0.0002, Accuracy: 9527/10000 (95%)\n","---->Test set 4: Average loss: 0.0037, Accuracy: 1494/10000 (15%)\n","---->Test set 5: Average loss: 0.0034, Accuracy: 1104/10000 (11%)\n","---->Test set 6: Average loss: 0.0042, Accuracy: 1458/10000 (15%)\n","---->Test set 7: Average loss: 0.0040, Accuracy: 1098/10000 (11%)\n","---->Test set 8: Average loss: 0.0055, Accuracy: 931/10000 (9%)\n","---->Test set 9: Average loss: 0.0048, Accuracy: 995/10000 (10%)\n","--> Training Task 4:\n","----> Epoch 0:\n","---->[0/60000 (0%)]\tPrecision: 0.187500\tLoss: 3.372723\tSurrogate Loss: 0.000000\tTotal Loss: 3.372723\n","---->[12800/60000 (21%)]\tPrecision: 0.859375\tLoss: 0.541998\tSurrogate Loss: 11.606524\tTotal Loss: 1.702651\n","---->[25600/60000 (43%)]\tPrecision: 0.812500\tLoss: 0.602225\tSurrogate Loss: 8.061245\tTotal Loss: 1.408349\n","---->[38400/60000 (64%)]\tPrecision: 0.843750\tLoss: 0.471741\tSurrogate Loss: 7.429900\tTotal Loss: 1.214731\n","---->[51200/60000 (85%)]\tPrecision: 0.828125\tLoss: 0.704844\tSurrogate Loss: 5.634694\tTotal Loss: 1.268314\n","----> Epoch 1:\n","---->[0/60000 (0%)]\tPrecision: 0.921875\tLoss: 0.388691\tSurrogate Loss: 5.041268\tTotal Loss: 0.892818\n","---->[12800/60000 (21%)]\tPrecision: 0.921875\tLoss: 0.233352\tSurrogate Loss: 4.280369\tTotal Loss: 0.661389\n","---->[25600/60000 (43%)]\tPrecision: 0.890625\tLoss: 0.391300\tSurrogate Loss: 4.848276\tTotal Loss: 0.876128\n","---->[38400/60000 (64%)]\tPrecision: 0.921875\tLoss: 0.245724\tSurrogate Loss: 4.383306\tTotal Loss: 0.684054\n","---->[51200/60000 (85%)]\tPrecision: 0.906250\tLoss: 0.571700\tSurrogate Loss: 4.480352\tTotal Loss: 1.019735\n","----> Epoch 2:\n","---->[0/60000 (0%)]\tPrecision: 0.812500\tLoss: 1.035792\tSurrogate Loss: 5.443369\tTotal Loss: 1.580129\n","---->[12800/60000 (21%)]\tPrecision: 0.937500\tLoss: 0.362023\tSurrogate Loss: 5.099443\tTotal Loss: 0.871968\n","---->[25600/60000 (43%)]\tPrecision: 0.984375\tLoss: 0.075313\tSurrogate Loss: 3.843512\tTotal Loss: 0.459664\n","---->[38400/60000 (64%)]\tPrecision: 0.937500\tLoss: 0.208233\tSurrogate Loss: 5.679024\tTotal Loss: 0.776135\n","---->[51200/60000 (85%)]\tPrecision: 0.953125\tLoss: 0.162814\tSurrogate Loss: 3.745946\tTotal Loss: 0.537409\n","----> Epoch 3:\n","---->[0/60000 (0%)]\tPrecision: 0.953125\tLoss: 0.127462\tSurrogate Loss: 4.929374\tTotal Loss: 0.620399\n","---->[12800/60000 (21%)]\tPrecision: 0.921875\tLoss: 0.258702\tSurrogate Loss: 3.514421\tTotal Loss: 0.610144\n","---->[25600/60000 (43%)]\tPrecision: 0.890625\tLoss: 0.345966\tSurrogate Loss: 3.323651\tTotal Loss: 0.678331\n","---->[38400/60000 (64%)]\tPrecision: 0.968750\tLoss: 0.085582\tSurrogate Loss: 4.016017\tTotal Loss: 0.487183\n","---->[51200/60000 (85%)]\tPrecision: 0.953125\tLoss: 0.136324\tSurrogate Loss: 3.371743\tTotal Loss: 0.473498\n","----> Epoch 4:\n","---->[0/60000 (0%)]\tPrecision: 0.953125\tLoss: 0.215086\tSurrogate Loss: 4.637317\tTotal Loss: 0.678818\n","---->[12800/60000 (21%)]\tPrecision: 0.937500\tLoss: 0.215520\tSurrogate Loss: 5.263727\tTotal Loss: 0.741893\n","---->[25600/60000 (43%)]\tPrecision: 0.953125\tLoss: 0.180498\tSurrogate Loss: 3.426443\tTotal Loss: 0.523143\n","---->[38400/60000 (64%)]\tPrecision: 0.937500\tLoss: 0.236456\tSurrogate Loss: 3.940008\tTotal Loss: 0.630457\n","---->[51200/60000 (85%)]\tPrecision: 0.953125\tLoss: 0.145316\tSurrogate Loss: 6.268546\tTotal Loss: 0.772170\n","----> Epoch 5:\n","---->[0/60000 (0%)]\tPrecision: 0.937500\tLoss: 0.269088\tSurrogate Loss: 3.570395\tTotal Loss: 0.626128\n","---->[12800/60000 (21%)]\tPrecision: 0.968750\tLoss: 0.074366\tSurrogate Loss: 4.379040\tTotal Loss: 0.512270\n","---->[25600/60000 (43%)]\tPrecision: 0.953125\tLoss: 0.116434\tSurrogate Loss: 4.261168\tTotal Loss: 0.542551\n","---->[38400/60000 (64%)]\tPrecision: 0.875000\tLoss: 0.350041\tSurrogate Loss: 6.703861\tTotal Loss: 1.020427\n","---->[51200/60000 (85%)]\tPrecision: 0.937500\tLoss: 0.163376\tSurrogate Loss: 3.221992\tTotal Loss: 0.485576\n","[3.372722864151001, 0.5419982671737671, 0.602224588394165, 0.47174134850502014, 0.7048444151878357, 0.388690710067749, 0.23335225880146027, 0.39130017161369324, 0.24572384357452393, 0.5716995596885681, 1.0357921123504639, 0.36202341318130493, 0.07531275600194931, 0.20823273062705994, 0.1628144234418869, 0.1274620145559311, 0.258701890707016, 0.34596574306488037, 0.08558166772127151, 0.13632389903068542, 0.21508602797985077, 0.21552018821239471, 0.180498406291008, 0.23645637929439545, 0.14531587064266205, 0.26908817887306213, 0.07436564564704895, 0.11643408238887787, 0.35004135966300964, 0.16337639093399048]\n","[3.372722864151001, 1.702650785446167, 1.4083490371704102, 1.2147313356399536, 1.2683137655258179, 0.8928175568580627, 0.661389172077179, 0.8761277794837952, 0.6840543746948242, 1.0197348594665527, 1.5801290273666382, 0.8719677925109863, 0.459663987159729, 0.7761352062225342, 0.5374090671539307, 0.6203994154930115, 0.6101440191268921, 0.6783308982849121, 0.48718342185020447, 0.4734981954097748, 0.6788176894187927, 0.74189293384552, 0.5231426954269409, 0.6304571628570557, 0.7721704840660095, 0.6261277198791504, 0.5122696161270142, 0.5425508618354797, 1.0204274654388428, 0.4855755865573883]\n","--> Finished Training Task 4. Starting Test phase:\n","---->Test set 0: Average loss: 0.0003, Accuracy: 9334/10000 (93%)\n","---->Test set 1: Average loss: 0.0016, Accuracy: 6227/10000 (62%)\n","---->Test set 2: Average loss: 0.0003, Accuracy: 9200/10000 (92%)\n","---->Test set 3: Average loss: 0.0002, Accuracy: 9355/10000 (94%)\n","---->Test set 4: Average loss: 0.0003, Accuracy: 9254/10000 (93%)\n","---->Test set 5: Average loss: 0.0035, Accuracy: 1236/10000 (12%)\n","---->Test set 6: Average loss: 0.0036, Accuracy: 1314/10000 (13%)\n","---->Test set 7: Average loss: 0.0041, Accuracy: 986/10000 (10%)\n","---->Test set 8: Average loss: 0.0059, Accuracy: 910/10000 (9%)\n","---->Test set 9: Average loss: 0.0039, Accuracy: 917/10000 (9%)\n","--> Training Task 5:\n","----> Epoch 0:\n","---->[0/60000 (0%)]\tPrecision: 0.156250\tLoss: 3.261202\tSurrogate Loss: 0.000000\tTotal Loss: 3.261202\n","---->[12800/60000 (21%)]\tPrecision: 0.890625\tLoss: 0.528120\tSurrogate Loss: 10.983145\tTotal Loss: 1.626434\n","---->[25600/60000 (43%)]\tPrecision: 0.859375\tLoss: 0.421186\tSurrogate Loss: 11.708959\tTotal Loss: 1.592082\n","---->[38400/60000 (64%)]\tPrecision: 0.875000\tLoss: 0.321604\tSurrogate Loss: 6.103961\tTotal Loss: 0.932000\n","---->[51200/60000 (85%)]\tPrecision: 0.906250\tLoss: 0.240412\tSurrogate Loss: 4.742356\tTotal Loss: 0.714648\n","----> Epoch 1:\n","---->[0/60000 (0%)]\tPrecision: 0.937500\tLoss: 0.244467\tSurrogate Loss: 5.414291\tTotal Loss: 0.785896\n","---->[12800/60000 (21%)]\tPrecision: 0.906250\tLoss: 0.356954\tSurrogate Loss: 5.720978\tTotal Loss: 0.929052\n","---->[25600/60000 (43%)]\tPrecision: 0.859375\tLoss: 0.396383\tSurrogate Loss: 5.510953\tTotal Loss: 0.947478\n","---->[38400/60000 (64%)]\tPrecision: 0.906250\tLoss: 0.411433\tSurrogate Loss: 3.976919\tTotal Loss: 0.809125\n","---->[51200/60000 (85%)]\tPrecision: 0.921875\tLoss: 0.336897\tSurrogate Loss: 5.573690\tTotal Loss: 0.894266\n","----> Epoch 2:\n","---->[0/60000 (0%)]\tPrecision: 0.968750\tLoss: 0.147402\tSurrogate Loss: 4.322647\tTotal Loss: 0.579667\n","---->[12800/60000 (21%)]\tPrecision: 0.968750\tLoss: 0.126781\tSurrogate Loss: 3.304772\tTotal Loss: 0.457258\n","---->[25600/60000 (43%)]\tPrecision: 0.984375\tLoss: 0.037065\tSurrogate Loss: 5.897700\tTotal Loss: 0.626835\n","---->[38400/60000 (64%)]\tPrecision: 0.859375\tLoss: 0.383959\tSurrogate Loss: 4.226966\tTotal Loss: 0.806656\n","---->[51200/60000 (85%)]\tPrecision: 0.875000\tLoss: 0.411510\tSurrogate Loss: 4.747678\tTotal Loss: 0.886277\n","----> Epoch 3:\n","---->[0/60000 (0%)]\tPrecision: 0.984375\tLoss: 0.099265\tSurrogate Loss: 3.832307\tTotal Loss: 0.482495\n","---->[12800/60000 (21%)]\tPrecision: 0.906250\tLoss: 0.196153\tSurrogate Loss: 4.315004\tTotal Loss: 0.627654\n","---->[25600/60000 (43%)]\tPrecision: 0.921875\tLoss: 0.224166\tSurrogate Loss: 3.772651\tTotal Loss: 0.601431\n","---->[38400/60000 (64%)]\tPrecision: 0.968750\tLoss: 0.185844\tSurrogate Loss: 3.894599\tTotal Loss: 0.575304\n","---->[51200/60000 (85%)]\tPrecision: 0.937500\tLoss: 0.394530\tSurrogate Loss: 5.061058\tTotal Loss: 0.900636\n","----> Epoch 4:\n","---->[0/60000 (0%)]\tPrecision: 0.937500\tLoss: 0.180778\tSurrogate Loss: 4.457159\tTotal Loss: 0.626493\n","---->[12800/60000 (21%)]\tPrecision: 0.921875\tLoss: 0.274775\tSurrogate Loss: 3.962975\tTotal Loss: 0.671072\n","---->[25600/60000 (43%)]\tPrecision: 0.953125\tLoss: 0.135615\tSurrogate Loss: 3.927245\tTotal Loss: 0.528340\n","---->[38400/60000 (64%)]\tPrecision: 0.937500\tLoss: 0.382757\tSurrogate Loss: 5.314655\tTotal Loss: 0.914222\n","---->[51200/60000 (85%)]\tPrecision: 0.953125\tLoss: 0.148866\tSurrogate Loss: 4.630770\tTotal Loss: 0.611943\n","----> Epoch 5:\n","---->[0/60000 (0%)]\tPrecision: 0.953125\tLoss: 0.125577\tSurrogate Loss: 4.771718\tTotal Loss: 0.602749\n","---->[12800/60000 (21%)]\tPrecision: 0.968750\tLoss: 0.248132\tSurrogate Loss: 5.580396\tTotal Loss: 0.806171\n","---->[25600/60000 (43%)]\tPrecision: 0.921875\tLoss: 0.372435\tSurrogate Loss: 5.519361\tTotal Loss: 0.924371\n","---->[38400/60000 (64%)]\tPrecision: 0.937500\tLoss: 0.274408\tSurrogate Loss: 4.298106\tTotal Loss: 0.704219\n","---->[51200/60000 (85%)]\tPrecision: 0.921875\tLoss: 0.357778\tSurrogate Loss: 3.813948\tTotal Loss: 0.739173\n","[3.261202335357666, 0.5281198024749756, 0.4211861193180084, 0.32160380482673645, 0.24041229486465454, 0.24446678161621094, 0.3569536507129669, 0.3963826894760132, 0.4114334285259247, 0.3368971347808838, 0.14740194380283356, 0.12678119540214539, 0.0370648168027401, 0.3839590549468994, 0.4115096628665924, 0.09926467388868332, 0.19615349173545837, 0.22416582703590393, 0.1858440488576889, 0.3945299983024597, 0.18077757954597473, 0.2747749388217926, 0.13561508059501648, 0.3827565014362335, 0.1488661766052246, 0.12557727098464966, 0.2481318563222885, 0.3724348247051239, 0.2744084596633911, 0.3577784597873688]\n","[3.261202335357666, 1.626434326171875, 1.5920820236206055, 0.931999921798706, 0.7146478891372681, 0.7858958840370178, 0.9290515184402466, 0.9474780559539795, 0.8091253042221069, 0.8942661285400391, 0.5796666741371155, 0.457258403301239, 0.6268348097801208, 0.8066556453704834, 0.886277437210083, 0.48249536752700806, 0.627653956413269, 0.6014309525489807, 0.5753039717674255, 0.900635838508606, 0.6264934539794922, 0.6710724234580994, 0.5283396244049072, 0.914222002029419, 0.6119431257247925, 0.6027491092681885, 0.8061714768409729, 0.9243708848953247, 0.7042190432548523, 0.7391732931137085]\n","--> Finished Training Task 5. Starting Test phase:\n","---->Test set 0: Average loss: 0.0003, Accuracy: 9300/10000 (93%)\n","---->Test set 1: Average loss: 0.0019, Accuracy: 6404/10000 (64%)\n","---->Test set 2: Average loss: 0.0004, Accuracy: 9009/10000 (90%)\n","---->Test set 3: Average loss: 0.0003, Accuracy: 9197/10000 (92%)\n","---->Test set 4: Average loss: 0.0003, Accuracy: 9221/10000 (92%)\n","---->Test set 5: Average loss: 0.0003, Accuracy: 9308/10000 (93%)\n","---->Test set 6: Average loss: 0.0033, Accuracy: 1464/10000 (15%)\n","---->Test set 7: Average loss: 0.0035, Accuracy: 1532/10000 (15%)\n","---->Test set 8: Average loss: 0.0057, Accuracy: 775/10000 (8%)\n","---->Test set 9: Average loss: 0.0034, Accuracy: 967/10000 (10%)\n","--> Training Task 6:\n","----> Epoch 0:\n","---->[0/60000 (0%)]\tPrecision: 0.140625\tLoss: 2.890597\tSurrogate Loss: 0.000000\tTotal Loss: 2.890597\n","---->[12800/60000 (21%)]\tPrecision: 0.859375\tLoss: 0.503103\tSurrogate Loss: 11.272712\tTotal Loss: 1.630374\n","---->[25600/60000 (43%)]\tPrecision: 0.796875\tLoss: 0.594603\tSurrogate Loss: 9.132946\tTotal Loss: 1.507897\n","---->[38400/60000 (64%)]\tPrecision: 0.875000\tLoss: 0.442989\tSurrogate Loss: 7.083875\tTotal Loss: 1.151376\n","---->[51200/60000 (85%)]\tPrecision: 0.890625\tLoss: 0.393100\tSurrogate Loss: 10.366238\tTotal Loss: 1.429724\n","----> Epoch 1:\n","---->[0/60000 (0%)]\tPrecision: 0.906250\tLoss: 0.277567\tSurrogate Loss: 6.867429\tTotal Loss: 0.964309\n","---->[12800/60000 (21%)]\tPrecision: 0.875000\tLoss: 0.325531\tSurrogate Loss: 5.339267\tTotal Loss: 0.859458\n","---->[25600/60000 (43%)]\tPrecision: 0.906250\tLoss: 0.775111\tSurrogate Loss: 4.456048\tTotal Loss: 1.220715\n","---->[38400/60000 (64%)]\tPrecision: 0.906250\tLoss: 0.305667\tSurrogate Loss: 4.151043\tTotal Loss: 0.720771\n","---->[51200/60000 (85%)]\tPrecision: 0.906250\tLoss: 0.450542\tSurrogate Loss: 4.801097\tTotal Loss: 0.930652\n","----> Epoch 2:\n","---->[0/60000 (0%)]\tPrecision: 0.921875\tLoss: 0.317286\tSurrogate Loss: 5.173653\tTotal Loss: 0.834651\n","---->[12800/60000 (21%)]\tPrecision: 0.906250\tLoss: 0.305041\tSurrogate Loss: 4.431881\tTotal Loss: 0.748229\n","---->[25600/60000 (43%)]\tPrecision: 0.875000\tLoss: 0.759845\tSurrogate Loss: 6.399403\tTotal Loss: 1.399786\n","---->[38400/60000 (64%)]\tPrecision: 0.921875\tLoss: 0.402613\tSurrogate Loss: 5.061134\tTotal Loss: 0.908726\n","---->[51200/60000 (85%)]\tPrecision: 0.968750\tLoss: 0.147394\tSurrogate Loss: 5.732226\tTotal Loss: 0.720616\n","----> Epoch 3:\n","---->[0/60000 (0%)]\tPrecision: 0.843750\tLoss: 0.419309\tSurrogate Loss: 8.624325\tTotal Loss: 1.281741\n","---->[12800/60000 (21%)]\tPrecision: 0.968750\tLoss: 0.118954\tSurrogate Loss: 4.398536\tTotal Loss: 0.558808\n","---->[25600/60000 (43%)]\tPrecision: 0.953125\tLoss: 0.161315\tSurrogate Loss: 4.994657\tTotal Loss: 0.660781\n","---->[38400/60000 (64%)]\tPrecision: 0.937500\tLoss: 0.196095\tSurrogate Loss: 5.799797\tTotal Loss: 0.776074\n","---->[51200/60000 (85%)]\tPrecision: 0.906250\tLoss: 0.209057\tSurrogate Loss: 4.577000\tTotal Loss: 0.666757\n","----> Epoch 4:\n","---->[0/60000 (0%)]\tPrecision: 0.953125\tLoss: 0.291100\tSurrogate Loss: 4.777664\tTotal Loss: 0.768866\n","---->[12800/60000 (21%)]\tPrecision: 0.937500\tLoss: 0.238341\tSurrogate Loss: 6.289405\tTotal Loss: 0.867282\n","---->[25600/60000 (43%)]\tPrecision: 0.937500\tLoss: 0.148667\tSurrogate Loss: 6.774339\tTotal Loss: 0.826101\n","---->[38400/60000 (64%)]\tPrecision: 0.937500\tLoss: 0.330820\tSurrogate Loss: 4.506839\tTotal Loss: 0.781504\n","---->[51200/60000 (85%)]\tPrecision: 0.921875\tLoss: 0.248208\tSurrogate Loss: 4.546893\tTotal Loss: 0.702897\n","----> Epoch 5:\n","---->[0/60000 (0%)]\tPrecision: 0.890625\tLoss: 0.325533\tSurrogate Loss: 5.510317\tTotal Loss: 0.876565\n","---->[12800/60000 (21%)]\tPrecision: 0.968750\tLoss: 0.066136\tSurrogate Loss: 3.228795\tTotal Loss: 0.389015\n","---->[25600/60000 (43%)]\tPrecision: 0.937500\tLoss: 0.233908\tSurrogate Loss: 3.535183\tTotal Loss: 0.587427\n","---->[38400/60000 (64%)]\tPrecision: 0.796875\tLoss: 0.463630\tSurrogate Loss: 5.673419\tTotal Loss: 1.030972\n","---->[51200/60000 (85%)]\tPrecision: 0.984375\tLoss: 0.033222\tSurrogate Loss: 3.858763\tTotal Loss: 0.419098\n","[2.890597343444824, 0.5031030774116516, 0.594602644443512, 0.4429888427257538, 0.3930998742580414, 0.2775665819644928, 0.3255313038825989, 0.7751105427742004, 0.3056670129299164, 0.4505421817302704, 0.3172857165336609, 0.30504080653190613, 0.759845495223999, 0.40261298418045044, 0.14739368855953217, 0.41930896043777466, 0.11895429342985153, 0.1613154411315918, 0.19609472155570984, 0.20905663073062897, 0.29110005497932434, 0.23834119737148285, 0.14866726100444794, 0.3308197855949402, 0.24820800125598907, 0.3255331516265869, 0.06613579392433167, 0.23390835523605347, 0.4636297821998596, 0.0332220084965229]\n","[2.890597343444824, 1.6303741931915283, 1.5078972578048706, 1.1513763666152954, 1.4297237396240234, 0.9643094539642334, 0.8594579696655273, 1.2207152843475342, 0.7207713723182678, 0.9306519031524658, 0.834650993347168, 0.7482289671897888, 1.3997857570648193, 0.9087263941764832, 0.7206162810325623, 1.2817413806915283, 0.558807909488678, 0.6607811450958252, 0.7760744094848633, 0.6667566299438477, 0.7688664197921753, 0.8672817349433899, 0.8261011838912964, 0.7815036773681641, 0.7028972506523132, 0.8765649199485779, 0.3890153169631958, 0.5874266624450684, 1.0309717655181885, 0.4190983474254608]\n","--> Finished Training Task 6. Starting Test phase:\n","---->Test set 0: Average loss: 0.0003, Accuracy: 9271/10000 (93%)\n","---->Test set 1: Average loss: 0.0021, Accuracy: 6105/10000 (61%)\n","---->Test set 2: Average loss: 0.0006, Accuracy: 8556/10000 (86%)\n","---->Test set 3: Average loss: 0.0006, Accuracy: 8415/10000 (84%)\n","---->Test set 4: Average loss: 0.0009, Accuracy: 7615/10000 (76%)\n","---->Test set 5: Average loss: 0.0004, Accuracy: 8735/10000 (87%)\n","---->Test set 6: Average loss: 0.0003, Accuracy: 9225/10000 (92%)\n","---->Test set 7: Average loss: 0.0041, Accuracy: 1329/10000 (13%)\n","---->Test set 8: Average loss: 0.0056, Accuracy: 919/10000 (9%)\n","---->Test set 9: Average loss: 0.0033, Accuracy: 1152/10000 (12%)\n","--> Training Task 7:\n","----> Epoch 0:\n","---->[0/60000 (0%)]\tPrecision: 0.125000\tLoss: 3.557415\tSurrogate Loss: 0.000000\tTotal Loss: 3.557415\n","---->[12800/60000 (21%)]\tPrecision: 0.843750\tLoss: 0.467984\tSurrogate Loss: 14.175033\tTotal Loss: 1.885487\n","---->[25600/60000 (43%)]\tPrecision: 0.828125\tLoss: 0.581351\tSurrogate Loss: 9.623538\tTotal Loss: 1.543705\n","---->[38400/60000 (64%)]\tPrecision: 0.906250\tLoss: 0.333883\tSurrogate Loss: 6.983859\tTotal Loss: 1.032269\n","---->[51200/60000 (85%)]\tPrecision: 0.843750\tLoss: 0.434685\tSurrogate Loss: 6.835680\tTotal Loss: 1.118253\n","----> Epoch 1:\n","---->[0/60000 (0%)]\tPrecision: 0.843750\tLoss: 0.425439\tSurrogate Loss: 11.603786\tTotal Loss: 1.585818\n","---->[12800/60000 (21%)]\tPrecision: 0.890625\tLoss: 0.365805\tSurrogate Loss: 6.065886\tTotal Loss: 0.972393\n","---->[25600/60000 (43%)]\tPrecision: 0.890625\tLoss: 0.298092\tSurrogate Loss: 8.007520\tTotal Loss: 1.098844\n","---->[38400/60000 (64%)]\tPrecision: 0.921875\tLoss: 0.423208\tSurrogate Loss: 9.385674\tTotal Loss: 1.361776\n","---->[51200/60000 (85%)]\tPrecision: 0.796875\tLoss: 0.720863\tSurrogate Loss: 6.156369\tTotal Loss: 1.336500\n","----> Epoch 2:\n","---->[0/60000 (0%)]\tPrecision: 0.828125\tLoss: 0.620924\tSurrogate Loss: 5.356316\tTotal Loss: 1.156556\n","---->[12800/60000 (21%)]\tPrecision: 0.984375\tLoss: 0.083239\tSurrogate Loss: 5.238014\tTotal Loss: 0.607040\n","---->[25600/60000 (43%)]\tPrecision: 0.921875\tLoss: 0.338862\tSurrogate Loss: 4.214271\tTotal Loss: 0.760289\n","---->[38400/60000 (64%)]\tPrecision: 0.875000\tLoss: 0.422765\tSurrogate Loss: 10.176316\tTotal Loss: 1.440396\n","---->[51200/60000 (85%)]\tPrecision: 0.890625\tLoss: 0.288739\tSurrogate Loss: 5.733284\tTotal Loss: 0.862068\n","----> Epoch 3:\n","---->[0/60000 (0%)]\tPrecision: 0.953125\tLoss: 0.231227\tSurrogate Loss: 5.159519\tTotal Loss: 0.747179\n","---->[12800/60000 (21%)]\tPrecision: 0.906250\tLoss: 0.273390\tSurrogate Loss: 5.014068\tTotal Loss: 0.774797\n","---->[25600/60000 (43%)]\tPrecision: 0.875000\tLoss: 0.662157\tSurrogate Loss: 4.879037\tTotal Loss: 1.150061\n","---->[38400/60000 (64%)]\tPrecision: 0.937500\tLoss: 0.213617\tSurrogate Loss: 4.373850\tTotal Loss: 0.651002\n","---->[51200/60000 (85%)]\tPrecision: 0.906250\tLoss: 0.293359\tSurrogate Loss: 6.248979\tTotal Loss: 0.918257\n","----> Epoch 4:\n","---->[0/60000 (0%)]\tPrecision: 0.890625\tLoss: 0.328906\tSurrogate Loss: 5.597546\tTotal Loss: 0.888660\n","---->[12800/60000 (21%)]\tPrecision: 0.906250\tLoss: 0.222667\tSurrogate Loss: 4.985201\tTotal Loss: 0.721187\n","---->[25600/60000 (43%)]\tPrecision: 0.906250\tLoss: 0.237151\tSurrogate Loss: 12.917678\tTotal Loss: 1.528919\n","---->[38400/60000 (64%)]\tPrecision: 0.953125\tLoss: 0.119062\tSurrogate Loss: 4.051986\tTotal Loss: 0.524261\n","---->[51200/60000 (85%)]\tPrecision: 0.968750\tLoss: 0.105002\tSurrogate Loss: 4.968079\tTotal Loss: 0.601810\n","----> Epoch 5:\n","---->[0/60000 (0%)]\tPrecision: 0.953125\tLoss: 0.203817\tSurrogate Loss: 5.333129\tTotal Loss: 0.737130\n","---->[12800/60000 (21%)]\tPrecision: 0.906250\tLoss: 0.288852\tSurrogate Loss: 4.315281\tTotal Loss: 0.720381\n","---->[25600/60000 (43%)]\tPrecision: 0.968750\tLoss: 0.115618\tSurrogate Loss: 5.517478\tTotal Loss: 0.667365\n","---->[38400/60000 (64%)]\tPrecision: 0.968750\tLoss: 0.114565\tSurrogate Loss: 4.587949\tTotal Loss: 0.573360\n","---->[51200/60000 (85%)]\tPrecision: 0.968750\tLoss: 0.082033\tSurrogate Loss: 4.265959\tTotal Loss: 0.508629\n","[3.55741548538208, 0.467984139919281, 0.5813513994216919, 0.33388322591781616, 0.43468520045280457, 0.4254389703273773, 0.365804523229599, 0.2980916500091553, 0.4232083559036255, 0.720863401889801, 0.6209239959716797, 0.0832388773560524, 0.3388618230819702, 0.42276477813720703, 0.2887392044067383, 0.2312273383140564, 0.27339044213294983, 0.6621571779251099, 0.21361736953258514, 0.2933587431907654, 0.3289058208465576, 0.22266684472560883, 0.23715080320835114, 0.11906206607818604, 0.10500244051218033, 0.20381726324558258, 0.2888524532318115, 0.11561756581068039, 0.11456532776355743, 0.082032710313797]\n","[3.55741548538208, 1.8854873180389404, 1.5437052249908447, 1.0322691202163696, 1.118253231048584, 1.585817575454712, 0.9723931550979614, 1.0988435745239258, 1.3617756366729736, 1.3365004062652588, 1.1565556526184082, 0.6070403456687927, 0.7602888941764832, 1.4403964281082153, 0.8620676398277283, 0.7471792697906494, 0.7747973203659058, 1.1500608921051025, 0.6510024070739746, 0.9182566404342651, 0.8886603713035583, 0.7211869359016418, 1.5289186239242554, 0.5242606401443481, 0.6018103361129761, 0.7371302247047424, 0.7203806042671204, 0.667365312576294, 0.5733602046966553, 0.5086286067962646]\n","--> Finished Training Task 7. Starting Test phase:\n","---->Test set 0: Average loss: 0.0003, Accuracy: 9278/10000 (93%)\n","---->Test set 1: Average loss: 0.0023, Accuracy: 6043/10000 (60%)\n","---->Test set 2: Average loss: 0.0006, Accuracy: 8418/10000 (84%)\n","---->Test set 3: Average loss: 0.0006, Accuracy: 8482/10000 (85%)\n","---->Test set 4: Average loss: 0.0006, Accuracy: 8062/10000 (81%)\n","---->Test set 5: Average loss: 0.0005, Accuracy: 8608/10000 (86%)\n","---->Test set 6: Average loss: 0.0003, Accuracy: 9199/10000 (92%)\n","---->Test set 7: Average loss: 0.0003, Accuracy: 9287/10000 (93%)\n","---->Test set 8: Average loss: 0.0062, Accuracy: 878/10000 (9%)\n","---->Test set 9: Average loss: 0.0035, Accuracy: 1200/10000 (12%)\n","--> Training Task 8:\n","----> Epoch 0:\n","---->[0/60000 (0%)]\tPrecision: 0.156250\tLoss: 5.923913\tSurrogate Loss: 0.000000\tTotal Loss: 5.923913\n","---->[12800/60000 (21%)]\tPrecision: 0.843750\tLoss: 0.698162\tSurrogate Loss: 14.150189\tTotal Loss: 2.113181\n","---->[25600/60000 (43%)]\tPrecision: 0.843750\tLoss: 0.464314\tSurrogate Loss: 9.249206\tTotal Loss: 1.389235\n","---->[38400/60000 (64%)]\tPrecision: 0.859375\tLoss: 0.660512\tSurrogate Loss: 6.981278\tTotal Loss: 1.358640\n","---->[51200/60000 (85%)]\tPrecision: 0.921875\tLoss: 0.210861\tSurrogate Loss: 13.569136\tTotal Loss: 1.567774\n","----> Epoch 1:\n","---->[0/60000 (0%)]\tPrecision: 0.828125\tLoss: 0.633443\tSurrogate Loss: 8.747161\tTotal Loss: 1.508159\n","---->[12800/60000 (21%)]\tPrecision: 0.875000\tLoss: 0.386580\tSurrogate Loss: 5.461990\tTotal Loss: 0.932779\n","---->[25600/60000 (43%)]\tPrecision: 0.890625\tLoss: 0.316988\tSurrogate Loss: 6.032470\tTotal Loss: 0.920235\n","---->[38400/60000 (64%)]\tPrecision: 0.953125\tLoss: 0.140934\tSurrogate Loss: 5.879694\tTotal Loss: 0.728903\n","---->[51200/60000 (85%)]\tPrecision: 0.953125\tLoss: 0.115525\tSurrogate Loss: 5.617874\tTotal Loss: 0.677313\n","----> Epoch 2:\n","---->[0/60000 (0%)]\tPrecision: 0.921875\tLoss: 0.278984\tSurrogate Loss: 7.491156\tTotal Loss: 1.028100\n","---->[12800/60000 (21%)]\tPrecision: 0.984375\tLoss: 0.068785\tSurrogate Loss: 4.947257\tTotal Loss: 0.563511\n","---->[25600/60000 (43%)]\tPrecision: 0.937500\tLoss: 0.240377\tSurrogate Loss: 7.593356\tTotal Loss: 0.999712\n","---->[38400/60000 (64%)]\tPrecision: 0.875000\tLoss: 0.296587\tSurrogate Loss: 5.524802\tTotal Loss: 0.849067\n","---->[51200/60000 (85%)]\tPrecision: 0.953125\tLoss: 0.172632\tSurrogate Loss: 4.838888\tTotal Loss: 0.656520\n","----> Epoch 3:\n","---->[0/60000 (0%)]\tPrecision: 0.906250\tLoss: 0.211426\tSurrogate Loss: 8.154104\tTotal Loss: 1.026836\n","---->[12800/60000 (21%)]\tPrecision: 0.906250\tLoss: 0.281492\tSurrogate Loss: 4.602934\tTotal Loss: 0.741786\n","---->[25600/60000 (43%)]\tPrecision: 0.890625\tLoss: 0.258810\tSurrogate Loss: 7.389254\tTotal Loss: 0.997736\n","---->[38400/60000 (64%)]\tPrecision: 0.921875\tLoss: 0.270006\tSurrogate Loss: 7.562637\tTotal Loss: 1.026270\n","---->[51200/60000 (85%)]\tPrecision: 0.890625\tLoss: 0.371484\tSurrogate Loss: 5.246571\tTotal Loss: 0.896141\n","----> Epoch 4:\n","---->[0/60000 (0%)]\tPrecision: 0.921875\tLoss: 0.214821\tSurrogate Loss: 4.821010\tTotal Loss: 0.696922\n","---->[12800/60000 (21%)]\tPrecision: 0.953125\tLoss: 0.133615\tSurrogate Loss: 5.095583\tTotal Loss: 0.643173\n","---->[25600/60000 (43%)]\tPrecision: 0.921875\tLoss: 0.119973\tSurrogate Loss: 4.622205\tTotal Loss: 0.582194\n","---->[38400/60000 (64%)]\tPrecision: 0.953125\tLoss: 0.137559\tSurrogate Loss: 6.907972\tTotal Loss: 0.828356\n","---->[51200/60000 (85%)]\tPrecision: 0.859375\tLoss: 0.448951\tSurrogate Loss: 8.880054\tTotal Loss: 1.336957\n","----> Epoch 5:\n","---->[0/60000 (0%)]\tPrecision: 0.968750\tLoss: 0.127163\tSurrogate Loss: 5.571259\tTotal Loss: 0.684289\n","---->[12800/60000 (21%)]\tPrecision: 0.968750\tLoss: 0.170170\tSurrogate Loss: 4.325625\tTotal Loss: 0.602733\n","---->[25600/60000 (43%)]\tPrecision: 0.921875\tLoss: 0.339630\tSurrogate Loss: 5.766320\tTotal Loss: 0.916262\n","---->[38400/60000 (64%)]\tPrecision: 0.921875\tLoss: 0.426142\tSurrogate Loss: 9.130681\tTotal Loss: 1.339210\n","---->[51200/60000 (85%)]\tPrecision: 0.968750\tLoss: 0.433490\tSurrogate Loss: 5.940178\tTotal Loss: 1.027508\n","[5.923912525177002, 0.6981616020202637, 0.46431413292884827, 0.6605117321014404, 0.21086068451404572, 0.6334433555603027, 0.3865804076194763, 0.3169875144958496, 0.1409337818622589, 0.11552535742521286, 0.2789844274520874, 0.06878501921892166, 0.24037672579288483, 0.2965869605541229, 0.17263160645961761, 0.2114255726337433, 0.2814922034740448, 0.2588100731372833, 0.2700062394142151, 0.3714838922023773, 0.21482136845588684, 0.13361480832099915, 0.11997319757938385, 0.13755883276462555, 0.4489513039588928, 0.12716302275657654, 0.17017041146755219, 0.3396296203136444, 0.42614179849624634, 0.4334900379180908]\n","[5.923912525177002, 2.113180637359619, 1.3892346620559692, 1.3586395978927612, 1.5677742958068848, 1.5081593990325928, 0.9327794313430786, 0.9202345013618469, 0.7289032936096191, 0.6773127913475037, 1.0281000137329102, 0.5635107159614563, 0.999712347984314, 0.8490670919418335, 0.6565203666687012, 1.0268360376358032, 0.7417856454849243, 0.9977355003356934, 1.0262699127197266, 0.8961410522460938, 0.6969223022460938, 0.643173098564148, 0.5821936726570129, 0.8283560872077942, 1.3369567394256592, 0.6842889785766602, 0.6027328968048096, 0.9162616729736328, 1.3392099142074585, 1.0275077819824219]\n","--> Finished Training Task 8. Starting Test phase:\n","---->Test set 0: Average loss: 0.0004, Accuracy: 9165/10000 (92%)\n","---->Test set 1: Average loss: 0.0027, Accuracy: 5763/10000 (58%)\n","---->Test set 2: Average loss: 0.0008, Accuracy: 7912/10000 (79%)\n","---->Test set 3: Average loss: 0.0007, Accuracy: 8318/10000 (83%)\n","---->Test set 4: Average loss: 0.0007, Accuracy: 7976/10000 (80%)\n","---->Test set 5: Average loss: 0.0006, Accuracy: 8300/10000 (83%)\n","---->Test set 6: Average loss: 0.0005, Accuracy: 8679/10000 (87%)\n","---->Test set 7: Average loss: 0.0003, Accuracy: 9173/10000 (92%)\n","---->Test set 8: Average loss: 0.0003, Accuracy: 9162/10000 (92%)\n","---->Test set 9: Average loss: 0.0039, Accuracy: 1179/10000 (12%)\n","--> Training Task 9:\n","----> Epoch 0:\n","---->[0/60000 (0%)]\tPrecision: 0.078125\tLoss: 3.682312\tSurrogate Loss: 0.000000\tTotal Loss: 3.682312\n","---->[12800/60000 (21%)]\tPrecision: 0.765625\tLoss: 0.693625\tSurrogate Loss: 18.733137\tTotal Loss: 2.566939\n","---->[25600/60000 (43%)]\tPrecision: 0.890625\tLoss: 0.371707\tSurrogate Loss: 11.109351\tTotal Loss: 1.482643\n","---->[38400/60000 (64%)]\tPrecision: 0.890625\tLoss: 0.371411\tSurrogate Loss: 12.890708\tTotal Loss: 1.660481\n","---->[51200/60000 (85%)]\tPrecision: 0.875000\tLoss: 0.365243\tSurrogate Loss: 9.794586\tTotal Loss: 1.344702\n","----> Epoch 1:\n","---->[0/60000 (0%)]\tPrecision: 0.890625\tLoss: 0.456963\tSurrogate Loss: 14.935081\tTotal Loss: 1.950472\n","---->[12800/60000 (21%)]\tPrecision: 0.906250\tLoss: 0.418938\tSurrogate Loss: 6.782763\tTotal Loss: 1.097214\n","---->[25600/60000 (43%)]\tPrecision: 0.906250\tLoss: 0.220196\tSurrogate Loss: 6.628133\tTotal Loss: 0.883009\n","---->[38400/60000 (64%)]\tPrecision: 0.906250\tLoss: 0.492439\tSurrogate Loss: 6.316072\tTotal Loss: 1.124046\n","---->[51200/60000 (85%)]\tPrecision: 0.921875\tLoss: 0.333963\tSurrogate Loss: 5.966456\tTotal Loss: 0.930609\n","----> Epoch 2:\n","---->[0/60000 (0%)]\tPrecision: 0.828125\tLoss: 0.543577\tSurrogate Loss: 7.540685\tTotal Loss: 1.297646\n","---->[12800/60000 (21%)]\tPrecision: 0.890625\tLoss: 0.319720\tSurrogate Loss: 6.504026\tTotal Loss: 0.970122\n","---->[25600/60000 (43%)]\tPrecision: 0.906250\tLoss: 0.231446\tSurrogate Loss: 5.641677\tTotal Loss: 0.795614\n","---->[38400/60000 (64%)]\tPrecision: 0.921875\tLoss: 0.157095\tSurrogate Loss: 6.199961\tTotal Loss: 0.777092\n","---->[51200/60000 (85%)]\tPrecision: 0.906250\tLoss: 0.301552\tSurrogate Loss: 7.787778\tTotal Loss: 1.080330\n","----> Epoch 3:\n","---->[0/60000 (0%)]\tPrecision: 0.890625\tLoss: 0.367876\tSurrogate Loss: 7.099052\tTotal Loss: 1.077781\n","---->[12800/60000 (21%)]\tPrecision: 0.968750\tLoss: 0.113123\tSurrogate Loss: 5.658911\tTotal Loss: 0.679014\n","---->[25600/60000 (43%)]\tPrecision: 0.953125\tLoss: 0.223428\tSurrogate Loss: 6.239135\tTotal Loss: 0.847341\n","---->[38400/60000 (64%)]\tPrecision: 0.984375\tLoss: 0.068245\tSurrogate Loss: 5.990710\tTotal Loss: 0.667316\n","---->[51200/60000 (85%)]\tPrecision: 0.906250\tLoss: 0.294309\tSurrogate Loss: 5.986362\tTotal Loss: 0.892945\n","----> Epoch 4:\n","---->[0/60000 (0%)]\tPrecision: 0.890625\tLoss: 0.380387\tSurrogate Loss: 5.994846\tTotal Loss: 0.979872\n","---->[12800/60000 (21%)]\tPrecision: 0.906250\tLoss: 0.318788\tSurrogate Loss: 6.378291\tTotal Loss: 0.956617\n","---->[25600/60000 (43%)]\tPrecision: 0.921875\tLoss: 0.199520\tSurrogate Loss: 6.558368\tTotal Loss: 0.855357\n","---->[38400/60000 (64%)]\tPrecision: 0.937500\tLoss: 0.242429\tSurrogate Loss: 6.628262\tTotal Loss: 0.905256\n","---->[51200/60000 (85%)]\tPrecision: 0.906250\tLoss: 0.280728\tSurrogate Loss: 6.334864\tTotal Loss: 0.914214\n","----> Epoch 5:\n","---->[0/60000 (0%)]\tPrecision: 0.953125\tLoss: 0.177343\tSurrogate Loss: 9.431892\tTotal Loss: 1.120533\n","---->[12800/60000 (21%)]\tPrecision: 0.906250\tLoss: 0.273019\tSurrogate Loss: 9.290271\tTotal Loss: 1.202046\n","---->[25600/60000 (43%)]\tPrecision: 0.968750\tLoss: 0.148405\tSurrogate Loss: 6.711452\tTotal Loss: 0.819550\n","---->[38400/60000 (64%)]\tPrecision: 0.937500\tLoss: 0.152534\tSurrogate Loss: 4.798842\tTotal Loss: 0.632418\n","---->[51200/60000 (85%)]\tPrecision: 0.921875\tLoss: 0.295071\tSurrogate Loss: 7.112095\tTotal Loss: 1.006281\n","[3.682312250137329, 0.6936248540878296, 0.3717074394226074, 0.37141063809394836, 0.36524346470832825, 0.45696330070495605, 0.41893792152404785, 0.22019605338573456, 0.4924390912055969, 0.3339630365371704, 0.5435774326324463, 0.31971967220306396, 0.23144610226154327, 0.15709546208381653, 0.3015517294406891, 0.3678761124610901, 0.11312316358089447, 0.22342796623706818, 0.0682450383901596, 0.2943089008331299, 0.38038700819015503, 0.3187878727912903, 0.19951997697353363, 0.24242940545082092, 0.28072792291641235, 0.17734330892562866, 0.2730187475681305, 0.1484048068523407, 0.15253376960754395, 0.2950713038444519]\n","[3.682312250137329, 2.566938638687134, 1.4826425313949585, 1.6604814529418945, 1.344702124595642, 1.9504715204238892, 1.0972142219543457, 0.8830094337463379, 1.1240463256835938, 0.9306086897850037, 1.2976460456848145, 0.9701223373413086, 0.7956138253211975, 0.7770916223526001, 1.0803295373916626, 1.0777814388275146, 0.6790143251419067, 0.8473414182662964, 0.6673160791397095, 0.8929451107978821, 0.9798716306686401, 0.956616997718811, 0.8553568124771118, 0.9052556753158569, 0.9142143130302429, 1.120532512664795, 1.2020457983016968, 0.8195499181747437, 0.6324180364608765, 1.006280779838562]\n","--> Finished Training Task 9. Starting Test phase:\n","---->Test set 0: Average loss: 0.0005, Accuracy: 8964/10000 (90%)\n","---->Test set 1: Average loss: 0.0021, Accuracy: 5850/10000 (58%)\n","---->Test set 2: Average loss: 0.0010, Accuracy: 7736/10000 (77%)\n","---->Test set 3: Average loss: 0.0006, Accuracy: 8445/10000 (84%)\n","---->Test set 4: Average loss: 0.0008, Accuracy: 7829/10000 (78%)\n","---->Test set 5: Average loss: 0.0006, Accuracy: 8469/10000 (85%)\n","---->Test set 6: Average loss: 0.0005, Accuracy: 8675/10000 (87%)\n","---->Test set 7: Average loss: 0.0004, Accuracy: 8950/10000 (90%)\n","---->Test set 8: Average loss: 0.0005, Accuracy: 8845/10000 (88%)\n","---->Test set 9: Average loss: 0.0002, Accuracy: 9429/10000 (94%)\n","Accuracy: 0.83192\n","Confusion matrix:\n","0.1225,0.1172,0.1097,0.1092,0.0893,0.0704,0.0844,0.1013,0.0832,0.0959\n","0.9533,0.0699,0.1161,0.1294,0.1335,0.1171,0.0826,0.1456,0.118,0.1201\n","0.9369,0.7196,0.1031,0.1126,0.0986,0.107,0.1788,0.0928,0.1123,0.0978\n","0.9391,0.6665,0.9602,0.0838,0.1662,0.0928,0.1226,0.1077,0.0967,0.1083\n","0.9439,0.6039,0.9493,0.9527,0.1494,0.1104,0.1458,0.1098,0.0931,0.0995\n","0.9334,0.6227,0.92,0.9355,0.9254,0.1236,0.1314,0.0986,0.091,0.0917\n","0.93,0.6404,0.9009,0.9197,0.9221,0.9308,0.1464,0.1532,0.0775,0.0967\n","0.9271,0.6105,0.8556,0.8415,0.7615,0.8735,0.9225,0.1329,0.0919,0.1152\n","0.9278,0.6043,0.8418,0.8482,0.8062,0.8608,0.9199,0.9287,0.0878,0.12\n","0.9165,0.5763,0.7912,0.8318,0.7976,0.83,0.8679,0.9173,0.9162,0.1179\n","0.8964,0.585,0.7736,0.8445,0.7829,0.8469,0.8675,0.895,0.8845,0.9429\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"jROeBqJssdlK"},"source":["result_path = '/content/drive/MyDrive/Colab_Notebooks/LAB/Some Models/SI/result/over_parameter'\n","# 10 tasks permuted:\n","# 1. c=0, hidden_neurons = 100\n","# 2. c = [0.1, 0.01, 1, 0.001]\n","result_name = '0.1_e6_small_si_permuted_10'\n","with open(result_path + \"/\" + result_name + \".txt\", \"w\") as f:\n","  for t in range(1, n_tasks+1):\n","    f.write(\"{} : \".format(str(t)))\n","    for acc in total_acc[t]:\n","      f.write(\"{:.4f} \".format(acc))\n","    # f.write(\" \".join(str(total_acc[t])))\n","    f.write(\"\\n\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LFGYcEk7CCy2","executionInfo":{"status":"ok","timestamp":1609321745854,"user_tz":-420,"elapsed":837,"user":{"displayName":"Lâm Trần","photoUrl":"","userId":"03210360344194418537"}},"outputId":"dcfa9eff-bfcc-48bf-9893-90c69e6a7bc2"},"source":["print(n_epochs)\n","print(total_acc)\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["8\n","[[0.0882, 0.1071, 0.0915, 0.118, 0.1085, 0.0606, 0.0731, 0.0982, 0.073, 0.1115], [0.9569, 0.1415, 0.1122, 0.0993, 0.1068, 0.1461, 0.0639, 0.0834, 0.1037, 0.0927], [0.9074, 0.0958, 0.0956, 0.098, 0.0954, 0.107, 0.0741, 0.1129, 0.1022, 0.0691], [0.9082, 0.0958, 0.1032, 0.1098, 0.1027, 0.1032, 0.1033, 0.104, 0.1055, 0.102], [0.9107, 0.0957, 0.1032, 0.1032, 0.1031, 0.1033, 0.1032, 0.1032, 0.1032, 0.1032], [0.9108, 0.0957, 0.1032, 0.1032, 0.1032, 0.1032, 0.1032, 0.1032, 0.1032, 0.1032], [0.9107, 0.0957, 0.1032, 0.1032, 0.1032, 0.1032, 0.1032, 0.1032, 0.1032, 0.1032], [0.9113, 0.0962, 0.1032, 0.1032, 0.1032, 0.1032, 0.1032, 0.1032, 0.1032, 0.1032], [0.9112, 0.0959, 0.1032, 0.1032, 0.1032, 0.1032, 0.1032, 0.1032, 0.1032, 0.1032], [0.9114, 0.0957, 0.1032, 0.1032, 0.1032, 0.1032, 0.1032, 0.1032, 0.1032, 0.1032], [0.9114, 0.0955, 0.1032, 0.1032, 0.1032, 0.1032, 0.1032, 0.1032, 0.1032, 0.1032]]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"FVib6Oe6Grtc"},"source":["# Tune Hyperparamters using Ax"]},{"cell_type":"code","metadata":{"id":"e8UnL5g8Ls8B","colab":{"base_uri":"https://localhost:8080/","height":445},"outputId":"f0bfb071-776e-4be3-dbd7-d9e69f38f15d"},"source":["def tune(config, objective):\n","    total_acc, total_loss = main(config)\n","    if objective == \"accuracy\":\n","        return np.mean(total_acc[n_tasks-1])\n","    elif objective == \"loss\":\n","        return np.mean(total_loss[n_tasks-1])\n","    else:\n","        return\n","\n","from ax import optimize\n","best_parameters, values, experiment, model = optimize(\n","    parameters=[\n","        {\n","            \"name\": \"lr\",\n","            \"type\": \"range\",\n","            \"bounds\": [1e-4, 0.4], \n","            \"log_scale\": True,\n","            \"value_type\": \"float\",\n","        },\n","        {  \n","            \"name\": \"si_c\",\n","            \"type\": \"range\",\n","            \"bounds\": [0.01, 0.5],\n","            \"value_type\": \"float\",\n","        },\n","        {  \n","            \"name\": \"si_epsilon\",\n","            \"type\": \"fixed\",\n","            \"value\": 0.01,\n","            \"value_type\": \"float\",\n","        },\n","        {  \n","            \"name\": \"batch_size\",\n","            \"type\": \"fixed\",\n","            \"value\": 64,\n","            \"value_type\": \"int\",\n","        },\n","        {  \n","            \"name\": \"sample_size\",\n","            \"type\": \"fixed\",\n","            \"value\": 10000,\n","            \"value_type\": \"int\",\n","        },\n","        {  \n","            \"name\": \"n_neurons\",\n","            \"type\": \"fixed\",\n","            \"value\": 100,\n","            \"value_type\": \"int\",\n","        },\n","        {  \n","            \"name\": \"optimizer\",\n","            \"type\": \"fixed\",\n","            \"value\": \"adam\",\n","            \"value_type\": \"str\",\n","        },\n","    ],\n","    evaluation_function=lambda p: tune(p, \"accuracy\"),\n","    objective_name='accuracy',\n",")\n","print(best_parameters)\n","print(values)\n","    #evaluation_function=lambda p: np.mean(main(p)[n_tasks-1]),\n","    #minimize=True,)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[INFO 04-05 20:41:10] ax.modelbridge.dispatch_utils: Using Bayesian Optimization generation strategy: GenerationStrategy(name='Sobol+GPEI', steps=[Sobol for 7 arms, GPEI for subsequent arms], generated 0 arm(s) so far). Iterations after 7 will take longer to generate due to model-fitting.\n","[INFO 04-05 20:41:10] ax.service.managed_loop: Started full optimization with 20 steps.\n","[INFO 04-05 20:41:10] ax.service.managed_loop: Running optimization trial 1...\n","[INFO 04-05 20:42:21] ax.service.managed_loop: Running optimization trial 2...\n","[INFO 04-05 20:43:32] ax.service.managed_loop: Running optimization trial 3...\n","[INFO 04-05 20:44:44] ax.service.managed_loop: Running optimization trial 4...\n","[INFO 04-05 20:45:55] ax.service.managed_loop: Running optimization trial 5...\n","[INFO 04-05 20:47:06] ax.service.managed_loop: Running optimization trial 6...\n","[INFO 04-05 20:48:17] ax.service.managed_loop: Running optimization trial 7...\n","[INFO 04-05 20:49:28] ax.service.managed_loop: Running optimization trial 8...\n","[INFO 04-05 20:50:40] ax.service.managed_loop: Running optimization trial 9...\n","[INFO 04-05 20:51:52] ax.service.managed_loop: Running optimization trial 10...\n","[INFO 04-05 20:53:04] ax.service.managed_loop: Running optimization trial 11...\n","[INFO 04-05 20:54:16] ax.service.managed_loop: Running optimization trial 12...\n","[INFO 04-05 20:55:28] ax.service.managed_loop: Running optimization trial 13...\n","[INFO 04-05 20:56:40] ax.service.managed_loop: Running optimization trial 14...\n","[INFO 04-05 20:57:56] ax.service.managed_loop: Running optimization trial 15...\n","[INFO 04-05 20:59:12] ax.service.managed_loop: Running optimization trial 16...\n","[INFO 04-05 21:00:27] ax.service.managed_loop: Running optimization trial 17...\n","[INFO 04-05 21:01:42] ax.service.managed_loop: Running optimization trial 18...\n","[INFO 04-05 21:02:58] ax.service.managed_loop: Running optimization trial 19...\n","[INFO 04-05 21:04:12] ax.service.managed_loop: Running optimization trial 20...\n"],"name":"stderr"},{"output_type":"stream","text":["{'lr': 0.002969074196382516, 'si_c': 0.441531161522652, 'si_epsilon': 0.01, 'batch_size': 64, 'sample_size': 10000, 'n_neurons': 100, 'optimizer': 'adam'}\n","({'accuracy': 0.6400499142783531}, {'accuracy': {'accuracy': 1.1108064544778255e-09}})\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"VY2BX7OXi0pF","colab":{"base_uri":"https://localhost:8080/","height":445},"outputId":"70638a27-0966-429b-8630-40537b22279d"},"source":["best_parameters, values, experiment, model = optimize(\n","    parameters=[\n","        {\n","            \"name\": \"lr\",\n","            \"type\": \"range\",\n","            \"bounds\": [1e-4, 0.4], \n","            \"log_scale\": True,\n","            \"value_type\": \"float\",\n","        },\n","        {  \n","            \"name\": \"si_c\",\n","            \"type\": \"fixed\",\n","            \"value\": 0.152,\n","            \"value_type\": \"float\",\n","        },\n","        {  \n","            \"name\": \"si_epsilon\",\n","            \"type\": \"fixed\",\n","            \"value\": 0.01,\n","            \"value_type\": \"float\",\n","        },\n","        {  \n","            \"name\": \"batch_size\",\n","            \"type\": \"choice\",\n","            \"values\": [64, 128, 256],\n","            \"value_type\": \"int\",\n","        },\n","        {  \n","            \"name\": \"sample_size\",\n","            \"type\": \"choice\",\n","            \"values\": [1000, 5000, 10000, 20000, 40000, 60000],\n","            \"value_type\": \"int\",\n","        },\n","        {  \n","            \"name\": \"n_neurons\",\n","            \"type\": \"fixed\",\n","            \"value\": 100,\n","            \"value_type\": \"int\",\n","        },\n","        {  \n","            \"name\": \"optimizer\",\n","            \"type\": \"fixed\",\n","            \"value\": \"adam\",\n","            \"value_type\": \"str\",\n","        },\n","    ],\n","    evaluation_function=lambda p: tune(p, \"accuracy\"),\n","    objective_name='accuracy',\n",")\n","print(best_parameters)\n","print(values)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[INFO 04-05 21:05:27] ax.modelbridge.dispatch_utils: Using Sobol generation strategy.\n","[INFO 04-05 21:05:27] ax.service.managed_loop: Started full optimization with 20 steps.\n","[INFO 04-05 21:05:27] ax.service.managed_loop: Running optimization trial 1...\n","[INFO 04-05 21:06:12] ax.service.managed_loop: Running optimization trial 2...\n","[INFO 04-05 21:06:51] ax.service.managed_loop: Running optimization trial 3...\n","[INFO 04-05 21:08:05] ax.service.managed_loop: Running optimization trial 4...\n","[INFO 04-05 21:08:57] ax.service.managed_loop: Running optimization trial 5...\n","[INFO 04-05 21:11:03] ax.service.managed_loop: Running optimization trial 6...\n","[INFO 04-05 21:13:53] ax.service.managed_loop: Running optimization trial 7...\n","[INFO 04-05 21:14:45] ax.service.managed_loop: Running optimization trial 8...\n","[INFO 04-05 21:15:29] ax.service.managed_loop: Running optimization trial 9...\n","[INFO 04-05 21:17:34] ax.service.managed_loop: Running optimization trial 10...\n","[INFO 04-05 21:20:38] ax.service.managed_loop: Running optimization trial 11...\n","[INFO 04-05 21:21:47] ax.service.managed_loop: Running optimization trial 12...\n","[INFO 04-05 21:22:36] ax.service.managed_loop: Running optimization trial 13...\n","[INFO 04-05 21:24:42] ax.service.managed_loop: Running optimization trial 14...\n","[INFO 04-05 21:26:18] ax.service.managed_loop: Running optimization trial 15...\n","[INFO 04-05 21:26:58] ax.service.managed_loop: Running optimization trial 16...\n","[INFO 04-05 21:28:11] ax.service.managed_loop: Running optimization trial 17...\n","[INFO 04-05 21:29:46] ax.service.managed_loop: Running optimization trial 18...\n","[INFO 04-05 21:30:27] ax.service.managed_loop: Running optimization trial 19...\n","[INFO 04-05 21:32:15] ax.service.managed_loop: Running optimization trial 20...\n"],"name":"stderr"},{"output_type":"stream","text":["{'lr': 0.004312738958492966, 'batch_size': 256, 'sample_size': 60000, 'si_c': 0.152, 'si_epsilon': 0.01, 'n_neurons': 100, 'optimizer': 'adam'}\n","({'accuracy': 0.551365}, {'accuracy': {'accuracy': 0.0}})\n"],"name":"stdout"}]}]}