{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import sys\n",
    "import time\n",
    "import random \n",
    "\n",
    "\n",
    "class Member:    # a document: tfidf representation, label, name\n",
    "    def __init__(self, r_d, label, doc_id):\n",
    "        self._r_d = r_d    # tfidf representation\n",
    "        self._label = label\n",
    "        self._doc_id = doc_id\n",
    "    \n",
    "class Cluster:   # contains its centroid and its members\n",
    "    def __init__(self):\n",
    "        self._centroid = None\n",
    "        self._members = []\n",
    "    def reset_members(self):  # after recalculating centroids => clear all clusters to assign again\n",
    "        self._members = []\n",
    "    def add_member(self, member):\n",
    "        self._members.append(member)\n",
    "    \n",
    "class K_Means:\n",
    "    def __init__(self, num_clusters):\n",
    "        self._num_clusters = num_clusters\n",
    "        self._clusters = [Cluster() for _ in range(self._num_clusters)]\n",
    "        self._E = []   # list of current centroids of K clusters\n",
    "        self._S = 0     # current total similarity\n",
    "        \n",
    "    def load_data(self, path):\n",
    "        def sparse_to_dense(sparsed_r_d, vocab_size):\n",
    "            r_d = np.zeros(vocab_size)\n",
    "            split_sparse_r_d = sparsed_r_d.split()\n",
    "            for item in split_sparse_r_d:\n",
    "                idx = int(item.split(':')[0])\n",
    "                tfidf = float(item.split(':')[1])\n",
    "                r_d[idx] = tfidf\n",
    "            return r_d    # numpy array\n",
    "        \n",
    "        with open(\"D:\\\\movedFromC\\\\123\\\\20192\\\\PRJ2\\\\Project2\\\\words_idfs.txt\") as f:\n",
    "            vocab_size = len(f.read().splitlines())\n",
    "            \n",
    "        with open(path) as f:\n",
    "            doc_lines = f.read().splitlines()\n",
    "        self._data = []       # one doc (a member) is an item of data\n",
    "        self._label_count = defaultdict(int)    # count number of occurences of one class\n",
    "        for data_id, data in enumerate(doc_lines):\n",
    "            features = data.split('<fff>')\n",
    "            label = int(features[0])\n",
    "            doc_id = int(features[1])\n",
    "            self._label_count[label] += 1\n",
    "            r_d = sparse_to_dense(features[2], vocab_size)\n",
    "            \n",
    "            self._data.append(Member(r_d, label, doc_id))\n",
    "        \n",
    "    def compute_similarity(self, member, centroid):   \n",
    "        # choosing Euclidean distance\n",
    "        dist = np.linalg.norm(member._r_d - centroid) + 1e-12\n",
    "        return 1. / dist\n",
    "\n",
    "    def select_cluster_for(self, member):\n",
    "        best_cluster = None\n",
    "        max_similarity = -1\n",
    "        for cluster in self._clusters:\n",
    "            similarity = self.compute_similarity(member, cluster._centroid)\n",
    "            if similarity > max_similarity:\n",
    "                best_cluster = cluster\n",
    "                best_similarity = similarity\n",
    "        best_cluster.add_member(member)      \n",
    "        return max_similarity      # similarity between member and its found cluster\n",
    "    \n",
    "    def update_centroids_for(self, cluster):\n",
    "        # after assigning data points to theirs clusters => update centroids\n",
    "        # one member => one vector of tfidf => compute mean vector \n",
    "        # in order to normalize, find the square of the sum of (each element)^2 from the mean vector\n",
    "    \n",
    "        member_r_ds = [member._r_d for member in cluster._members]   # get tfidf\n",
    "        aver_r_d = np.mean(member_r_ds, axis = 0)  # compute mean => new_centroid\n",
    "        denom = np.sqrt(np.sum(aver_r_d ** 2))\n",
    "       # new_centroid = np.array([numerator / denom for numerator in aver_r_d])  # normalize new_centroid\n",
    "        new_centroid = aver_r_d / denom\n",
    "        cluster._centroid = new_centroid\n",
    "        \n",
    "    def init_centroids(self, init_strategy):\n",
    "        types = ['k-means++', 'random']\n",
    "        assert init_strategy in types\n",
    "        \n",
    "        if init_strategy == 'k-means++':\n",
    "            # randomly pick one data point as the first centroid\n",
    "            self._clusters[0]._centroid = self._data[random.randrange(0, len(self._data)-1), 1]._r_d\n",
    "            idx = 0\n",
    "            d = sys.maxsize\n",
    "            # a list in which each item is the distance between a data point and the nearest previously chosen centroid\n",
    "            dist_points_centroid = [d] * len(self._data)\n",
    "            dist_points_centroid = np.array(dist_points_centroid)\n",
    "            \n",
    "            # loop to find the rest K-1 centroids\n",
    "            for centroid_idx in range(self._num_clusters - 1):\n",
    "                for i in range(len(self._data)):\n",
    "                    point = self._data[i]._r_d\n",
    "                    # only need to compute distance between the point and the newest centroid\n",
    "                    temp_dist = np.sum((self._clusters[idx] - point)**2)\n",
    "                    dist_points_centroid[i] = min(dist_points_centroid[i], temp_dist)\n",
    "                \n",
    "                # choose the data point that has the largest distance with its nearest centroid\n",
    "                next_centroid = self._data[np.argmax(dist_points_centroid)]._r_d\n",
    "                self._clusters[idx+1]._centroid = next_centroid\n",
    "                idx += 1\n",
    "                \n",
    "        if init_strategy == 'random':\n",
    "            randomed = []\n",
    "            for i in range(self._num_clusters):\n",
    "                data_idx = random.randrange(0, len(self._data)-1)\n",
    "                while (data_idx in randomed):\n",
    "                    data_idx = random.randrange(0, len(self._data)-1)\n",
    "                self._clusters[i]._centroid = self._data[data_idx]._r_d\n",
    "                randomed.append(data_idx)\n",
    "            \n",
    "        \n",
    "    def stopping_condition(self, criterion, threshold):\n",
    "        criteria = ['centroid', 'similarity', 'max_iters']\n",
    "        assert criterion in criteria\n",
    "        if criterion == 'max_iters':\n",
    "            if self._iterations >= threshold:\n",
    "                return True\n",
    "            else: \n",
    "                return False\n",
    "        elif criterion == 'centroid':    # |E_new \\ E| < threshold\n",
    "            E_new = [list(cluster._centroid) for cluster in self._clusters]\n",
    "            E_new_minus_E = [centroid for centroid in E_new if centroid not in self._E]\n",
    "            self._E = E_new\n",
    "            if (len(E_new_minus_E) < threshold):\n",
    "                return True\n",
    "            else:\n",
    "                return False\n",
    "        else: \n",
    "            S_new_minus_S = self._new_S - self._S\n",
    "            self._S = self._new_S\n",
    "            if S_new_minus_S <= threshold:\n",
    "                return True\n",
    "            else:\n",
    "                return False\n",
    "            \n",
    "    def run(self, init_strategy, criterion, threshold):\n",
    "        #init centroids\n",
    "        self.init_centroids(init_strategy)\n",
    "        \n",
    "        self._iterations = 0\n",
    "        self._new_S = 0\n",
    "        \n",
    "        while True:\n",
    "            print(\"Epoch \", str(self._iterations+1))\n",
    "            # clear all members in each cluster\n",
    "            for cluster in self._clusters:\n",
    "                cluster.reset_members()\n",
    "            # select cluster for every data point\n",
    "            for member in self._data:\n",
    "                max_similarity = self.select_cluster_for(member)\n",
    "                self._new_S += max_similarity\n",
    "            # update centroids for clusters:\n",
    "            for cluster in self._clusters:\n",
    "                self.update_centroids_for(cluster)\n",
    "            \n",
    "            #check stopping_conddition:\n",
    "            self._iterations += 1;\n",
    "            if self.stopping_condition(criterion, threshold):\n",
    "                break\n",
    "        \n",
    "        \n",
    "    def compute_purity(self):\n",
    "        numerator = 0\n",
    "        for cluster in self._clusters:\n",
    "            members_labels = [member._label for member in cluster._members]   # list of labels of all doc in one cluster\n",
    "            max_count = max([members_labels.count(label) for label in range(20)]) # find the number of times the class that appears the most in that cluster\n",
    "            numerator += max_count\n",
    "        return numerator * 1. / len(self._data)\n",
    "    \n",
    "    def comupte_NMI(self):\n",
    "        I, H_omega, H_c, N = 0., 0., 0., len(self._data)\n",
    "        for label in range(20):\n",
    "            # compute H_c\n",
    "            cj = self._label_count[label]\n",
    "            H_c += -(cj/N * np.log10(cj/N))\n",
    "        for cluster in self._clusters:\n",
    "            # compute H_omage\n",
    "            wk = len(cluster._members) * 1.\n",
    "            H_omega += -(wk/N * np.log10(wk/N))\n",
    "            \n",
    "            # compute I\n",
    "            member_labels = [member._label for member in cluster._members]\n",
    "            for label in range(20):\n",
    "                wk_cj = member_labels.count(label) * 1.\n",
    "                cj = self._label_count[label]\n",
    "                I += (wk_cj / N) * np.log10(N * wk_cj / (wk * cj) + 1e-12)\n",
    "        \n",
    "        return I * 2. / (H_c + H_omega)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_clusters = 20\n",
    "km = K_Means(num_clusters)\n",
    "\n",
    "km.load_data(\"D:\\\\movedFromC\\\\123\\\\20192\\\\PRJ2\\\\Project2\\\\words_tfidfs.txt\")\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11314"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#len(km._data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "best_model = km\n",
    "tmp_training_purity = 0\n",
    "best_training_purity  = 0\n",
    "\n",
    "num_attempts = 5\n",
    "init_strategy = \"random\"\n",
    "criterion = \"max_iters\"\n",
    "threshold = 25\n",
    "\n",
    "for i in range(num_attempts):\n",
    "    t1 = time.time()\n",
    "    km.run(init_strategy, criterion, threshold)\n",
    "    t2 = time.time()\n",
    "    print(\"Training time: \", t2-t1)\n",
    "    tmp_training_purity = km.compute_purity()\n",
    "    if (tmp_training_purity > best_training_purity):\n",
    "        best_model = km\n",
    "    print(\"Purity: \", tmp_training_purity)\n",
    "    print(\"-------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best model\")\n",
    "print(\"Purity: \", km.compute_purity())\n",
    "print(\"NMI: \", km.compute_NMI())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
