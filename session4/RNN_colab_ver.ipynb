{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import  isfile\n",
    "from collections import defaultdict\n",
    "import re\n",
    "\n",
    "def gen_data_and_vocab():\n",
    "    def collect_data_from(par_path, newsgroup_list, word_count=None):\n",
    "        data = []\n",
    "        for group_id, newsgroup in enumerate(newsgroup_list):\n",
    "            dir_path = par_path + '\\\\' + newsgroup + '\\\\'\n",
    "            files = [(filename, dir_path+filename) for filename in listdir(dir_path) if isfile(dir_path+filename)]\n",
    "            files.sort()\n",
    "            label = group_id\n",
    "\n",
    "            for filename, file_path in files:\n",
    "                with open(file_path) as f:\n",
    "                    text = f.read().lower()\n",
    "                    words = re.split('\\W+', text)\n",
    "                    if word_count is not None:    # only get words from traning data\n",
    "                        for word in words:\n",
    "                            word_count[word] += 1\n",
    "                    content = ' '.join(words)\n",
    "                    assert len(content.splitlines()) == 1\n",
    "                    data.append(str(label) + '<fff>' + filename + '<fff>' + content)\n",
    "        return data\n",
    "\n",
    "    word_count = defaultdict(int)\n",
    "\n",
    "    path = 'D:\\\\movedFromC\\\\123\\\\20192\\\\PRJ2\\\\20news-bydate\\\\'\n",
    "    parts = [path + dir_name + '\\\\' for dir_name in listdir(path) if not isfile(path + dir_name)]\n",
    "    train_path, test_path = (parts[0], parts[1]) if 'train' in parts[0] else (parts[1], parts[0])\n",
    "\n",
    "    newsgroups_list = [newsgroup for newsgroup in listdir(train_path)]\n",
    "    newsgroups_list.sort()\n",
    "\n",
    "    train_data = collect_data_from(par_path=train_path, newsgroup_list=newsgroups_list, word_count=word_count)\n",
    "    vocab = [word for word, fre in zip(word_count.keys(), word_count.values()) if fre > 10]\n",
    "    vocab.sort()\n",
    "    with open('D:\\\\movedFromC\\\\123\\\\20192\\\\PRJ2\\\\data_RNN\\\\vocab_raw.txt', 'w') as f:\n",
    "        f.write('\\n'.join(vocab))\n",
    "\n",
    "    test_data = collect_data_from(par_path=test_path, newsgroup_list=newsgroups_list)\n",
    "\n",
    "    with open('D:\\\\movedFromC\\\\123\\\\20192\\\\PRJ2\\\\data_RNN\\\\train_raw.txt', 'w') as f:\n",
    "        f.write('\\n'.join(train_data))\n",
    "    with open('D:\\\\movedFromC\\\\123\\\\20192\\\\PRJ2\\\\data_RNN\\\\test_raw.txt', 'w') as f:\n",
    "        f.write('\\n'.join(test_data))\n",
    "\n",
    "\n",
    "MAX_DOC_LENGTH = 500\n",
    "unknown_ID = 0\n",
    "padding_ID = 1\n",
    "def encode_data(data_path, vocab_path):\n",
    "    with open(vocab_path) as f:\n",
    "        vocab = dict([(word, word_ID+2) for word_ID, word in enumerate(f.read().splitlines())])\n",
    "    \n",
    "    with open(data_path) as f:\n",
    "        docs = [(line.split('<fff>')[0], line.split('<fff>')[1], line.split('<fff>')[2])\n",
    "               for line in f.read().splitlines()]\n",
    "    \n",
    "    encoded_data = []\n",
    "    for doc in docs:\n",
    "        label, doc_id, text = doc\n",
    "        words = text.split()[:MAX_DOC_LENGTH]\n",
    "        sentence_len = len(words)\n",
    "        \n",
    "        encode_text = []\n",
    "        for word in words:\n",
    "            if word in vocab:\n",
    "                encode_text.append(str(vocab[word]))\n",
    "            else:\n",
    "                encode_text.append(str(unknown_ID))\n",
    "        if sentence_len < MAX_DOC_LENGTH:\n",
    "            num_padding = MAX_DOC_LENGTH - sentence_len\n",
    "            for i in range(num_padding):\n",
    "                encode_text.append(str(padding_ID))\n",
    "        \n",
    "        encoded_data.append(str(label) + '<fff>' + str(doc_id) + '<fff>' + str(sentence_len) + '<fff>' + ' '.join(encode_text))\n",
    "        \n",
    "        dir_name = '\\\\'.join(data_path.split('\\\\')[:-1])\n",
    "        file_name = '_'.join(data_path.split('\\\\')[-1].split('_')[:-1]) + '_encoded.txt'\n",
    "        with open(dir_name + '\\\\' + file_name, 'w') as f:\n",
    "            f.write('\\n'.join(encoded_data))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_data_and_vocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_path = 'D:\\\\movedFromC\\\\123\\\\20192\\\\PRJ2\\\\data_RNN\\\\train_raw.txt'\n",
    "test_data_path = 'D:\\\\movedFromC\\\\123\\\\20192\\\\PRJ2\\\\data_RNN\\\\test_raw.txt'\n",
    "vocab_path = 'D:\\\\movedFromC\\\\123\\\\20192\\\\PRJ2\\\\data_RNN\\\\vocab_raw.txt'\n",
    "encode_data(train_data_path, vocab_path)\n",
    "encode_data(test_data_path, vocab_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "class DataReader:\n",
    "    def __init__(self, data_path, bs, vocab_size):\n",
    "    self._bs = bs\n",
    "    with open(data_path) as f:\n",
    "        d_lines = f.read().splitlines()\n",
    "\n",
    "    self._data = []\n",
    "    self._labels = []\n",
    "    self._sentence_len = []\n",
    "    for data_id, line in enumerate(d_lines):\n",
    "        if len(line) > 1:\n",
    "            feature = line.split('<fff>')\n",
    "            label, doc_id, sentence_len = int(feature[0]), int(feature[1]), int(feature[2])\n",
    "            tokens = feature[3].split()\n",
    "            vector = [int(token) for token in tokens]\n",
    "            self._data.append(vector)\n",
    "            self._labels.append(label)\n",
    "            self._sentence_len.append(sentence_len)\n",
    "    self._data = np.array(self._data)\n",
    "    self._labels = np.array(self._labels)\n",
    "    self._sentence_len = np.array(self._sentence_len)\n",
    "\n",
    "    self._num_epoch = 0\n",
    "    self._batch_id = 0\n",
    "\n",
    "    def next_batch(self):\n",
    "    start = self._batch_id * self._bs\n",
    "    end = start + self._bs\n",
    "    self._batch_id += 1\n",
    "\n",
    "    if (end > len(self._data)):\n",
    "        end = len(self._data)\n",
    "        start = end - self._bs\n",
    "        self._num_epoch += 1\n",
    "        self._batch_id = 0\n",
    "        indices = list(range(len(self._data)))\n",
    "        random.seed(2101)\n",
    "        random.shuffle(indices)\n",
    "        self._data = self._data[indices]\n",
    "        self_labels = self._labels[indices]\n",
    "        self._sentence_len = self._sentence_len[indices]\n",
    "\n",
    "    return self._data[start:end], self._labels[start:end], self._sentence_len[start:end]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device_name = tf.test.gpu_device_name()\n",
    "print(device_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_eager_execution()\n",
    "NUM_CLASSES = 20\n",
    "MAX_DOC_LENGTH = 500\n",
    "\n",
    "class RNN:\n",
    "    def __init__(self, vocab_size, embedding_size, lstm_size, bs):\n",
    "    self._vocab_size = vocab_size\n",
    "    self._embedding_size = embedding_size\n",
    "    self._lstm_size = lstm_size\n",
    "    self._bs = bs\n",
    "\n",
    "    with tf.device(device_name):\n",
    "\n",
    "        self._data = tf.placeholder(tf.int32, shape=[bs, MAX_DOC_LENGTH])\n",
    "        self._labels = tf.placeholder(tf.int32, shape=[bs,])\n",
    "        self._sentence_len = tf.placeholder(tf.int32, shape=[bs,])\n",
    "\n",
    "    def embedding_layer(self, indices):\n",
    "    pretrain_vecs = []\n",
    "    pretrain_vecs.append(np.zeros(self._embedding_size))\n",
    "    np.random.seed(2101)\n",
    "    for i in range(self._vocab_size+1):\n",
    "        pretrain_vecs.append(np.random.normal(\n",
    "            loc=0.,\n",
    "            scale=1.,\n",
    "            size=self._embedding_size\n",
    "        ))\n",
    "    pretrain_vecs = np.array(pretrain_vecs)\n",
    "\n",
    "    self._embedding_maxtrix = tf.get_variable(\n",
    "        name = 'embedding',\n",
    "        shape = (self._vocab_size+2, self._embedding_size),\n",
    "        initializer = tf.constant_initializer(pretrain_vecs)\n",
    "    )\n",
    "\n",
    "    return tf.nn.embedding_lookup(self._embedding_maxtrix, indices)\n",
    "\n",
    "    def LSTM_layer(self, embeddings):\n",
    "    lstm_cell = tf.nn.rnn_cell.BasicLSTMCell(self._lstm_size)\n",
    "    zero_state = tf.zeros(shape=(self._bs, self._lstm_size))\n",
    "    initial_state = tf.nn.rnn_cell.LSTMStateTuple(zero_state, zero_state)\n",
    "\n",
    "    lstm_inputs = tf.unstack(tf.transpose(embeddings, perm=[1,0,2]))\n",
    "    lstm_outputs, last_state = tf.nn.static_rnn(\n",
    "        cell = lstm_cell,\n",
    "        inputs = lstm_inputs,\n",
    "        initial_state = initial_state,\n",
    "        sequence_length = self._sentence_len\n",
    "    )\n",
    "    lstm_outputs = tf.unstack(tf.transpose(lstm_outputs, perm=[1,0,2]))\n",
    "    lstm_outputs = tf.concat(lstm_outputs, axis=0)\n",
    "\n",
    "    mask = tf.sequence_mask(\n",
    "        lengths = self._sentence_len,\n",
    "        maxlen = MAX_DOC_LENGTH,\n",
    "        dtype=tf.float32\n",
    "    )\n",
    "    mask = tf.concat(tf.unstack(mask, axis=0), axis=0)\n",
    "    mask = tf.expand_dims(mask, -1)\n",
    "\n",
    "    lstm_outputs = mask*lstm_outputs\n",
    "    lstm_outputs_split = tf.split(lstm_outputs, num_or_size_splits=self._bs)\n",
    "    lstm_outputs_sum = tf.reduce_sum(lstm_outputs_split, axis=1)\n",
    "    lstm_outputs_aver = lstm_outputs_sum / tf.expand_dims(tf.cast(self._sentence_len, tf.float32), -1)\n",
    "\n",
    "    return lstm_outputs_aver\n",
    "\n",
    "    def build_graph(self):\n",
    "    embeddings = self.embedding_layer(self._data)\n",
    "    lstm_outputs = self.LSTM_layer(embeddings)\n",
    "\n",
    "    weights = tf.get_variable(\n",
    "        name = 'final_layer_weights',\n",
    "        shape = (self._lstm_size, NUM_CLASSES),\n",
    "        initializer = tf.random_normal_initializer(seed=2101)\n",
    "    )\n",
    "    biases = tf.get_variable(\n",
    "        name = 'final_layer_biases',\n",
    "        shape = (NUM_CLASSES),\n",
    "        initializer = tf.random_normal_initializer(seed=2101)\n",
    "    )\n",
    "    logits = tf.matmul(lstm_outputs, weights) + biases\n",
    "\n",
    "    label_one_hot = tf.one_hot(\n",
    "        indices = self._labels,\n",
    "        depth = NUM_CLASSES,\n",
    "        dtype = tf.float32\n",
    "    )\n",
    "\n",
    "    loss = tf.nn.softmax_cross_entropy_with_logits(\n",
    "        labels = label_one_hot,\n",
    "        logits = logits\n",
    "    )\n",
    "\n",
    "    loss = tf.reduce_mean(loss)\n",
    "\n",
    "    probs = tf.nn.softmax(logits)\n",
    "    pred_labels = tf.argmax(probs, axis=1)\n",
    "    pred_labels = tf.squeeze(pred_labels)\n",
    "\n",
    "    return pred_labels, loss\n",
    "\n",
    "    def trainer(self, loss, lr):\n",
    "    optimizer = tf.train.AdamOptimizer(lr).minimize(loss)\n",
    "    return optimizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_eval_RNN():\n",
    "    with open('/content/drive/My Drive/Colab Notebooks/LAB/data_20newsgroups/w2v/vocab_raw.txt', 'rb') as f:\n",
    "        vocab_size = len(f.readlines())\n",
    "\n",
    "    tf.set_random_seed(2101)\n",
    "\n",
    "    rnn = RNN(\n",
    "        vocab_size=vocab_size,\n",
    "        embedding_size=300,\n",
    "        lstm_size=50,\n",
    "        bs=20\n",
    "    )\n",
    "\n",
    "    pred_labels, loss = rnn.build_graph()\n",
    "    op = rnn.trainer(loss=loss, lr=0.0005)\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        train_data_reader = DataReader(\n",
    "            data_path='/content/drive/My Drive/Colab Notebooks/LAB/data_20newsgroups/w2v/train_encoded.txt',\n",
    "            bs = 20,\n",
    "            vocab_size = vocab_size\n",
    "        )\n",
    "        test_data_reader = DataReader(\n",
    "            data_path='/content/drive/My Drive/Colab Notebooks/LAB/data_20newsgroups/w2v/test_encoded.txt',\n",
    "            bs = 20,\n",
    "            vocab_size = vocab_size\n",
    "        )\n",
    "\n",
    "        step, MAX_STEP = 0, 2000\n",
    "\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        while step < MAX_STEP:\n",
    "            next_train_batch = train_data_reader.next_batch()\n",
    "            train_data, train_labels, train_sentence_len = next_train_batch\n",
    "            plabel_eval,loss_eval, _ = sess.run(\n",
    "                [pred_labels, loss, op],\n",
    "                feed_dict={\n",
    "                    rnn._data: train_data,\n",
    "                    rnn._labels: train_labels,\n",
    "                    rnn._sentence_len: train_sentence_len\n",
    "                }\n",
    "            )\n",
    "            step += 1\n",
    "            if step % 20 == 0:\n",
    "                print(\"step: \" + str(step) + \" loss: \" + str(loss_eval))\n",
    "            if train_data_reader._batch_id == 0:\n",
    "                corr = 0\n",
    "                while True:\n",
    "                    next_test_batch = test_data_reader.next_batch()\n",
    "                    test_data, test_labels, test_sentence_len = next_test_batch\n",
    "                    test_plabel_eval= sess.run(\n",
    "                        pred_labels,\n",
    "                        feed_dict={\n",
    "                            rnn._data: test_data,\n",
    "                            rnn._labels: test_labels,\n",
    "                            rnn._sentence_len: test_sentence_len\n",
    "                        }\n",
    "                    )\n",
    "                    matches = np.equal(test_plabel_eval, test_labels)\n",
    "                    corr += np.sum(matches.astype(float))\n",
    "                    if test_data_reader._batch_id == 0:\n",
    "                        break\n",
    "            print(\"Epoch: \", train_data_reader._num_epoch)\n",
    "            print(\"Test accuracy: \", corr*100. / len(test_data_reader._data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "train_and_eval_RNN()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
