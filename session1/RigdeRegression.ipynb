{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(path):\n",
    "    data = np.loadtxt(path)\n",
    "    X = np.array(data[:, 1:-1])\n",
    "    Y = np.array(data[:, -1])\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = get_data(\"D:\\\\movedFromC\\\\123\\LabML\\\\session1\\\\deathRate_data.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_and_add_ones(X):    # X is already in numpy arr form\n",
    "    # a matrix where every row element is the maximum value\n",
    "    X_max = np.array([[np.amax(X[:, col_id]) for col_id in range(X.shape[1])]\n",
    "                     for _ in range(X.shape[0])])\n",
    "    \n",
    "    # a matrix where every row element is the minimum value\n",
    "    X_min = np.array([[np.amin(X[:, col_id]) for col_id in range(X.shape[1])]\n",
    "                     for _ in range(X.shape[0])])\n",
    "    \n",
    "    # feature scaling\n",
    "    X_normalized = (X - X_min) / (X_max - X_min)\n",
    "    \n",
    "    ones = np.array([[1] for _ in range(X.shape[0])])\n",
    "    \n",
    "    return np.column_stack((ones, X_normalized))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RidgeRegression:\n",
    "    def __init__(self):\n",
    "        return\n",
    "    \n",
    "    def fit(self, X_train, Y_train, LAMBDA):\n",
    "        assert len(X_train.shape) == 2 and X_train.shape[0] == Y_train.shape[0]\n",
    "        \n",
    "        W = np.linalg.inv(X_train.transpose().dot(X_train)\n",
    "                          + LAMBDA * np.identity(X_train.shape[1])\n",
    "                         ).dot(X_train.transpose()).dot(Y_train)\n",
    "        \n",
    "      #  W = np.dot(np.dot(np.linalg.inv(np.dot(X_train.transpose(), X_train) + LAMBDA * np.identity(X_train.shape[1])), X_train.transpose()), Y_train)\n",
    "        return W\n",
    "        \n",
    "    def predict(self, W, X_new):\n",
    "        Y_new = X_new.dot(W)\n",
    "        return Y_new\n",
    "    \n",
    "    def compute_RSS(self, Y_new, Y_predicted):\n",
    "        loss = 1. / Y_new.shape[0] * np.sum((Y_new - Y_predicted) ** 2)\n",
    "        return loss\n",
    "    \n",
    "    def fit_gradient(self, X_train, Y_train, LAMBDA, learning_rate, max_num_epoch=100, batch_size=128):\n",
    "        # init W\n",
    "        W = np.random.randn(X_train.shape[1])\n",
    "        last_loss = 10e+8\n",
    "        for ep in range(max_num_epoch):\n",
    "            # shuffle data points X\n",
    "            arr = np.array(range(X_train.shape[0]))\n",
    "            np.random.shuffle(arr)\n",
    "            X_train = X_train[arr]\n",
    "            Y_train = Y_train[arr]   # Y_train must be respectively shuffled\n",
    "            total_minibatch = int(np.ceil(X_train.shape[0] / batch_size))\n",
    "            for i in range(total_minibatch):\n",
    "                index = i * batch_size\n",
    "                X_train_sub = X_train[index: index+batch_size]\n",
    "                Y_train_sub = Y_train[index: index+batch_size]\n",
    "                grad = X_train_sub.transpose().dot(X_train_sub.dot(W) - Y_train_sub) + LAMBDA * W\n",
    "                W = W - learning_rate*grad\n",
    "            Y_predicted = predict(W, X_train)\n",
    "            new_loss = compute_RSS(Y_train, Y_predicted)\n",
    "            if (np.abs(new_loss - last_loss) < 1e-5):\n",
    "                break\n",
    "            last_lost = new_lost\n",
    "        return W\n",
    "    \n",
    "    def get_the_best_LAMBDA(self, X_train, Y_train):\n",
    "        def cross_validation(num_folds, LAMBDA):\n",
    "            # all operations should be on indices of X_train \n",
    "            row_ids = np.array(range(X_train.shape[0]))\n",
    "            # first, make sure indicec is divisiable by num_folds\n",
    "            valid_ids = np.split(row_ids[: len(row_ids) - len(row_ids) % num_folds], num_folds)\n",
    "            # then, append the rest to the last valid_id[]\n",
    "            valid_ids[-1] = np.append(valid_ids[-1], row_ids[len(row_ids) - len(row_ids) % num_folds :])\n",
    "            # create an array holding lists of train_ids respectively with valid_ids[i]\n",
    "            train_ids = [[k for k in row_ids if k not in valid_ids[i]] for i in range(num_folds)]\n",
    "            \n",
    "            #init aver_RSS\n",
    "            aver_RSS = 0\n",
    "            for i in range(num_folds):\n",
    "                valid_part = {'X': X_train[valid_ids[i]], 'Y': Y_train[valid_ids[i]]}\n",
    "                train_part = {'X': X_train[train_ids[i]], 'Y': Y_train[train_ids[i]]}\n",
    "                W = self.fit(train_part['X'], train_part['Y'], LAMBDA)\n",
    "                Y_predicted = self.predict(W, valid_part['X'])\n",
    "                aver_RSS += self.compute_RSS(valid_part['Y'], Y_predicted)\n",
    "            return aver_RSS / num_folds\n",
    "        \n",
    "        def range_scan(best_LAMBDA, minimum_RSS, LAMBDA_values):\n",
    "            for curr_LAMBDA in LAMBDA_values:\n",
    "                curr_RSS = cross_validation(5, curr_LAMBDA)\n",
    "                if (minimum_RSS > curr_RSS):\n",
    "                    best_LAMBDA = curr_LAMBDA\n",
    "                    minimum_RSS = curr_RSS\n",
    "            return best_LAMBDA, minimum_RSS\n",
    "        \n",
    "        best_LAMBDA, minimum_RSS = range_scan(0, 10000**2, range(50))\n",
    "        LAMBDA_values = [k * 1. / 1000 for k in range(max(0, best_LAMBDA-1) * 1000, (best_LAMBDA+1) * 1000, 1)]\n",
    "        \n",
    "        best_LAMBDA, minimum_RSS = range_scan(best_LAMBDA, minimum_RSS, LAMBDA_values)\n",
    "        return best_LAMBDA\n",
    "        \n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best LAMBDA is:  0.002\n",
      "1527.0698078029754\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    X, Y = get_data(\"D:\\\\movedFromC\\\\123\\LabML\\\\session1\\\\deathRate_data.txt\")\n",
    "    X = normalize_and_add_ones(X)\n",
    "    X_train = X[: 50]\n",
    "    Y_train = Y[: 50]\n",
    "    X_test = X[50 :]\n",
    "    Y_test = Y[50 :]\n",
    "    \n",
    "    ridge_regression = RidgeRegression()\n",
    "    best_LAMBDA = ridge_regression.get_the_best_LAMBDA(X_train, Y_train)\n",
    "    print('Best LAMBDA is: ', best_LAMBDA)\n",
    "    W_learned = ridge_regression.fit(X_train, Y_train, best_LAMBDA)\n",
    "    Y_predicted = ridge_regression.predict(W_learned, X_test)\n",
    "    print (ridge_regression.compute_RSS(Y_test, Y_predicted))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
