{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import WordPunctTokenizer       # split words and punctuations\n",
    "import string\n",
    "from string import digits\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "def preprocess(stringDoc):  # stringDoc is a list of lines from one doc\n",
    "    \n",
    "    def splitWord(words):       \n",
    "        new_punctuations = (string.punctuation)\n",
    "        table = str.maketrans('', '', new_punctuations)     # remove punctuations\n",
    "        tk = WordPunctTokenizer() \n",
    "        new_word = tk.tokenize(words)\n",
    "        new_word = [word.translate(table) for word in new_word]            # delete those punctuations\n",
    "        return new_word \n",
    "    \n",
    "    \n",
    "    def preprocess(words):      # input is an array of words (array of strings)\n",
    "\n",
    "        table1 = str.maketrans('', '', '\\t')        # remove tabs\n",
    "        words = [word.translate(table1) for word in words]    # translate func works with string    \n",
    "\n",
    "        #there are enters at several lines, or at the end of a line when reading line by line => remove them\n",
    "        table3 = str.maketrans('', '', '\\n')\n",
    "        words = [word.translate(table3) for word in words]\n",
    "\n",
    "        res = []\n",
    "        for word in words:\n",
    "            tem = splitWord(word)\n",
    "            for i in tem:\n",
    "                res.append(i)\n",
    "        words = res\n",
    "\n",
    "        # remove spaces (maybe after translation, --- becomes space)\n",
    "        words = [word for word in words if word]\n",
    "\n",
    "        # remove numbers\n",
    "        words = [word for word in words if not word.isdigit()]\n",
    "\n",
    "        # remove number from word (ex: 3DDD)\n",
    "        remove_digits = str.maketrans('', '', digits) \n",
    "        words = [word.translate(remove_digits) for word in words]\n",
    "\n",
    "        # after removing numbers, there might be single characters or 2-letter-characters, remove them\n",
    "        words = [word for word in words if len(word) > 2]\n",
    "\n",
    "        # lowercase\n",
    "        words = [word.lower() for word in words]\n",
    "\n",
    "        # final step: remove left blank spaces \n",
    "        words = [word for word in words if word]\n",
    "\n",
    "        return words\n",
    "    \n",
    "    def remove_metadata(listOfLines):\n",
    "        for i in range(len(listOfLines)):\n",
    "            if (listOfLines[i] == '\\n'):\n",
    "                break\n",
    "        newList = listOfLines[i+1:]\n",
    "        return newList\n",
    "    \n",
    "    def remove_stopwords(words):          # input is an array of words\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        number = ['one', 'two', 'three', 'four', 'five', 'six', 'seven', 'eight', 'nine', 'ten', 'hundred', 'thousand', '1st', '2nd', '3rd',\n",
    "        '4th', '5th', '6th', '7th', '8th', '9th', '10th']\n",
    "        for i in number:\n",
    "            stop_words.add(i)\n",
    "        new_words = [word for word in words if word not in stop_words]\n",
    "        return new_words\n",
    "    \n",
    "    \n",
    " \n",
    "\n",
    "    def sentence_tokenize(line):          # token a single sentence in form of a string, return an array of words\n",
    "        ps = PorterStemmer()\n",
    "        words = line.strip().split()      # strip() removes leading and trailing spaces, split() removes inbetween spaces, split into list of words \n",
    "        words = preprocess(words)         # after process, words are in lowercase => can check with stopwords\n",
    "        words = remove_stopwords(words)\n",
    "        words = [ps.stem(word) for word in words]\n",
    "        return words\n",
    "    \n",
    "    def tokenize(listOfLines):           # token a whole document in a form of list of lines (strings)\n",
    "        # firstly, remove metadata\n",
    "        #newList = remove_metadata(listOfLines)             \n",
    "\n",
    "        wordsOfDoc = []     # an array to store words of one document\n",
    "        for line in listOfLines:\n",
    "            words = sentence_tokenize(line)\n",
    "            if (len(words) > 0):              # remove enter-character-line\n",
    "                wordsOfDoc.append(words)\n",
    "        return wordsOfDoc\n",
    "    \n",
    "    def flatten(list):\n",
    "        res = []\n",
    "        for i in list:\n",
    "            for j in i: \n",
    "                res.append(j)\n",
    "        return res\n",
    "    \n",
    "    result = flatten(tokenize(stringDoc))    # transform a document to a vector\n",
    "    \n",
    "    return result      # an array of cleaned words from one doc\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import join\n",
    "\n",
    "train_path = \"D:\\\\movedFromC\\\\123\\\\20192\\\\PRJ2\\\\20news-bydate\\\\20news-bydate-train\"\n",
    "test_path = \"D:\\\\movedFromC\\\\123\\\\20192\\\\PRJ2\\\\20news-bydate\\\\20news-bydate-test\"\n",
    "\n",
    "folders = [f for f in listdir(train_path)]     # labels\n",
    "\n",
    "singleBOW = []\n",
    "train_label = []\n",
    "singleBOWTest = []\n",
    "test_label = []\n",
    "# read train data\n",
    "for i in range(len(folders)):\n",
    "    files = listdir(join(train_path, folders[i]))\n",
    "    for j in range(len(files)):\n",
    "        contentFile = open(join(join(train_path, folders[i]), files[j]), \"r\")\n",
    "        singleBOW.append(preprocess(contentFile.readlines()))\n",
    "        train_label.append(i)\n",
    "        \n",
    "# read test data\n",
    "for i in range(len(folders)):\n",
    "    files = listdir(join(test_path, folders[i]))\n",
    "    for j in range(len(files)):\n",
    "        contentFile = open(join(join(test_path, folders[i]), files[j]), \"r\")\n",
    "        singleBOWTest.append(preprocess(contentFile.readlines()))\n",
    "        test_label.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uniqueWords = []                       # a dictionary of whole docs\n",
    "for i in range(len(singleBOW)):\n",
    "    uniqueWords = set(uniqueWords).union(set(singleBOW[i]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " #len(uniqueWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = dict.fromkeys(uniqueWords, 0)\n",
    "\n",
    "for tem in singleBOW:\n",
    "    for word in tem:\n",
    "        dfs[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = 10\n",
    "\n",
    "trimmed_uniqueWords = [word for word in uniqueWords if dfs[word] > df]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trimmed_singleBOW = []\n",
    "for tem in singleBOW:\n",
    "    trimmed_tem = [word for word in tem if dfs[word] > df]\n",
    "    trimmed_singleBOW.append(trimmed_tem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(trimmed_singleBOW)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
