{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"PLSI.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyO+K6ShZ+tbXtwIQoP5XTYj"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"E21TJuXOuIdC","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1599923995539,"user_tz":-420,"elapsed":47303,"user":{"displayName":"Lâm Trần","photoUrl":"","userId":"03210360344194418537"}},"outputId":"ff469eba-e996-4a3d-87ed-b0e85a312d90"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"f7wB7Zf622HW","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1599923995542,"user_tz":-420,"elapsed":9745,"user":{"displayName":"Lâm Trần","photoUrl":"","userId":"03210360344194418537"}}},"source":["import numpy as np\n","import re\n","import random "],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"QEAP_Kqdud4j","colab_type":"code","colab":{}},"source":["def preprocess(dataPath, stopwordsPath):\n","  \n","  with open(stopwordsPath) as f:\n","    stopwords = [line.strip() for line in f]\n","\n","  with open(dataPath) as f:\n","    docs = [doc.strip() for doc in f]\n","  \n","  N = len(docs)\n","\n","  word2id = {}\n","  id2word = {}\n","  currId = 0\n","  wordids = list()\n","  wordcts = list()\n","\n","  for doc in docs:  \n","    doc = doc.lower()\n","    doc = re.sub(r'-', ' ', doc)\n","    doc = re.sub(r'[^a-z]', ' ', doc)\n","    doc = re.sub(r' +', ' ', doc)\n","    words = doc.split()\n","    wordCount = {}    \n","    \n","    for word in words:\n","      word = word.lower().strip()\n","      if len(word)>1 and word not in stopwords and not re.search('[0-9]', word):\n","        if word not in word2id.keys():\n","          word2id[word] = currId\n","          id2word[currId] = word\n","          currId += 1\n","        if word in wordCount:\n","          wordCount[word] += 1\n","        else: wordCount[word] = 1\n","\n","    ids = []\n","    for word in wordCount.keys():\n","      ids.append(word2id[word])\n","    wordids.append(ids)\n","    wordcts.append(list(wordCount.values()))\n","\n","  M = len(word2id)\n","\n","  return N, M, (wordids, wordcts), word2id, id2word\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pHnEkgN-LQih","colab_type":"code","colab":{}},"source":["def init(K):\n","  theta = np.random.rand(N, K)\n","  beta = np.random.rand(K,M)\n","  for n in range(0, N):\n","    normalization = sum(theta[n, :])\n","    theta[n, :] = theta[n, :] / normalization\n","  for k in range(0, K):\n","    normalization = sum(beta[k, :])\n","    beta[k, :] = beta[k,:] / normalization\n","  return theta, beta"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1a5pChH4BlgA","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":119},"executionInfo":{"status":"ok","timestamp":1599757876843,"user_tz":-420,"elapsed":1434,"user":{"displayName":"Lâm Trần","photoUrl":"","userId":"03210360344194418537"}},"outputId":"286a369a-f323-414c-8c0f-98ff53d84c2c"},"source":["theta, beta = init(K)\n","\n","ids = wordids[0]\n","cts = wordcts[0]\n","\n","thetad = theta[0, :]\n","betad = beta[:, ids]\n","\n","print(\"shape thetad:\", thetad.shape)\n","print(\"shape betad:\", betad.shape)\n","print(\"shape of sum:\", np.sum(betad, axis=1).shape)\n","a = (thetad * np.sum(betad, axis=1)) / sum(cts)\n","print(a)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["shape thetad: (10,)\n","shape betad: (10, 171)\n","shape of sum: (10,)\n","[8.89971106e-07 1.76284264e-05 1.79170880e-05 1.11236559e-06\n"," 1.72732532e-05 9.62232781e-06 1.42350131e-05 1.08435072e-05\n"," 1.60445396e-05 7.39686682e-06]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"1X0fGsdsMemy","colab_type":"code","colab":{}},"source":["theta, beta = init(K)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"g9JLbY3KFRxL","colab_type":"code","colab":{}},"source":["def logLikelihood():\n","  res = 0\n","  for d in range(0, N):\n","    ids = wordids[d]\n","    thetad = theta[d,:]\n","    betad = beta[:, ids]\n","    res += np.sum(np.dot(thetad, betad))\n","  return res\n","\n","def EM(max_iter, threshold):\n","  old_loglikelihood = logLikelihood()\n","\n","  for iter in range(0, max_iter):\n","    beta_component = np.zeros((K, M))\n","    for d in range(0, N):\n","      ids = wordids[d]\n","      cts = wordcts[d]\n","\n","      thetad = theta[d, :]\n","      betad = beta[:, ids]\n","\n","      to_norm = np.dot(thetad, betad)\n","\n","      theta[d,:] = (thetad * np.sum(betad / to_norm, axis=1)) / sum(cts)\n","\n","      beta_component[:, ids] += np.expand_dims(thetad,axis=1) * ((betad / to_norm) * cts)\n","\n","    beta = beta_component / np.sum(beta_component, axis=0)\n","\n","    new_loglikelihood = logLikelihood()\n","    print(\"old:\", old_loglikelihood)\n","    print(\"new:\", new_loglikelihood)\n","    if (new_loglikelihood - old_loglikelihood < threshold):\n","      break\n","    old_loglikelihood = new_loglikelihood\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wHJ5Dwv97EaN","colab_type":"code","colab":{}},"source":["def output():\n","  for i in range(0, K):\n","    print(\"Topic\", i)\n","    topicword = []\n","    ids = (beta[i, :]).argsort()\n","    for j in ids:\n","      topicword.append(id2word[j])\n","    for word in range(0, min(10, len(topicword))):\n","      print(topicword[word], end=\" \")\n","    print('\\n')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kt12dwB3tKy8","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1599931064428,"user_tz":-420,"elapsed":1209,"user":{"displayName":"Lâm Trần","photoUrl":"","userId":"03210360344194418537"}}},"source":["class PLSI:\n","  \n","  def __init__(self, max_iter, tol, dataPath, stopwordsPath, K):\n","    self._K = K\n","    self._max_iter = max_iter\n","    self._tol = tol\n","    self._dataPath = dataPath\n","    self._stopwordsPath = stopwordsPath\n","    \n","  def preprocess(self):\n","    \n","    with open(self._stopwordsPath) as f:\n","      stopwords = [line.strip() for line in f]\n","\n","    with open(self._dataPath) as f:\n","      docs = [doc.strip() for doc in f]\n","    \n","    self._N = len(docs)\n","\n","    self._word2id = {}\n","    self._id2word = {}\n","    currId = 0\n","    self._wordids = list()\n","    self._wordcts = list()\n","\n","    for doc in docs:  \n","      doc = doc.lower()\n","      doc = re.sub(r'-', ' ', doc)\n","      doc = re.sub(r'[^a-z]', ' ', doc)\n","      doc = re.sub(r' +', ' ', doc)\n","      words = doc.split()\n","      wordCount = {}    \n","      \n","      for word in words:\n","        word = word.lower().strip()\n","        if len(word)>1 and word not in stopwords and not re.search('[0-9]', word):\n","          if word not in self._word2id.keys():\n","            self._word2id[word] = currId\n","            self._id2word[currId] = word\n","            currId += 1\n","          if word in wordCount:\n","            wordCount[word] += 1\n","          else: wordCount[word] = 1\n","\n","      ids = []\n","      for word in wordCount.keys():\n","        ids.append(self._word2id[word])\n","      self._wordids.append(ids)\n","      self._wordcts.append(list(wordCount.values()))\n","\n","    self._M = len(self._word2id)\n","\n","  def init(self):\n","    np.random.seed(0)\n","    self._theta = np.random.rand(self._N, self._K)\n","    self._beta = np.random.rand(self._K,self._M)\n","    for n in range(0, self._N):\n","      normalization = sum(self._theta[n, :])\n","      self._theta[n, :] = self._theta[n, :] / normalization\n","    for k in range(0, self._K):\n","      normalization = sum(self._beta[k, :])\n","      self._beta[k, :] = self._beta[k,:] / normalization\n","\n","  def logLikelihood(self):\n","    res = 0\n","    for d in range(0, self._N):\n","      ids = self._wordids[d]\n","      cts = self._wordcts[d]\n","      thetad = self._theta[d,:]\n","      for j in ids:\n","        res += np.log(np.dot(thetad, self._beta[:, j])) * cts[ids.index(j)]\n","    return res\n","\n","  def EM(self):\n","    old_loglikelihood = self.logLikelihood()\n","\n","    for iter in range(0, self._max_iter):\n","      beta_component = np.zeros((self._K, self._M))\n","      for d in range(0, self._N):\n","        ids = self._wordids[d]\n","        cts = self._wordcts[d]\n","\n","        thetad = self._theta[d, :]\n","        betad = self._beta[:, ids]\n","\n","        to_norm = np.dot(thetad, betad)\n","\n","        self._theta[d,:] = (thetad * np.sum(betad / to_norm * cts, axis=1)) / sum(cts)\n","\n","        beta_component[:, ids] += np.expand_dims(thetad,axis=1) * ((betad / to_norm) * cts)\n","\n","      self._beta = beta_component / np.expand_dims(np.sum(beta_component, axis=1), axis=1)\n","\n","      new_loglikelihood = self.logLikelihood()\n","      print(\"iter:\", iter)\n","      print(\"old:\", old_loglikelihood)\n","      print(\"new:\", new_loglikelihood)\n","      if (new_loglikelihood - old_loglikelihood < self._tol):\n","        break\n","      old_loglikelihood = new_loglikelihood\n","\n","  def output(self):\n","    for i in range(0, self._K):\n","      print(\"Topic\", i)\n","      topicword = []\n","      ids = (self._beta[i, :]).argsort()\n","      for j in ids:\n","        topicword.insert(0, self._id2word[j])\n","      for word in range(0, min(10, len(topicword))):\n","        print(topicword[word], end=\" \")\n","      print('\\n')\n","\n","  def EM2(self):\n","    old_loglikelihood = self.logLikelihood()\n","    to = np.zeros((self._N, self._M, self._K))\n","    for iter in range(self._max_iter):\n","      # e_step\n","      for d in range(0,self._N):\n","        ids = self._wordids[d]\n","        for n in ids:\n","          norm_k = 0\n","          for k in range(0, self._K):\n","            to[d,n,k] = self._theta[d,k] * self._beta[k,n] \n","            norm_k += to[d,n,k]\n","          for k in range(0, self._K):\n","            to[d,n,k] /= norm_k\n","    \n","      # m_step\n","      # update theta\n","      for d in range(0, self._N):\n","        ids = self._wordids[d]\n","        cts = self._wordcts[d]\n","        for k in range(0, self._K):\n","          self._theta[d,k] = 0\n","          for n in ids:\n","            self._theta[d,k] += to[d,n,k] * cts[ids.index(n)]\n","          self._theta[d,k] /= sum(cts)\n","      # update beta\n","      for k in range(0, self._K):\n","        norm_j = 0\n","        for j in range(0, self._M):\n","          self._beta[k,j] = 0\n","          for d in range(0, self._N):\n","            ids = self._wordids[d]\n","            cts = self._wordcts[d]\n","            if j in ids:\n","              idx = ids.index(j)\n","              self._beta[k,j] += cts[idx] * to[d,j,k]\n","          norm_j += self._beta[k,j]\n","        for j in range(0, self._M):\n","          self._beta[k,j] /= norm_j\n","\n","      new_loglikelihood = self.logLikelihood()\n","      print(\"iter:\", iter)\n","      print(\"old:\", old_loglikelihood)\n","      print(\"new:\", new_loglikelihood)\n","      if (new_loglikelihood - old_loglikelihood < self._tol):\n","        break\n","      old_loglikelihood = new_loglikelihood\n","    \n","    self._to = to"],"execution_count":66,"outputs":[]},{"cell_type":"code","metadata":{"id":"fN5lFlgMVBvT","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1599931072545,"user_tz":-420,"elapsed":1163,"user":{"displayName":"Lâm Trần","photoUrl":"","userId":"03210360344194418537"}}},"source":["dataPath = \"/content/drive/My Drive/Colab Notebooks/LAB/Some Models/PLSI/dataset2.txt\"\n","stopwordsPath = \"/content/drive/My Drive/Colab Notebooks/LAB/Some Models/PLSI/stopwords.dic\"\n","\n","K = 10\n","max_iter = 20\n","tol = 10.0\n","\n","plsi = PLSI(max_iter, tol, dataPath, stopwordsPath, K)\n"],"execution_count":67,"outputs":[]},{"cell_type":"code","metadata":{"id":"9GD6hJX5eotB","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1599931075064,"user_tz":-420,"elapsed":1601,"user":{"displayName":"Lâm Trần","photoUrl":"","userId":"03210360344194418537"}}},"source":["plsi.preprocess()\n"],"execution_count":68,"outputs":[]},{"cell_type":"code","metadata":{"id":"WhftJoYzJfPy","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1599931314771,"user_tz":-420,"elapsed":239446,"user":{"displayName":"Lâm Trần","photoUrl":"","userId":"03210360344194418537"}},"outputId":"9150fb66-ca42-4d08-b7d0-6dfb7154ee7a"},"source":["plsi.init()\n","plsi.EM2()\n","plsi.output()"],"execution_count":69,"outputs":[{"output_type":"stream","text":["iter: 0\n","old: -161129.26056834045\n","new: -147490.1087418391\n","iter: 1\n","old: -147490.1087418391\n","new: -145094.7533286213\n","iter: 2\n","old: -145094.7533286213\n","new: -141859.22296432525\n","iter: 3\n","old: -141859.22296432525\n","new: -138166.5713788146\n","iter: 4\n","old: -138166.5713788146\n","new: -134702.02753051175\n","iter: 5\n","old: -134702.02753051175\n","new: -131928.68138301602\n","iter: 6\n","old: -131928.68138301602\n","new: -129885.72768146568\n","iter: 7\n","old: -129885.72768146568\n","new: -128382.23918464764\n","iter: 8\n","old: -128382.23918464764\n","new: -127248.53571419256\n","iter: 9\n","old: -127248.53571419256\n","new: -126385.07388998519\n","iter: 10\n","old: -126385.07388998519\n","new: -125716.9750676066\n","iter: 11\n","old: -125716.9750676066\n","new: -125203.0741985239\n","iter: 12\n","old: -125203.0741985239\n","new: -124827.77063270802\n","iter: 13\n","old: -124827.77063270802\n","new: -124547.2589346319\n","iter: 14\n","old: -124547.2589346319\n","new: -124336.74772982775\n","iter: 15\n","old: -124336.74772982775\n","new: -124177.70425092605\n","iter: 16\n","old: -124177.70425092605\n","new: -124050.75726202774\n","iter: 17\n","old: -124050.75726202774\n","new: -123954.04131283324\n","iter: 18\n","old: -123954.04131283324\n","new: -123889.90936952127\n","iter: 19\n","old: -123889.90936952127\n","new: -123839.96208627304\n","Topic 0\n","percent bank government dukakis rose police oil rate month inflation \n","\n","Topic 1\n","percent economy company central gorbachev greyhound nation economic snow fbi \n","\n","Topic 2\n","administration noriega duracell soviet farmer gorbachev percent panama government sen \n","\n","Topic 3\n","california scientists magellan peres spacecraft offer york official thursday gasoline \n","\n","Topic 4\n","bush barry rating gas moore record president day ratings summer \n","\n","Topic 5\n","soviet percent children grain nikolais immigration calgary doctors ferrets manufacturing \n","\n","Topic 6\n","soviet people roberts polish officers pope church skins war day \n","\n","Topic 7\n","people bush campaign florio congress exxon jewish president jews germany \n","\n","Topic 8\n","waste saturday people police saudi plant school national boy arco \n","\n","Topic 9\n","fire north summit friday warming officials leaders police global kim \n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"QyZKjKAKL9rz","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1599929926689,"user_tz":-420,"elapsed":1137,"user":{"displayName":"Lâm Trần","photoUrl":"","userId":"03210360344194418537"}}},"source":["plsi.init()"],"execution_count":62,"outputs":[]},{"cell_type":"code","metadata":{"id":"fFRSPlJkf1zV","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1599929930696,"user_tz":-420,"elapsed":2877,"user":{"displayName":"Lâm Trần","photoUrl":"","userId":"03210360344194418537"}},"outputId":"1ff416e7-941f-482d-abbe-7b1b32260516"},"source":["\n","plsi.EM()\n","plsi.output()"],"execution_count":63,"outputs":[{"output_type":"stream","text":["iter: 0\n","old: -161129.26056834045\n","new: -147521.97155748782\n","iter: 1\n","old: -147521.97155748782\n","new: -144514.56722731955\n","iter: 2\n","old: -144514.56722731955\n","new: -140120.9172173746\n","iter: 3\n","old: -140120.9172173746\n","new: -135351.9043962023\n","iter: 4\n","old: -135351.9043962023\n","new: -131550.7549654084\n","iter: 5\n","old: -131550.7549654084\n","new: -128897.79986439472\n","iter: 6\n","old: -128897.79986439472\n","new: -127128.14500525457\n","iter: 7\n","old: -127128.14500525457\n","new: -125965.9534177318\n","iter: 8\n","old: -125965.9534177318\n","new: -125204.80860313786\n","iter: 9\n","old: -125204.80860313786\n","new: -124711.7005655186\n","iter: 10\n","old: -124711.7005655186\n","new: -124353.42764977404\n","iter: 11\n","old: -124353.42764977404\n","new: -124076.04624864932\n","iter: 12\n","old: -124076.04624864932\n","new: -123881.81579112173\n","iter: 13\n","old: -123881.81579112173\n","new: -123764.09798884019\n","iter: 14\n","old: -123764.09798884019\n","new: -123683.74041467332\n","iter: 15\n","old: -123683.74041467332\n","new: -123626.19751354681\n","iter: 16\n","old: -123626.19751354681\n","new: -123588.40420854598\n","iter: 17\n","old: -123588.40420854598\n","new: -123563.50547535485\n","iter: 18\n","old: -123563.50547535485\n","new: -123542.21165335558\n","iter: 19\n","old: -123542.21165335558\n","new: -123521.09988117644\n","Topic 0\n","percent bank dukakis police government united production told billion settlements \n","\n","Topic 1\n","percent prices economy company report nation rate economic gorbachev greyhound \n","\n","Topic 2\n","administration noriega duracell farmer gorbachev percent panama sen soviet government \n","\n","Topic 3\n","california scientists peres magellan spacecraft offer official york thursday receptor \n","\n","Topic 4\n","bush barry rating president moore record monday ratings summer tuesday \n","\n","Topic 5\n","soviet children grain nikolais doctors ferrets immigration calgary center health \n","\n","Topic 6\n","soviet roberts polish people officers gas skins enron immigration border \n","\n","Topic 7\n","people bush campaign congress florio jewish exxon germany jews president \n","\n","Topic 8\n","people waste pope church president plant national police saudi mexico \n","\n","Topic 9\n","fire north warming friday central officials leaders police global wednesday \n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"s-tjbhz3PM1U","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1599819454739,"user_tz":-420,"elapsed":784,"user":{"displayName":"Lâm Trần","photoUrl":"","userId":"03210360344194418537"}},"outputId":"c591b6fe-db2d-4fd1-cffb-97946cd910ed"},"source":["thetad = plsi._theta[0]    # 1*10\n","cts = plsi._wordcts[0]     # 1*171\n","ids = plsi._wordids[0]\n","betad = plsi._beta[:, ids] # 10*171\n","to_norm = np.dot(thetad, betad)  # 1*171\n","\n","res = thetad * np.sum(betad/to_norm*cts, axis=1) /sum(cts)\n","\n","# beta_component[:, ids] += np.expand_dims(thetad,axis=1) * ((betad / to_norm) * cts)\n","# self._beta = beta_component / np.sum(beta_component, axis=0)\n","\n","ans = np.expand_dims(thetad,axis=1) * ((betad / to_norm) * cts)\n","print(ans.shape)\n","#print((np.sum(betad/to_norm*cts, axis=1)).shape)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["(10, 171)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"nG1EmrhAwPhH","colab_type":"code","colab":{}},"source":["from numpy import zeros, int8, log\n","from pylab import random\n","import sys\n","import jieba\n","import re\n","import time\n","import codecs\n","\n","# segmentation, stopwords filtering and document-word matrix generating\n","# [return]:\n","# N : number of documents\n","# M : length of dictionary\n","# word2id : a map mapping terms to their corresponding ids\n","# id2word : a map mapping ids to terms\n","# X : document-word matrix, N*M, each line is the number of terms that show up in the document\n","def preprocessing(datasetFilePath, stopwordsFilePath):\n","    \n","    # read the stopwords file\n","    file = codecs.open(stopwordsFilePath, 'r', 'utf-8')\n","    stopwords = [line.strip() for line in file] \n","    file.close()\n","    \n","    # read the documents\n","    file = codecs.open(datasetFilePath, 'r', 'utf-8')\n","    documents = [document.strip() for document in file] \n","    file.close()\n","\n","    # number of documents\n","    N = len(documents)\n","\n","    wordCounts = [];\n","    word2id = {}\n","    id2word = {}\n","    currentId = 0;\n","    # generate the word2id and id2word maps and count the number of times of words showing up in documents\n","    for document in documents:\n","        segList = jieba.cut(document)\n","        wordCount = {}\n","        for word in segList:\n","            word = word.lower().strip()\n","            if len(word) > 1 and not re.search('[0-9]', word) and word not in stopwords:               \n","                if word not in word2id.keys():\n","                    word2id[word] = currentId;\n","                    id2word[currentId] = word;\n","                    currentId += 1;\n","                if word in wordCount:\n","                    wordCount[word] += 1\n","                else:\n","                    wordCount[word] = 1\n","        wordCounts.append(wordCount);\n","    \n","    # length of dictionary\n","    M = len(word2id)  \n","\n","    # generate the document-word matrix\n","    X = zeros([N, M], int8)\n","    for word in word2id.keys():\n","        j = word2id[word]\n","        for i in range(0, N):\n","            if word in wordCounts[i]:\n","                X[i, j] = wordCounts[i][word];    \n","\n","    return N, M, word2id, id2word, X\n","\n","def initializeParameters():\n","    for i in range(0, N):\n","        normalization = sum(lamda[i, :])\n","        for j in range(0, K):\n","            lamda[i, j] /= normalization;\n","\n","    for i in range(0, K):\n","        normalization = sum(theta[i, :])\n","        for j in range(0, M):\n","            theta[i, j] /= normalization;\n","\n","def EStep():\n","    for i in range(0, N):\n","        for j in range(0, M):\n","            denominator = 0;\n","            for k in range(0, K):\n","                p[i, j, k] = theta[k, j] * lamda[i, k];\n","                denominator += p[i, j, k];\n","            if denominator == 0:\n","                for k in range(0, K):\n","                    p[i, j, k] = 0;\n","            else:\n","                for k in range(0, K):\n","                    p[i, j, k] /= denominator;\n","\n","def MStep():\n","    # update theta\n","    for k in range(0, K):\n","        denominator = 0\n","        for j in range(0, M):\n","            theta[k, j] = 0\n","            for i in range(0, N):\n","                theta[k, j] += X[i, j] * p[i, j, k]\n","            denominator += theta[k, j]\n","        if denominator == 0:\n","            for j in range(0, M):\n","                theta[k, j] = 1.0 / M\n","        else:\n","            for j in range(0, M):\n","                theta[k, j] /= denominator\n","        \n","    # update lamda\n","    for i in range(0, N):\n","        for k in range(0, K):\n","            lamda[i, k] = 0\n","            denominator = 0\n","            for j in range(0, M):\n","                lamda[i, k] += X[i, j] * p[i, j, k]\n","                denominator += X[i, j];\n","            if denominator == 0:\n","                lamda[i, k] = 1.0 / K\n","            else:\n","                lamda[i, k] /= denominator\n","\n","# calculate the log likelihood\n","def LogLikelihood():\n","    loglikelihood = 0\n","    for i in range(0, N):\n","        for j in range(0, M):\n","            tmp = 0\n","            for k in range(0, K):\n","                tmp += theta[k, j] * lamda[i, k]\n","            if tmp > 0:\n","                loglikelihood += X[i, j] * log(tmp)\n","    return loglikelihood\n","\n","# output the params of model and top words of topics to files\n","def output():\n","  \n","    # top words of each topic\n","    for i in range(0, K):\n","        topicword = []\n","        ids = theta[i, :].argsort()\n","        for j in ids:\n","            topicword.insert(0, id2word[j])\n","        tmp = ''\n","        for word in topicword[0:min(topicWordsNum, len(topicword))]:\n","            tmp += word + ' '\n","\n","        print(\"topic \", i)\n","        print(tmp)\n","    \n","# set the default params and read the params from cmd\n","datasetFilePath = 'dataset.txt'\n","stopwordsFilePath = 'stopwords.dic'\n","threshold = 10.0\n","datasetFilePath = \"/content/drive/My Drive/Colab Notebooks/LAB/Some Models/PLSI/dataset2.txt\"\n","stopwordsFilePath = \"/content/drive/My Drive/Colab Notebooks/LAB/Some Models/PLSI/stopwords.dic\"\n","K = 10\n","maxIteration = 20\n","\n","# preprocessing\n","N, M, word2id, id2word, X = preprocessing(datasetFilePath, stopwordsFilePath)\n","\n","np.random.seed(0)\n","# lamda[i, j] : p(zj|di)\n","lamda = random([N, K])\n","\n","# theta[i, j] : p(wj|zi)\n","theta = random([K, M])\n","\n","# p[i, j, k] : p(zk|di,wj)\n","p = zeros([N, M, K])\n","\n","\n","initializeParameters()\n","\n","# EM algorithm\n","oldLoglikelihood = 1\n","newLoglikelihood = 1\n","for i in range(0, maxIteration):\n","    EStep()\n","    MStep()\n","    newLoglikelihood = LogLikelihood()\n","    print(\"[\", time.strftime('%Y-%m-%d %H:%M:%S',time.localtime(time.time())), \"] \", i+1, \" iteration  \", str(newLoglikelihood))\n","    print(\"theta0:\", lamda[0])\n","    if(oldLoglikelihood != 1 and newLoglikelihood - oldLoglikelihood < threshold):\n","        break\n","    oldLoglikelihood = newLoglikelihood\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"diHV-v9SKRPg","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":51},"executionInfo":{"status":"ok","timestamp":1599928460948,"user_tz":-420,"elapsed":7947,"user":{"displayName":"Lâm Trần","photoUrl":"","userId":"03210360344194418537"}},"outputId":"fcdb8d23-0d30-45e3-be9c-4cd03dee268c"},"source":["plsi._theta = lamda\n","plsi._beta = theta\n","print(plsi.logLikelihood())\n","print(LogLikelihood())"],"execution_count":35,"outputs":[{"output_type":"stream","text":["-229513.3658809677\n","-124024.8353040919\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"eGWrXprxzCK5","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":357},"executionInfo":{"status":"ok","timestamp":1599928178019,"user_tz":-420,"elapsed":2164,"user":{"displayName":"Lâm Trần","photoUrl":"","userId":"03210360344194418537"}},"outputId":"2697d753-f8ac-49a0-a7f3-cdd2fe432da2"},"source":["topicWordsNum = 10\n","output()"],"execution_count":29,"outputs":[{"output_type":"stream","text":["topic  0\n","percent bank prices government oil rate rose police month inflation \n","topic  1\n","barry fbi percent gorbachev greyhound company union moore study program \n","topic  2\n","administration soviet noriega percent duracell gorbachev farmer people president panama \n","topic  3\n","official scientists magellan peres spacecraft offer grain contact receptor bechtel \n","topic  4\n","gas record percent nelson summer tuesday monday immigration power season \n","topic  5\n","soviet percent children nikolais immigration calgary month manufacturing drug anthrax \n","topic  6\n","people california soviet central roberts waste polish nation snow northern \n","topic  7\n","rating people bush campaign florio congress president exxon jewish washington \n","topic  8\n","dukakis bush president people saturday plant saudi front american school \n","topic  9\n","fire owned north summit friday officials reported police black kim \n"],"name":"stdout"}]}]}