{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow.compat.v1 as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.disable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLASSES = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP:\n",
    "    def __init__(self, vocab_size, hidden_size):\n",
    "        self._vocab_size = vocab_size\n",
    "        self._hidden_size = hidden_size\n",
    "        self.X = tf.placeholder(tf.float32, shape=[None, vocab_size])\n",
    "        self._real_Y = tf.placeholder(tf.int32, shape=[None, ])\n",
    "    \n",
    "    def build_graph(self):\n",
    "        w1 = tf.get_variable(\n",
    "            name = 'weights_input_hidden',\n",
    "            shape = (self._vocab_size, self._hidden_size),\n",
    "            initializer = tf.random_normal_initializer(seed=2608)\n",
    "        )\n",
    "        b1 = tf.get_variable(\n",
    "            name = 'biases_input_hidden',\n",
    "            shape = (self._hidden_size),\n",
    "            initializer = tf.random_normal_initializer(seed=2608)\n",
    "        )\n",
    "        w2 = tf.get_variable(\n",
    "            name = 'weights_hidden_output',\n",
    "            shape = (self._hidden_size, NUM_CLASSES),\n",
    "            initializer = tf.random_normal_initializer(seed=2608)\n",
    "        )\n",
    "        b2 = tf.get_variable(\n",
    "            name = 'biases_hidden_output',\n",
    "            shape = (NUM_CLASSES),\n",
    "            initializer = tf.random_normal_initializer(seed=2608)\n",
    "        )\n",
    "        \n",
    "        hidden = tf.matmul(self.X, w1) + b1   # linear hidden layer \n",
    "        hidden = tf.sigmoid(hidden)           # activated output\n",
    "        logits = tf.matmul(hidden, w2) + b2   # linear \n",
    "        \n",
    "        labels_one_hot = tf.one_hot(indices=self._real_Y, depth=NUM_CLASSES, dtype=tf.float32)   # create one hot vector representing ground truth\n",
    "        loss = tf.nn.softmax_cross_entropy_with_logits_v2(labels=labels_one_hot, logits=logits)   # NOTICE: CRE loss working with logits input\n",
    "        loss = tf.reduce_mean(loss)\n",
    "        \n",
    "        probs = tf.nn.softmax(logits)\n",
    "        predicted_labels = tf.argmax(probs, axis=1)\n",
    "        predicted_labels = tf.squeeze(predicted_labels)\n",
    "        \n",
    "        return predicted_labels, loss\n",
    "    \n",
    "    def trainer(self, loss, lr):\n",
    "        optimizer = tf.train.AdamOptimizer(lr).minimize(loss)\n",
    "        return optimizer\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "class DataReader:\n",
    "    def __init__(self, data_path, batch_size, vocab_size):\n",
    "        \n",
    "        self._batch_size = batch_size\n",
    "        \n",
    "        with open(data_path) as f:\n",
    "            doc_lines = f.read().splitlines()\n",
    "        \n",
    "        self._data  = []\n",
    "        self._labels = []\n",
    "        \n",
    "        for data_id, line in enumerate(doc_lines):\n",
    "            vector = [0.0 for _ in range(vocab_size)]\n",
    "            features = line.split('<fff>')\n",
    "            label, doc_id = int(features[0]), int(features[1])\n",
    "            tfidfs = features[2].split()\n",
    "            for tfidf in tfidfs:\n",
    "                idx = int(tfidf.split(':')[0])\n",
    "                val = float(tfidf.split(':')[1])\n",
    "                vector[idx] = val\n",
    "            self._data.append(vector)\n",
    "            self._labels.append(label)\n",
    "        \n",
    "        self._data = np.array(self._data)\n",
    "        self._labels = np.array(self._labels)\n",
    "        \n",
    "        self._num_epochs = 0\n",
    "        self._batch_id = 0\n",
    "        \n",
    "    def next_batch(self):\n",
    "        start = self._batch_id * self._batch_size   # starting idx of the batch\n",
    "        end = start + self._batch_size              # ending idx of the batch\n",
    "        self._batch_id += 1                         \n",
    "        \n",
    "        if end + self._batch_size > len(self._data):    # when the last batch exceed the length of dataset\n",
    "            end = len(self._data)\n",
    "            self._num_epochs += 1\n",
    "            self._batch_id = 0                         # reset _batch_id = 0 for the next epoch\n",
    "            indices = list(range(len(self._data)))\n",
    "            random.seed(2608)\n",
    "            random.shuffle(indices)                    # shuffle data for the next epoch\n",
    "            self._data = self._data[indices]\n",
    "            self._labels = self._labels[indices]\n",
    "        \n",
    "        return self._data[start : end], self._labels[start : end]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_params(name, val, epoch):\n",
    "    file_name = name.replace(':', '-colon-') + '-epoch-{}.txt'.format(epoch)\n",
    "    if len(val.shape) == 1:   # biases\n",
    "        string_form = ','.join([str(num) for num in val])\n",
    "    else:    # weights matrix\n",
    "        string_form = '\\n'.join([','.join([str(num) for num in val[row]])\n",
    "                                for row in range(val.shape[0])])\n",
    "    # with open('D:\\\\movedFromC\\\\123\\\\LabML\\\\session1\\\\MLLab\\\\session3\\\\params\\\\' + filename, 'w') as f:    # run on local\n",
    "    #     f.write(string_form)\n",
    "\n",
    "    with open('/content/drive/My Drive/Colab Notebooks/LAB/session3/params/' + file_name, 'w') as f:    # run on local\n",
    "      f.write(string_form)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def restore_params(name, epoch):\n",
    "    file_name = name.replace(':', '-colon-') + '-epoch-{}.txt'.format(epoch)\n",
    "    with open('/content/drive/My Drive/Colab Notebooks/LAB/session3/params/' + file_name) as f:\n",
    "        lines = f.readlines()\n",
    "    if len(lines) == 1:   # biases\n",
    "        val = [float(num) for num in lines[0].split(',')]\n",
    "    else:    # weights matrix\n",
    "        val = [[float(num) for num in lines[row].split(',')]\n",
    "                  for row in range(len(lines))]\n",
    "    return val\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def train():\n",
    "    \n",
    "  with open('/content/drive/My Drive/Colab Notebooks/PRJ2/words_idfs.txt') as f:\n",
    "    vocab_size = len(f.readlines())\n",
    "    \n",
    "  def load_dataset():\n",
    "    train_reader = DataReader(\n",
    "        data_path = '/content/drive/My Drive/Colab Notebooks/PRJ2/words_tfidfs.txt',\n",
    "        batch_size = 32,\n",
    "        vocab_size = vocab_size\n",
    "    )\n",
    "    return train_reader\n",
    "    \n",
    "  mlp = MLP(vocab_size = vocab_size, hidden_size = 50)\n",
    "\n",
    "  predicted_labels, loss = mlp.build_graph()\n",
    "  train_op = mlp.trainer(loss, lr = 0.001)\n",
    "\n",
    "  train_loss = []   # for plotting\n",
    "  steps = []\n",
    "    \n",
    "  with tf.Session() as sess:\n",
    "    train_reader = load_dataset()\n",
    "    epoch, MAX_EPOCHS = 0, 10000\n",
    "\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    while epoch < MAX_EPOCHS:\n",
    "      train_data, train_labels = train_reader.next_batch()\n",
    "      p_labels_eval, loss_eval, _ = sess.run(\n",
    "        [predicted_labels, loss, train_op],\n",
    "        feed_dict={\n",
    "            mlp.X: train_data,\n",
    "            mlp._real_Y: train_labels\n",
    "        }\n",
    "      )\n",
    "      epoch += 1\n",
    "      # train_loss.append(loss_eval)\n",
    "      # steps.append(epoch)\n",
    "      if epoch % 500 == 0:\n",
    "        print (\"step: {}, loss: {}\".format(epoch, loss_eval))\n",
    "      if loss_eval < 1e-5:\n",
    "        break\n",
    "    trainable_variables = tf.trainable_variables()\n",
    "    for var in trainable_variables:\n",
    "      save_params(name=var.name, val=var.eval(), epoch=train_reader._num_epochs)\n",
    "\n",
    "  return train_reader._num_epochs   # return the number of epochs to pass to the test funtion      \n",
    "        # plt.plot(steps[:1000], train_loss[:1000], 'dt')\n",
    "        # plt.xlabel('Epoch')\n",
    "        # plt.ylabel('Loss')\n",
    "        # plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(num_epochs):\n",
    "    with open('/content/drive/My Drive/Colab Notebooks/PRJ2/words_idfs.txt') as f:\n",
    "        vocab_size = len(f.readlines())\n",
    "\n",
    "    test_reader = DataReader(\n",
    "        data_path = '/content/drive/My Drive/Colab Notebooks/PRJ2/words_tfidfs_test.txt',\n",
    "        batch_size = 32,\n",
    "        vocab_size = vocab_size\n",
    "    )\n",
    "    \n",
    "    mlp = MLP(vocab_size = vocab_size, hidden_size = 50)\n",
    "    predicted_labels,loss = mlp.build_graph()\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        epoch = num_epochs\n",
    "        \n",
    "        trainable_variables = tf.trainable_variables()\n",
    "        for var in trainable_variables:\n",
    "            saved_val = restore_params(var.name, epoch)\n",
    "            assign_op = var.assign(saved_val)\n",
    "            sess.run(assign_op)\n",
    "        corr_pred = 0\n",
    "        while True:\n",
    "            test_data, test_labels = test_reader.next_batch()\n",
    "            test_plabels_eval = sess.run(predicted_labels,\n",
    "                                        feed_dict={\n",
    "                                            mlp.X: test_data,\n",
    "                                            mlp._real_Y: test_labels\n",
    "                                            }\n",
    "                                        )\n",
    "            matches = np.equal(test_plabels_eval, test_labels)\n",
    "            corr_pred += np.sum(matches.astype(float))\n",
    "            if test_reader._batch_id == 0: \n",
    "                break\n",
    "        print('Epoch: ', epoch)\n",
    "        print('Acc: ', corr_pred/len(test_reader._data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Firstly, train the model to get the number of epochs, then pass that number\n",
    "# as a param to test function\n",
    "\n",
    "tf.reset_default_graph()\n",
    "epoch = train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "test(epoch)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
